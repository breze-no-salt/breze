{
 "metadata": {
  "name": "",
  "signature": "sha256:b047bf0f51ff87a61e70f1df3d15323ac55dfc1738701b11d201ffe27a263560"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import itertools\n",
      "import gzip\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "import climin.stops\n",
      "import climin.initialize\n",
      "import climin.project\n",
      "import climin.schedule\n",
      "\n",
      "from breze.learn.cnn import Cnn\n",
      "import breze.arch.util\n",
      "\n",
      "import breze.learn.base\n",
      "from breze.learn.data import one_hot\n",
      "\n",
      "from climin.util import minibatches\n",
      "\n",
      "# theano.config.optimizer = 'None'\n",
      "theano.config.compute_test_value = 'raise'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "1 #define _CUDA_NDARRAY_C\n",
        "2 \n",
        "3 #include <Python.h>\n",
        "4 #include <structmember.h>\n",
        "5 \n",
        "6 #include <numpy/arrayobject.h>\n",
        "7 #include <iostream>\n",
        "8 \n",
        "9 #include \"cuda_ndarray.cuh\"\n",
        "10 \n",
        "11 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
        "12 #define COMPUTE_GPU_MEM_USED 0\n",
        "13 \n",
        "14 //If true, we fill with NAN allocated device memory.\n",
        "15 #define ALLOC_MEMSET 0\n",
        "16 \n",
        "17 //If true, we print out when we free a device pointer, uninitialize a\n",
        "18 //CudaNdarray, or allocate a device pointer\n",
        "19 #define PRINT_FREE_MALLOC 0\n",
        "20 \n",
        "21 //If true, we do error checking at the start of functions, to make sure there\n",
        "22 //is not a pre-existing error when the function is called.\n",
        "23 //You probably need to set the environment variable\n",
        "24 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
        "25 //preprocessor macro in cuda_ndarray.cuh\n",
        "26 //if you want this to work.\n",
        "27 #define PRECHECK_ERROR 0\n",
        "28 \n",
        "29 //If true, we release the GIL around blocking GPU calls, to allow other Python\n",
        "30 //threads to run in the meantime. For a single-threaded program, the overhead\n",
        "31 //is neglectible (about 20ms for 1 million GIL release/reclaim cycles). Can\n",
        "32 //still be overridden on compilation with -DRELEASE_GIL=0 in nvcc.flags.\n",
        "33 #ifndef RELEASE_GIL\n",
        "34 #define RELEASE_GIL 1\n",
        "35 #endif\n",
        "36 #if RELEASE_GIL\n",
        "37 #define CNDA_BEGIN_ALLOW_THREADS Py_BEGIN_ALLOW_THREADS\n",
        "38 #define CNDA_END_ALLOW_THREADS Py_END_ALLOW_THREADS\n",
        "39 #else\n",
        "40 #define CNDA_BEGIN_ALLOW_THREADS\n",
        "41 #define CNDA_END_ALLOW_THREADS\n",
        "42 #endif\n",
        "43 \n",
        "44 cublasHandle_t handle = NULL;\n",
        "45 \n",
        "46 /////////////////////////\n",
        "47 // Alloc and Free\n",
        "48 /////////////////////////\n",
        "49 \n",
        "50 static int g_gpu_context_active = 0;\n",
        "51 \n",
        "52 \n",
        "53 PyObject *\n",
        "54 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
        "55 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
        "56 \n",
        "57 \n",
        "58 /**\n",
        "59  *\n",
        "60  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
        "61  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
        "62  *\n",
        "63  */\n",
        "64 int _outstanding_mallocs[] = {0,0};\n",
        "65 \n",
        "66 #if COMPUTE_GPU_MEM_USED\n",
        "67 size_t _allocated_size = 0;\n",
        "68 size_t _max_allocated_size = 0;\n",
        "69 \n",
        "70 const int TABLE_SIZE = 10000;\n",
        "71 struct table_struct{\n",
        "72     void* ptr;\n",
        "73     size_t size;\n",
        "74 };\n",
        "75 table_struct _alloc_size_table[TABLE_SIZE];\n",
        "76 #endif\n",
        "77 \n",
        "78 void * device_malloc(size_t size)\n",
        "79 {\n",
        "80     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
        "81 }\n",
        "82 \n",
        "83 void * device_malloc(size_t size, int verbose)\n",
        "84 {\n",
        "85     #if PRECHECK_ERROR\n",
        "86         cudaThreadSynchronize();\n",
        "87         cudaError_t prevError = cudaGetLastError();\n",
        "88         if (cudaSuccess != prevError)\n",
        "89         {\n",
        "90             fprintf(stderr,\n",
        "91                     \"Error existed before calling device_malloc. %s\\n\",\n",
        "92                     cudaGetErrorString(prevError)\n",
        "93                     );\n",
        "94         }\n",
        "95     #endif\n",
        "96     void * rval=NULL;\n",
        "97     cudaError_t err = cudaMalloc(&rval, size);\n",
        "98     if (cudaSuccess != err)\n",
        "99     {\n",
        "100         // Clear the error flag, cudaMalloc doesn't do it.\n",
        "101         // Currently this returns the same thing as err, but if in future\n",
        "102         // it returns something else I still don't see why we should ignore\n",
        "103         // it.  All we want to do here is reset the flag.\n",
        "104         cudaGetLastError();\n",
        "105         if (verbose)\n",
        "106         {\n",
        "107             size_t free = 0, total = 0;\n",
        "108             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
        "109             if (err2 != cudaSuccess){\n",
        "110                 cudaGetLastError();\n",
        "111                 fprintf(stderr,\n",
        "112                         \"Error when trying to find the memory information\"\n",
        "113                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
        "114             }\n",
        "115             #if COMPUTE_GPU_MEM_USED\n",
        "116                 fprintf(stderr,\n",
        "117                         \"Error allocating %zd bytes of device memory (%s).\"\n",
        "118                         \" new total bytes allocated: %d.\"\n",
        "119                         \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
        "120                         size, cudaGetErrorString(err), _allocated_size,\n",
        "121                         free, total);\n",
        "122             #else\n",
        "123                 fprintf(stderr,\n",
        "124                         \"Error allocating %zd bytes of device memory (%s).\"\n",
        "125                         \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
        "126                         size, cudaGetErrorString(err), free, total);\n",
        "127             #endif\n",
        "128         }\n",
        "129         PyErr_Format(PyExc_MemoryError,\n",
        "130                      \"Error allocating %zd bytes of device memory (%s).\",\n",
        "131                      size, cudaGetErrorString(err));\n",
        "132         return NULL;\n",
        "133     }\n",
        "134     if (rval != NULL){\n",
        "135         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
        "136         // Could this be what happen if size is 0?\n",
        "137         _outstanding_mallocs[0] += 1;\n",
        "138 \n",
        "139 #if COMPUTE_GPU_MEM_USED\n",
        "140         _allocated_size += size;\n",
        "141         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
        "142         int i = 0;\n",
        "143         for(;i<TABLE_SIZE;i++){\n",
        "144             if(NULL==_alloc_size_table[i].ptr){\n",
        "145                 _alloc_size_table[i].ptr=rval;\n",
        "146                 _alloc_size_table[i].size=size;\n",
        "147                 break;\n",
        "148             }\n",
        "149         }\n",
        "150         if (i == TABLE_SIZE){\n",
        "151             fprintf(stderr,\n",
        "152                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
        "153                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
        "154         }\n",
        "155 #endif\n",
        "156     }\n",
        "157     //fprintf(stderr,\n",
        "158     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
        "159     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
        "160 \n",
        "161     if(ALLOC_MEMSET){\n",
        "162         //We init them to nan to make sure we catch more debug case.\n",
        "163         cudaMemset(rval, 0xFF, size);\n",
        "164         //printf(\"MEMSET\\n\");\n",
        "165     }\n",
        "166     #if PRINT_FREE_MALLOC\n",
        "167         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
        "168     #endif\n",
        "169     return rval;\n",
        "170 }\n",
        "171 \n",
        "172 int device_free(void *ptr)\n",
        "173 {\n",
        "174     #if PRECHECK_ERROR\n",
        "175         cudaThreadSynchronize();\n",
        "176         cudaError_t prevError = cudaGetLastError();\n",
        "177         if (cudaSuccess != prevError)\n",
        "178         {\n",
        "179             fprintf(stderr,\n",
        "180                     \"Error existed before calling device_free. %s\\n\",\n",
        "181                     cudaGetErrorString(prevError)\n",
        "182                     );\n",
        "183         }\n",
        "184     #endif\n",
        "185     #if PRINT_FREE_MALLOC\n",
        "186         size_t free = 0, total = 0;\n",
        "187         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
        "188         if (err2 != cudaSuccess){\n",
        "189             cudaGetLastError();\n",
        "190             fprintf(stderr,\n",
        "191                     \"Error when tring to find the memory information\"\n",
        "192                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
        "193         }\n",
        "194         #if COMPUTE_GPU_MEM_USED\n",
        "195         {\n",
        "196             int i = 0;\n",
        "197             for(;i<TABLE_SIZE;i++)\n",
        "198                 if(_alloc_size_table[i].ptr==ptr){\n",
        "199                     break;\n",
        "200                 }\n",
        "201             assert(i<TABLE_SIZE);\n",
        "202             fprintf(stderr, \"device_free %p of size %d.\"\n",
        "203                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
        "204                     ptr, _alloc_size_table[i].size, free, total);\n",
        "205         }\n",
        "206         #else\n",
        "207             fprintf(stderr, \"device_free %p.\"\n",
        "208                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
        "209                     ptr, free, total);\n",
        "210         #endif\n",
        "211     #endif\n",
        "212 \n",
        "213     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
        "214     if(!g_gpu_context_active) {\n",
        "215         return 0;\n",
        "216     }\n",
        "217 \n",
        "218     // We need sync as the Theano's GC could remove intermediate variable that\n",
        "219     // are still needed as the gpu kernel are running or in the queue.\n",
        "220     CNDA_BEGIN_ALLOW_THREADS\n",
        "221     cudaThreadSynchronize();\n",
        "222     CNDA_END_ALLOW_THREADS\n",
        "223 \n",
        "224     cudaError_t err =  cudaFree(ptr);\n",
        "225     if (cudaSuccess != err)\n",
        "226     {\n",
        "227         // Clear the error flag, cudaFree doesn't do it.\n",
        "228         // Currently this returns the same thing as err, but if in future\n",
        "229         // it returns something else I still don't see why we should ignore\n",
        "230         // it.  All we want to do here is reset the flag.\n",
        "231         cudaGetLastError();\n",
        "232         size_t free = 0, total = 0;\n",
        "233         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
        "234         if (err2 != cudaSuccess){\n",
        "235             cudaGetLastError();\n",
        "236             fprintf(stderr,\n",
        "237                     \"Error when tring to find the memory information\"\n",
        "238                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
        "239         }\n",
        "240         #if COMPUTE_GPU_MEM_USED\n",
        "241         {\n",
        "242             int i = 0;\n",
        "243             for(;i<TABLE_SIZE;i++)\n",
        "244                 if(_alloc_size_table[i].ptr==ptr){\n",
        "245                     break;\n",
        "246                 }\n",
        "247             assert(i<TABLE_SIZE);\n",
        "248             fprintf(stderr,\n",
        "249                     \"Error freeing device pointer %p (%s) of size %d. %zd byte already allocated.\"\n",
        "250                     \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
        "251                     ptr, cudaGetErrorString(err),\n",
        "252                     _alloc_size_table[i].size, _allocated_size, free, total);\n",
        "253         }\n",
        "254         #else\n",
        "255             fprintf(stderr,\n",
        "256                     \"Error freeing device pointer %p (%s).\"\n",
        "257                     \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
        "258                     ptr,\n",
        "259                     cudaGetErrorString(err), free, total);\n",
        "260         #endif\n",
        "261         if (NULL != PyErr_Occurred()){\n",
        "262             fprintf(stderr,\n",
        "263                     \"device_free: cudaFree() returned an error, but there is already an\"\n",
        "264                     \" Python error set. This happen during the clean up when there is a\"\n",
        "265                     \" first error and the CUDA driver is in a so bad state that it don't\"\n",
        "266                     \" work anymore. We keep the previous error set to help debugging it.\");\n",
        "267             return -1;\n",
        "268         }\n",
        "269         PyErr_Format(PyExc_MemoryError,\n",
        "270                 \"error freeing device pointer %p (%s)\",\n",
        "271                 ptr,\n",
        "272                 cudaGetErrorString(err));\n",
        "273         return -1;\n",
        "274     }\n",
        "275     _outstanding_mallocs[0] -= (ptr != NULL);\n",
        "276     #if COMPUTE_GPU_MEM_USED\n",
        "277         int i=0;\n",
        "278         size_t total_freed = 0;\n",
        "279         for(;i<TABLE_SIZE;i++)\n",
        "280             if(_alloc_size_table[i].ptr==ptr){\n",
        "281                 _allocated_size -= _alloc_size_table[i].size;\n",
        "282                 total_freed += _alloc_size_table[i].size;\n",
        "283                 _alloc_size_table[i].ptr=0;\n",
        "284                 _alloc_size_table[i].size=0;\n",
        "285 \n",
        "286                 break;\n",
        "287             }\n",
        "288         //if(i==TABLE_SIZE)\n",
        "289         //    printf(\"Unallocated unknow size!\\n\");\n",
        "290         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
        "291     #endif\n",
        "292     return 0;\n",
        "293 }\n",
        "294 \n",
        "295 static PyObject *\n",
        "296 outstanding_mallocs(PyObject* self, PyObject * args)\n",
        "297 {\n",
        "298     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
        "299 }\n",
        "300 \n",
        "301 /////////////////////////\n",
        "302 // Static helper methods\n",
        "303 /////////////////////////\n",
        "304 \n",
        "305 static void\n",
        "306 CudaNdarray_null_init(CudaNdarray*self)\n",
        "307 {\n",
        "308     self->base = NULL;\n",
        "309     self->nd = -1;\n",
        "310     self->host_structure = NULL;\n",
        "311     self->data_allocated = 0;\n",
        "312     self->dev_structure_fresh = 1;\n",
        "313     self->dev_structure = NULL;\n",
        "314     self->devdata = NULL;\n",
        "315 }\n",
        "316 \n",
        "317 static int\n",
        "318 CudaNdarray_uninit(CudaNdarray*self)\n",
        "319 {\n",
        "320     #if PRINT_FREE_MALLOC\n",
        "321         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
        "322     #endif\n",
        "323     int rval = 0;\n",
        "324     if (self->data_allocated) {\n",
        "325         assert(self->devdata);\n",
        "326         if (device_free(self->devdata))\n",
        "327         {\n",
        "328             fprintf(stderr,\n",
        "329                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
        "330                     self, self->devdata);\n",
        "331             rval = -1;\n",
        "332         }\n",
        "333         self->devdata = NULL;\n",
        "334         self->data_allocated = 0;\n",
        "335     }\n",
        "336     if (self->dev_structure)\n",
        "337     {\n",
        "338         if (device_free(self->dev_structure))\n",
        "339         {\n",
        "340             fprintf(stderr,\n",
        "341                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
        "342                     self->dev_structure, self);\n",
        "343             rval = -1;\n",
        "344         }\n",
        "345         self->dev_structure = NULL;\n",
        "346     }\n",
        "347     if (self->host_structure)\n",
        "348     {\n",
        "349         free(self->host_structure);\n",
        "350         self->host_structure = NULL;\n",
        "351     }\n",
        "352     self->nd = -1;\n",
        "353     Py_XDECREF(self->base);\n",
        "354     self->base = NULL;\n",
        "355     return rval;\n",
        "356 }\n",
        "357 \n",
        "358 \n",
        "359 //make the rightmost coords change fastest\n",
        "360 //TODO: why does a downward for-loop not work????\n",
        "361 //TODO: use the log2_dims and driver code to remove / and %\n",
        "362 //TODO: skip the last division (when d == 0)\n",
        "363 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
        "364 __global__ void name (unsigned int numEls,  \\\n",
        "365         unsigned int nd, \\\n",
        "366         const int * dim,  \\\n",
        "367         const float * a_data, const int * a_str, \\\n",
        "368         float * z_data, const int * z_str) \\\n",
        "369 { \\\n",
        "370     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
        "371     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
        "372  \\\n",
        "373     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
        "374     { \\\n",
        "375         unsigned int ii = i; \\\n",
        "376         const float * a_i = a_data; \\\n",
        "377         float * z_i = z_data; \\\n",
        "378         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
        "379         { \\\n",
        "380             unsigned int d = nd - _d-1;  \\\n",
        "381             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
        "382             ii = ii / dim[d]; \\\n",
        "383             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
        "384             z_i += i_d * z_str[d]; \\\n",
        "385         } \\\n",
        "386         z_i[0] = F(a_i[0]); \\\n",
        "387     } \\\n",
        "388 }\n",
        "389 \n",
        "390 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
        "391 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
        "392 \n",
        "393 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
        "394 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
        "395 \n",
        "396 /////////////////////////////\n",
        "397 // Satisfying reqs to be Type\n",
        "398 /////////////////////////////\n",
        "399 \n",
        "400 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
        "401 static void\n",
        "402 CudaNdarray_dealloc(CudaNdarray* self)\n",
        "403 {\n",
        "404     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
        "405     if(Py_REFCNT(self) > 1)\n",
        "406       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
        "407     CudaNdarray_uninit(self);\n",
        "408     Py_TYPE(self)->tp_free((PyObject*)self);\n",
        "409     --_outstanding_mallocs[1];\n",
        "410     if (0)\n",
        "411     {\n",
        "412         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
        "413                 _outstanding_mallocs[0],\n",
        "414                 _outstanding_mallocs[1]);\n",
        "415     }\n",
        "416 }\n",
        "417 \n",
        "418 static PyObject *\n",
        "419 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
        "420 {\n",
        "421     CudaNdarray *self;\n",
        "422 \n",
        "423     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
        "424     if (self != NULL)\n",
        "425     {\n",
        "426         CudaNdarray_null_init(self);\n",
        "427         ++_outstanding_mallocs[1];\n",
        "428     }\n",
        "429     return (PyObject *)self;\n",
        "430 }\n",
        "431 static int\n",
        "432 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
        "433 {\n",
        "434     PyObject *arr=NULL;\n",
        "435 \n",
        "436     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
        "437         return -1;\n",
        "438     if (! PyArray_Check(arr))\n",
        "439     {\n",
        "440         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
        "441         return -1;\n",
        "442     }\n",
        "443     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
        "444     return rval;\n",
        "445 }\n",
        "446 static PyMemberDef CudaNdarray_members[] =\n",
        "447 {\n",
        "448     /*\n",
        "449     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
        "450      \"first name\"},\n",
        "451     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
        "452      \"last name\"},\n",
        "453     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
        "454      \"noddy number\"},\n",
        "455      */\n",
        "456     {NULL}  /* Sentinel */\n",
        "457 };\n",
        "458 \n",
        "459 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
        "460 {\n",
        "461     PyObject * dtype = NULL;\n",
        "462     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
        "463         return NULL;\n",
        "464     if (dtype) {\n",
        "465         PyArray_Descr* dtype2;\n",
        "466         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
        "467         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
        "468         {\n",
        "469             PyObject * str = PyObject_Repr(dtype);\n",
        "470             PyErr_Format(PyExc_TypeError,\n",
        "471                          \"CudaNdarray dtype parameter not understood: %s\",\n",
        "472                          PyString_AsString(str)\n",
        "473                          );\n",
        "474             Py_CLEAR(str);\n",
        "475             return NULL;\n",
        "476         }\n",
        "477         int typeNum = dtype2->type_num;\n",
        "478         Py_DECREF(dtype2);\n",
        "479         if (typeNum != NPY_FLOAT32)\n",
        "480         {\n",
        "481             PyObject * str = PyObject_Repr(dtype);\n",
        "482             PyErr_Format(PyExc_TypeError,\n",
        "483                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
        "484                          typeNum\n",
        "485                          );\n",
        "486             Py_CLEAR(str);\n",
        "487             return NULL;\n",
        "488         }\n",
        "489     }\n",
        "490 \n",
        "491     int verbose = 0;\n",
        "492     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
        "493         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
        "494         assert (npydims);\n",
        "495         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
        "496         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
        "497         free(npydims);\n",
        "498         if (!rval){\n",
        "499             return NULL;\n",
        "500         }\n",
        "501         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
        "502         return rval;\n",
        "503     }\n",
        "504     if ((self->nd < 0) || (self->devdata == 0))\n",
        "505     {\n",
        "506         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
        "507         return NULL;\n",
        "508     }\n",
        "509     CudaNdarray * contiguous_self = NULL;\n",
        "510     if (CudaNdarray_is_c_contiguous(self))\n",
        "511     {\n",
        "512         contiguous_self = self;\n",
        "513         Py_INCREF(contiguous_self);\n",
        "514         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
        "515     }\n",
        "516     else\n",
        "517     {\n",
        "518         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
        "519         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
        "520     }\n",
        "521     if (!contiguous_self)\n",
        "522     {\n",
        "523         return NULL;\n",
        "524     }\n",
        "525 \n",
        "526     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
        "527     assert (npydims);\n",
        "528     for (int i = 0; i < self->nd; ++i)\n",
        "529         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
        "530     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
        "531                                                                npydims,\n",
        "532                                                                REAL_TYPENUM);\n",
        "533     free(npydims);\n",
        "534     if (!rval)\n",
        "535     {\n",
        "536         Py_DECREF(contiguous_self);\n",
        "537         return NULL;\n",
        "538     }\n",
        "539 \n",
        "540     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
        "541 \n",
        "542     npy_intp rval_size = PyArray_SIZE(rval);\n",
        "543     void *rval_data = PyArray_DATA(rval);\n",
        "544     cublasStatus_t err;\n",
        "545     CNDA_BEGIN_ALLOW_THREADS\n",
        "546     err = cublasGetVector(rval_size, sizeof(real),\n",
        "547                           contiguous_self->devdata, 1,\n",
        "548                           rval_data, 1);\n",
        "549     //CNDA_THREAD_SYNC;  // unneeded because cublasGetVector is blocking anyway\n",
        "550     CNDA_END_ALLOW_THREADS\n",
        "551 \n",
        "552     if (CUBLAS_STATUS_SUCCESS != err)\n",
        "553     {\n",
        "554         PyErr_SetString(PyExc_RuntimeError, \"error copying data to host\");\n",
        "555         Py_DECREF(rval);\n",
        "556         rval = NULL;\n",
        "557     }\n",
        "558 \n",
        "559     Py_DECREF(contiguous_self);\n",
        "560     return (PyObject *)rval;\n",
        "561 }\n",
        "562 \n",
        "563 // TODO-- we have two functions here, ZEROS and Zeros.\n",
        "564 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
        "565 // but this naming is very weird, makes it look like a macro\n",
        "566 // we should figure out the correct convention and change to that\n",
        "567 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
        "568 {\n",
        "569 \n",
        "570     int total_elements = 1;\n",
        "571     for(int i=0;i<n;i++)\n",
        "572         total_elements*=dims[i];\n",
        "573 \n",
        "574     // total_elements now contains the size of the array, in reals\n",
        "575     int total_size = total_elements * sizeof(real);\n",
        "576 \n",
        "577     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
        "578     if (!rval)\n",
        "579     {\n",
        "580         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
        "581         return NULL;\n",
        "582     }\n",
        "583 \n",
        "584     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
        "585     {\n",
        "586         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
        "587         Py_DECREF(rval);\n",
        "588         return NULL;\n",
        "589     }\n",
        "590 \n",
        "591     // Fill with zeros\n",
        "592     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
        "593     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
        "594     {\n",
        "595         PyErr_Format(PyExc_MemoryError, \"CudaNdarray_ZEROS: Error memsetting %d bytes of device memory.\", total_size);\n",
        "596         Py_DECREF(rval);\n",
        "597         return NULL;\n",
        "598     }\n",
        "599 \n",
        "600     if (cnda_copy_structure_to_device(rval))\n",
        "601     {\n",
        "602         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
        "603         Py_DECREF(rval);\n",
        "604         return NULL;\n",
        "605     }\n",
        "606     return (PyObject*) rval;\n",
        "607 }\n",
        "608 \n",
        "609 // declared as a static method (hence 1st parameter is not used)\n",
        "610 // Based on _Copy and _dimshuffle\n",
        "611 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
        "612 {\n",
        "613     if(!shape)\n",
        "614     {\n",
        "615         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
        "616         return NULL;\n",
        "617     }\n",
        "618     if(!PySequence_Check(shape))\n",
        "619     {\n",
        "620         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
        "621         return NULL;\n",
        "622     }\n",
        "623 \n",
        "624     int shplen = PySequence_Length(shape);\n",
        "625 \n",
        "626     if (shplen == 0)\n",
        "627     {\n",
        "628         return CudaNdarray_ZEROS(0, NULL);\n",
        "629     }\n",
        "630 \n",
        "631     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
        "632 \n",
        "633     if (!newdims)\n",
        "634     {\n",
        "635         PyErr_SetString(PyExc_MemoryError,\n",
        "636             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
        "637         return NULL;\n",
        "638     }\n",
        "639 \n",
        "640     // start from the end to compute strides\n",
        "641     for (int i = shplen-1; i >= 0; --i)\n",
        "642     {\n",
        "643         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
        "644         if(shp_el_obj == NULL)\n",
        "645         {\n",
        "646             // shouldn't happen since we checked length before...\n",
        "647             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
        "648             free(newdims);\n",
        "649             return NULL;\n",
        "650         }\n",
        "651 \n",
        "652         int shp_el = PyInt_AsLong(shp_el_obj);\n",
        "653         Py_DECREF(shp_el_obj);\n",
        "654 \n",
        "655         if (shp_el < 0)\n",
        "656         {\n",
        "657             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
        "658             free(newdims);\n",
        "659             return NULL;\n",
        "660         }\n",
        "661 \n",
        "662         newdims[i] = shp_el;\n",
        "663     }\n",
        "664 \n",
        "665     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
        "666 \n",
        "667     free(newdims);\n",
        "668 \n",
        "669     return (PyObject*)rval;\n",
        "670 }\n",
        "671 \n",
        "672 \n",
        "673 \n",
        "674 \n",
        "675 \n",
        "676 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
        "677 {\n",
        "678     PyObject * rval = CudaNdarray_New();\n",
        "679     if ((!rval) || (-1 == self->nd))\n",
        "680     {\n",
        "681         return rval;\n",
        "682     }\n",
        "683     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
        "684     {\n",
        "685         Py_DECREF(rval);\n",
        "686         return NULL;\n",
        "687     }\n",
        "688     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
        "689     {\n",
        "690         Py_DECREF(rval);\n",
        "691         return NULL;\n",
        "692     }\n",
        "693     return rval;\n",
        "694 }\n",
        "695 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
        "696 {\n",
        "697     assert(PyDict_Check(memo));\n",
        "698     PyObject * selfkey = PyInt_FromLong((long)self);\n",
        "699     assert(selfkey);\n",
        "700     if (PyDict_Contains(memo, selfkey))\n",
        "701     {\n",
        "702         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
        "703         Py_DECREF(selfkey);\n",
        "704         Py_XINCREF(rval);\n",
        "705         return rval;\n",
        "706     }\n",
        "707     else\n",
        "708     {\n",
        "709         PyObject * rval = CudaNdarray_Copy(self);\n",
        "710         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
        "711         if (NULL == rval)\n",
        "712         {\n",
        "713             Py_DECREF(selfkey);\n",
        "714             return NULL;\n",
        "715         }\n",
        "716         if (PyDict_SetItem(memo, selfkey, rval))\n",
        "717         {\n",
        "718             Py_DECREF(rval);\n",
        "719             Py_DECREF(selfkey);\n",
        "720             return NULL;\n",
        "721         }\n",
        "722         Py_DECREF(selfkey);\n",
        "723         return rval;\n",
        "724     }\n",
        "725 }\n",
        "726 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
        "727 {\n",
        "728     if (!PySequence_Check(py_reduce_mask))\n",
        "729     {\n",
        "730         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
        "731         return NULL;\n",
        "732     }\n",
        "733     int len = PySequence_Length(py_reduce_mask);\n",
        "734     if (len != self->nd)\n",
        "735     {\n",
        "736         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
        "737         return NULL;\n",
        "738     }\n",
        "739     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
        "740     if (!self_sum)\n",
        "741     {\n",
        "742         return NULL;\n",
        "743     }\n",
        "744     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
        "745     //      and use it if it is big enough.\n",
        "746     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
        "747     int * sum_dims = dimshuffle_pattern + len;\n",
        "748     int n_remaining_dims = 0;\n",
        "749     if (!dimshuffle_pattern)\n",
        "750     {\n",
        "751         Py_DECREF(self_sum);\n",
        "752         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
        "753         return NULL;\n",
        "754     }\n",
        "755     for (int i = 0; i < len; ++i)\n",
        "756     {\n",
        "757         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
        "758         int o_i_int = PyInt_AsLong(o_i);\n",
        "759         Py_XDECREF(o_i);\n",
        "760         if (PyErr_Occurred())\n",
        "761         {\n",
        "762             Py_DECREF(self_sum);\n",
        "763             free(dimshuffle_pattern);\n",
        "764             return NULL;\n",
        "765         }\n",
        "766         if (o_i_int) // this is a dimension over which we are reducing\n",
        "767         {\n",
        "768             sum_dims[i] = 1;\n",
        "769         }\n",
        "770         else\n",
        "771         {\n",
        "772             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
        "773             dimshuffle_pattern[n_remaining_dims++] = i;\n",
        "774         }\n",
        "775     }\n",
        "776     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
        "777             || CudaNdarray_reduce_sum(self_sum, self)\n",
        "778             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
        "779     {\n",
        "780         Py_DECREF(self_sum);\n",
        "781         free(dimshuffle_pattern);\n",
        "782         return NULL;\n",
        "783     }\n",
        "784     free(dimshuffle_pattern);\n",
        "785     return (PyObject*)self_sum;\n",
        "786 }\n",
        "787 \n",
        "788 // Reshape self to the new shape gived by the tuple shape.\n",
        "789 //\n",
        "790 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
        "791 // TODO: make it return a view when the strides allow it even if it is not\n",
        "792 //       c contiguous\n",
        "793 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
        "794 {\n",
        "795     if(!CudaNdarray_is_c_contiguous(self))\n",
        "796     {\n",
        "797         // allocate new space\n",
        "798         //TODO: test to see if we can re-use old one and take a new param to\n",
        "799         //  use this\n",
        "800         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
        "801         if (!rval)\n",
        "802         {\n",
        "803             return NULL;\n",
        "804         }\n",
        "805 \n",
        "806         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
        "807         Py_XDECREF(rval);\n",
        "808         return (PyObject*)ret;\n",
        "809     }\n",
        "810 \n",
        "811     // check shape tuple\n",
        "812     unsigned int rval_nd;\n",
        "813     unsigned int * rval_dims;\n",
        "814     unsigned int rval_size = 1;\n",
        "815 \n",
        "816     if (PyTuple_Check(shape)){\n",
        "817         // copy shape to integer array\n",
        "818         rval_nd = PyTuple_Size(shape);\n",
        "819     }else if (PyInt_Check(shape)){\n",
        "820         rval_nd = 1;\n",
        "821     }else{\n",
        "822         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
        "823         return NULL;\n",
        "824     }\n",
        "825     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
        "826 \n",
        "827     if(PyTuple_Check(shape)){\n",
        "828         for (int i = 0; i < rval_nd; ++i)\n",
        "829         {\n",
        "830             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
        "831             if (PyErr_Occurred()) //error in AsLong\n",
        "832             {\n",
        "833                 free(rval_dims);\n",
        "834                 return NULL;\n",
        "835             }\n",
        "836             if(rval_dims[i]<=0){\n",
        "837                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >0)\",rval_dims[i]);\n",
        "838                 free(rval_dims);\n",
        "839                 return NULL;\n",
        "840             }\n",
        "841             rval_size = rval_size * rval_dims[i];\n",
        "842         }\n",
        "843     }else{\n",
        "844         rval_size = PyInt_AsLong(shape);\n",
        "845         rval_dims[0] = rval_size;\n",
        "846     }\n",
        "847     // calculate new size, assert same as old size\n",
        "848     if (rval_size != CudaNdarray_SIZE(self))\n",
        "849     {\n",
        "850         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %i to %i\", CudaNdarray_SIZE(self), rval_size);\n",
        "851         free(rval_dims);\n",
        "852         return NULL;\n",
        "853     }\n",
        "854     if (rval_size==0)\n",
        "855     {\n",
        "856         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
        "857         free(rval_dims);\n",
        "858         return rval;\n",
        "859     }\n",
        "860 \n",
        "861     //return a view, not a copy\n",
        "862     //we can do this as we checked self is c_contiguous\n",
        "863     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
        "864 \n",
        "865     if (!rval || 0 != rval->data_allocated\n",
        "866         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
        "867     {\n",
        "868         Py_XDECREF(rval);\n",
        "869         free(rval_dims);\n",
        "870         return NULL;\n",
        "871     }\n",
        "872     //set dim and stride\n",
        "873     int size = 1;\n",
        "874     for (int i = rval_nd-1; i >= 0; --i)\n",
        "875     {\n",
        "876         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
        "877         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
        "878         size = size * rval_dims[i];\n",
        "879     }\n",
        "880     free(rval_dims);\n",
        "881     return (PyObject*)rval;\n",
        "882 }\n",
        "883 \n",
        "884 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
        "885 {\n",
        "886     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
        "887     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
        "888     {\n",
        "889         Py_XDECREF(rval);\n",
        "890         rval = NULL;\n",
        "891     }\n",
        "892     else\n",
        "893     {\n",
        "894         for (int i = 0; i < self->nd; ++i)\n",
        "895         {\n",
        "896             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
        "897             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
        "898         }\n",
        "899     }\n",
        "900     return (PyObject*)rval;\n",
        "901 }\n",
        "902 \n",
        "903 /*\n",
        "904  * d0,... are the output dims\n",
        "905  * indices are a list of index to operate on\n",
        "906  *         They are int32 viewed as float32.\n",
        "907  * a is the output\n",
        "908  * b is the input\n",
        "909  * dB0, the source leading dimensions size\n",
        "910  */\n",
        "911 template <int operator_num>\n",
        "912 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
        "913                          const npy_int64* indices,\n",
        "914                          float* a,\n",
        "915                          const int sA0, const int sA1, const int sA2,\n",
        "916                          const float* b, const int dB0,\n",
        "917                          const int sB0, const int sB1, const int sB2,\n",
        "918                          int* err){\n",
        "919     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
        "920         npy_int64 idx = indices[i0];\n",
        "921         if (idx<0)\n",
        "922             idx += dB0; // To allow negative indexing.\n",
        "923         if ((idx < 0) || (idx >= dB0)){\n",
        "924             // Any value other the 0 probably work. But to be more safe, I want\n",
        "925             // to change all bits to prevent problem with concurrent write that\n",
        "926             // could cross cache line. But this should not happen with the\n",
        "927             // current code and driver.\n",
        "928             *err = 0xFFFF;\n",
        "929             continue;\n",
        "930         }\n",
        "931         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
        "932             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
        "933                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
        "934                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
        "935                 a[a_idx] = b[b_idx];\n",
        "936             }\n",
        "937         }\n",
        "938     }\n",
        "939 }\n",
        "940 \n",
        "941 // Pointor to 1 int on the device\n",
        "942 // Used in CudaNdarray_TakeFrom to tell that there is an out of bound error\n",
        "943 // When it is allocated, it should always be 0\n",
        "944 // So if there is an error, we must reset it to 0 BEFORE we raise the error\n",
        "945 // This prevent us from setting it to 0 before each use\n",
        "946 static int* err_var = NULL;\n",
        "947 \n",
        "948 // We try to be similar to the PyArray_TakeFrom function\n",
        "949 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
        "950 //TODO: support other clip mode then raise(clip, wrap)\n",
        "951 //self is the input that we copy data from.\n",
        "952 //The indices that we receive MUST be an CudaNdarray(float32)\n",
        "953 //    that is in fact a view to int64 indices\n",
        "954 PyObject*\n",
        "955 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
        "956     int verbose = 0;\n",
        "957     PyObject * indices_obj = NULL;\n",
        "958     //int axis; Default None, that mean the flattened array.\n",
        "959     PyObject * axis_obj = Py_None;\n",
        "960     PyObject * out_obj = Py_None;\n",
        "961     PyObject * clipmode_obj = NULL;\n",
        "962     int max_threads = 1; // max threads per blocks\n",
        "963 \n",
        "964     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
        "965                            &out_obj, &clipmode_obj, &max_threads))\n",
        "966         return NULL;\n",
        "967 \n",
        "968     //Check argument indices\n",
        "969     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
        "970     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
        "971     //TODO: Support ndarray of other dtype then int32\n",
        "972     //TODO: support list of indices that are not c_contiguous\n",
        "973     CudaNdarray * indices = NULL;\n",
        "974     if (CudaNdarray_Check(indices_obj)) {\n",
        "975         if (verbose) printf(\"cudandarray indices\\n\");\n",
        "976         indices = (CudaNdarray*) indices_obj;\n",
        "977         Py_INCREF(indices);\n",
        "978     } else if (0 && PyArray_Check(indices_obj)) {\n",
        "979         PyErr_SetString(PyExc_NotImplementedError, \"CudaNdarray_TakeFrom: The indices must cudandarray with float32 value.\");\n",
        "980         return NULL;\n",
        "981 \n",
        "982         if (verbose) printf(\"ndarray indices\\n\");\n",
        "983         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT32) {\n",
        "984             PyErr_SetString(PyExc_TypeError, \"CudaNdarray_TakeFrom: need a ndarray for indices with dtype int32\");\n",
        "985             return NULL;\n",
        "986         }\n",
        "987         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
        "988             PyErr_SetString(PyExc_TypeError, \"CudaNdarray_TakeFrom: need a CudaNdarray of indices with only 1 dimensions\");\n",
        "989             return NULL;\n",
        "990         }\n",
        "991         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
        "992         PyObject * indices_float32 = NULL;\n",
        "993         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
        "994                                                   float32_descr, NULL);\n",
        "995         Py_DECREF(float32_descr);\n",
        "996         if (verbose) printf(\"ndarray indices\\n\");\n",
        "997         //indices_float32 = PyArray_Cast((PyArrayObject*)indices_obj,\n",
        "998         //                              NPY_FLOAT32);\n",
        "999         //Py_INCREF(indices_float32);\n",
        "1000         if (verbose) printf(\"ndarray indices\\n\");\n",
        "1001         if (!indices_float32)\n",
        "1002             return NULL;\n",
        "1003 \n",
        "1004         indices = (CudaNdarray*) CudaNdarray_New();\n",
        "1005         if (verbose) printf(\"ndarray after new\\n\");\n",
        "1006         if (! indices){\n",
        "1007             Py_DECREF(indices_float32);\n",
        "1008             return NULL;\n",
        "1009         }\n",
        "1010         if (CudaNdarray_CopyFromArray(indices,\n",
        "1011                                       (PyArrayObject *)indices_float32)){\n",
        "1012             Py_DECREF(indices_float32);\n",
        "1013 \n",
        "1014             return NULL;\n",
        "1015         }\n",
        "1016         Py_DECREF(indices_float32);\n",
        "1017     } else {\n",
        "1018         PyErr_SetString(PyExc_TypeError,\n",
        "1019                         \"CudaNdarray_TakeFrom: need a CudaNdarray(float32) that\"\n",
        "1020                         \" is a view from int64 data for indices\");\n",
        "1021         return NULL;\n",
        "1022     }\n",
        "1023 \n",
        "1024     if (verbose) {\n",
        "1025         printf(\"indices used on the gpu\\n\");\n",
        "1026         fprint_CudaNdarray(stdout, indices);\n",
        "1027         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
        "1028         PyObject_Print(used_indices, stdout, 0);\n",
        "1029         Py_DECREF(used_indices);\n",
        "1030     }\n",
        "1031     if (verbose) printf(\"after print of object\\n\");\n",
        "1032     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
        "1033         PyErr_SetString(PyExc_NotImplementedError,\n",
        "1034                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
        "1035         Py_DECREF(indices_obj);\n",
        "1036         return NULL;\n",
        "1037     }\n",
        "1038     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
        "1039 \n",
        "1040     //Check argument axis\n",
        "1041     //TODO: implement the default and other axis\n",
        "1042     PyObject * axis_iobj = PyNumber_Long(axis_obj);\n",
        "1043     if (!axis_iobj) {\n",
        "1044         PyErr_SetString(PyExc_NotImplementedError,\"CudaNdarray_TakeFrom: axis must be convertable to a long\");\n",
        "1045         Py_DECREF(indices);\n",
        "1046         return NULL;\n",
        "1047     }\n",
        "1048     long axis = PyInt_AsLong(axis_iobj);\n",
        "1049     Py_DECREF(axis_iobj); axis_iobj=NULL;\n",
        "1050     if (axis != 0) {\n",
        "1051         PyErr_SetString(PyExc_NotImplementedError,\"CudaNdarray_TakeFrom: only axis=0 is currently supported\");\n",
        "1052         Py_DECREF(indices);\n",
        "1053         return NULL;\n",
        "1054     }\n",
        "1055 \n",
        "1056     //Check argument out_obj\n",
        "1057     CudaNdarray * out = NULL;\n",
        "1058     if (out_obj && CudaNdarray_Check(out_obj))\n",
        "1059         out = (CudaNdarray*) out_obj;\n",
        "1060     if (out && (out->nd != self->nd ||\n",
        "1061                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
        "1062         out = NULL;\n",
        "1063     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
        "1064     dims[0] = nb_indices;\n",
        "1065 \n",
        "1066     for (int i=1 ; i<self->nd ; i++) {\n",
        "1067         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
        "1068         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
        "1069             out = NULL;\n",
        "1070         }\n",
        "1071     }\n",
        "1072     if (!out) {\n",
        "1073         out = (CudaNdarray*)CudaNdarray_New();\n",
        "1074         if (!out){\n",
        "1075             Py_DECREF(indices);\n",
        "1076             free(dims);\n",
        "1077             return NULL;\n",
        "1078         }\n",
        "1079         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
        "1080             Py_DECREF(out);\n",
        "1081             Py_DECREF(indices);\n",
        "1082             free(dims);\n",
        "1083             return NULL;\n",
        "1084         }\n",
        "1085     }else {\n",
        "1086         Py_INCREF(out);\n",
        "1087     }\n",
        "1088 \n",
        "1089     //Check argument clipmode\n",
        "1090     if (clipmode_obj) {\n",
        "1091         char * clipmode = PyString_AsString(clipmode_obj);\n",
        "1092         if (! clipmode){\n",
        "1093             Py_DECREF(indices);\n",
        "1094             Py_DECREF(out);\n",
        "1095             free(dims);\n",
        "1096             return NULL;\n",
        "1097         }\n",
        "1098         if (strcmp(clipmode, \"raise\") != 0) {\n",
        "1099             PyErr_Format(PyExc_NotImplementedError,\n",
        "1100                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
        "1101                          clipmode);\n",
        "1102             Py_DECREF(indices);\n",
        "1103             Py_DECREF(out);\n",
        "1104             free(dims);\n",
        "1105             return NULL;\n",
        "1106         }\n",
        "1107     }\n",
        "1108     void (*k3)(const int, const int, const int,\n",
        "1109                const npy_int64*,\n",
        "1110                float*, const int, const int, const int,\n",
        "1111                const float*, const int,\n",
        "1112                const int, const int, const int,\n",
        "1113                int*);\n",
        "1114     k3 = k_take_3<CPY>;\n",
        "1115 \n",
        "1116     // Create the memory place that will store the error information.\n",
        "1117     if (err_var == NULL) {\n",
        "1118         err_var = (int*)device_malloc(sizeof(int));\n",
        "1119         if (!err_var) { // PyErr set by device_malloc\n",
        "1120             Py_DECREF(indices);\n",
        "1121             Py_DECREF(out);\n",
        "1122             free(dims);\n",
        "1123             return NULL;\n",
        "1124         }\n",
        "1125         cudaError_t err = cudaMemset((void*)err_var, 0, sizeof(int));\n",
        "1126         if (cudaSuccess != err) {\n",
        "1127             // Clear the error flag, cudaMemset doesn't do it.\n",
        "1128             // Currently this returns the same thing as err, but if in future\n",
        "1129             // it returns something else I still don't see why we should ignore\n",
        "1130             // it.  All we want to do here is reset the flag.\n",
        "1131             cudaGetLastError();\n",
        "1132             PyErr_Format(PyExc_RuntimeError,\n",
        "1133                          \"Error setting device error code to 0. %s\",\n",
        "1134                          cudaGetErrorString(err));\n",
        "1135             Py_DECREF(indices);\n",
        "1136             Py_DECREF(out);\n",
        "1137             free(dims);\n",
        "1138             return NULL;\n",
        "1139         }\n",
        "1140     }\n",
        "1141 \n",
        "1142     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
        "1143 \n",
        "1144     switch (self->nd) {\n",
        "1145         case 1:\n",
        "1146             {\n",
        "1147                 dim3 n_threads(1, 1, 1);\n",
        "1148                 if (verbose)\n",
        "1149                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
        "1150                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
        "1151                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
        "1152                            self->nd, cudaGetLastError(),\n",
        "1153                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
        "1154                 k3<<<n_blocks, n_threads>>>(\n",
        "1155                         dims[0],\n",
        "1156                         1,\n",
        "1157                         1,\n",
        "1158                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
        "1159                         CudaNdarray_DEV_DATA(out),\n",
        "1160                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
        "1161                         1,\n",
        "1162                         1,\n",
        "1163                         CudaNdarray_DEV_DATA(self),\n",
        "1164                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
        "1165                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
        "1166                         1,\n",
        "1167                         1,\n",
        "1168                         err_var);\n",
        "1169             }\n",
        "1170             break;\n",
        "1171         case 2:\n",
        "1172             {\n",
        "1173                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
        "1174 \n",
        "1175                 if (verbose)\n",
        "1176                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
        "1177                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
        "1178                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
        "1179                            cudaGetLastError(), self->nd,\n",
        "1180                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
        "1181 \n",
        "1182                 k3<<<n_blocks, n_threads>>>(\n",
        "1183                         dims[0], //dimensions\n",
        "1184                         dims[1],\n",
        "1185                         1,\n",
        "1186                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
        "1187                         CudaNdarray_DEV_DATA(out),\n",
        "1188                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
        "1189                         CudaNdarray_HOST_STRIDES(out)[1],\n",
        "1190                         1,\n",
        "1191                         CudaNdarray_DEV_DATA(self),\n",
        "1192                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
        "1193                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
        "1194                         CudaNdarray_HOST_STRIDES(self)[1],\n",
        "1195                         1,\n",
        "1196                         err_var);\n",
        "1197             }\n",
        "1198             break;\n",
        "1199         case 3:\n",
        "1200             {\n",
        "1201                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
        "1202                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
        "1203                 dim3 n_threads(tx, ty, 1);\n",
        "1204                 if (verbose)\n",
        "1205                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
        "1206                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
        "1207                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
        "1208                            self->nd, cudaGetLastError(),\n",
        "1209                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
        "1210                 k3<<<n_blocks, n_threads>>>(\n",
        "1211                         dims[0], //dimensions\n",
        "1212                         dims[1],\n",
        "1213                         dims[2],\n",
        "1214                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
        "1215                         CudaNdarray_DEV_DATA(out),\n",
        "1216                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
        "1217                         CudaNdarray_HOST_STRIDES(out)[1],\n",
        "1218                         CudaNdarray_HOST_STRIDES(out)[2],\n",
        "1219                         CudaNdarray_DEV_DATA(self),\n",
        "1220                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
        "1221                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
        "1222                         CudaNdarray_HOST_STRIDES(self)[1],\n",
        "1223                         CudaNdarray_HOST_STRIDES(self)[2],\n",
        "1224                         err_var);\n",
        "1225             }\n",
        "1226             break;\n",
        "1227     default:\n",
        "1228         PyErr_SetString(PyExc_NotImplementedError,\n",
        "1229                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
        "1230                         \" dimensions are currently supported\");\n",
        "1231 \n",
        "1232     }\n",
        "1233     free(dims);\n",
        "1234     CNDA_THREAD_SYNC;\n",
        "1235     cudaError_t err = cudaGetLastError();\n",
        "1236     if (cudaSuccess != err) {\n",
        "1237         PyErr_Format(PyExc_RuntimeError,\n",
        "1238                      \"Cuda error: %s: %s.\\n\",\n",
        "1239                      \"CudaNdarray_TakeFrom\",\n",
        "1240                      cudaGetErrorString(err));\n",
        "1241         Py_DECREF(indices);\n",
        "1242         Py_DECREF(out);\n",
        "1243         return NULL;\n",
        "1244     }\n",
        "1245     //-10 could be any value different then 0.\n",
        "1246     int cpu_err_var=-10;\n",
        "1247 \n",
        "1248     CNDA_BEGIN_ALLOW_THREADS\n",
        "1249     // As we execute cudaMemcpy on the default stream, it waits for all\n",
        "1250     // kernels (on all streams) to be finished before starting to copy\n",
        "1251     err = cudaMemcpy(&cpu_err_var, err_var, sizeof(int),\n",
        "1252                      cudaMemcpyDeviceToHost);\n",
        "1253     CNDA_END_ALLOW_THREADS\n",
        "1254     if (cudaSuccess != err) {\n",
        "1255         PyErr_Format(\n",
        "1256             PyExc_RuntimeError,\n",
        "1257             \"Cuda error: %s: %s when trying to get the error value.\\n\",\n",
        "1258             \"CudaNdarray_TakeFrom\",\n",
        "1259             cudaGetErrorString(err));\n",
        "1260         Py_DECREF(indices);\n",
        "1261         Py_DECREF(out);\n",
        "1262         return NULL;\n",
        "1263     }\n",
        "1264 \n",
        "1265     if (cpu_err_var != 0) {\n",
        "1266         PyErr_Format(\n",
        "1267             PyExc_IndexError,\n",
        "1268             \"Cuda error: %s: The error code on the gpu is %i.\\n\",\n",
        "1269             \"CudaNdarray_TakeFrom\",\n",
        "1270             cpu_err_var);\n",
        "1271         // Must reset it to 0 to don't reset it before each use.\n",
        "1272         err = cudaMemset((void*)err_var, 0, sizeof(int));\n",
        "1273         if (cudaSuccess != err) {\n",
        "1274             PyErr_Format(PyExc_MemoryError, \"Error setting device error code to 0 after having an index error. %s\", cudaGetErrorString(err));\n",
        "1275             Py_DECREF(indices);\n",
        "1276             Py_DECREF(out);\n",
        "1277             return NULL;\n",
        "1278         }\n",
        "1279         Py_DECREF(indices);\n",
        "1280         Py_DECREF(out);\n",
        "1281         return NULL;\n",
        "1282 \n",
        "1283     }\n",
        "1284 \n",
        "1285     Py_DECREF(indices);\n",
        "1286 \n",
        "1287     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
        "1288     return (PyObject *)out;\n",
        "1289 }\n",
        "1290 \n",
        "1291 \n",
        "1292 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
        "1293 {\n",
        "1294     int pos, stride;\n",
        "1295     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
        "1296         return NULL;\n",
        "1297     if ((pos < 0) || (pos >= self->nd))\n",
        "1298     {\n",
        "1299         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
        "1300         return NULL;\n",
        "1301     }\n",
        "1302     CudaNdarray_set_stride(self, pos, stride);\n",
        "1303     if (cnda_copy_structure_to_device(self))\n",
        "1304     {\n",
        "1305         return NULL;\n",
        "1306     }\n",
        "1307     Py_INCREF(Py_None);\n",
        "1308     return Py_None;\n",
        "1309 }\n",
        "1310 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
        "1311 {\n",
        "1312     int pos, dim;\n",
        "1313     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
        "1314         return NULL;\n",
        "1315     if ((pos < 0) || (pos >= self->nd))\n",
        "1316     {\n",
        "1317         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
        "1318         return NULL;\n",
        "1319     }\n",
        "1320     CudaNdarray_set_dim(self, pos, dim);\n",
        "1321     if (cnda_copy_structure_to_device(self))\n",
        "1322     {\n",
        "1323         return NULL;\n",
        "1324     }\n",
        "1325     Py_INCREF(Py_None);\n",
        "1326     return Py_None;\n",
        "1327 }\n",
        "1328 \n",
        "1329 static PyObject *\n",
        "1330 CudaNdarray_exp(CudaNdarray* self)\n",
        "1331 {\n",
        "1332     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
        "1333     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
        "1334     {\n",
        "1335         Py_XDECREF(rval);\n",
        "1336         return NULL;\n",
        "1337     }\n",
        "1338     unsigned int size = 1;\n",
        "1339     for (int i = 0; i < self->nd; i++)\n",
        "1340     {\n",
        "1341         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
        "1342     }\n",
        "1343     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
        "1344     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
        "1345     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
        "1346             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
        "1347             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
        "1348 \n",
        "1349     //TODO: don't do this right away, do it when we need the result\n",
        "1350     CNDA_THREAD_SYNC;\n",
        "1351     cudaError_t err = cudaGetLastError();\n",
        "1352     if( cudaSuccess != err)\n",
        "1353     {\n",
        "1354         Py_DECREF(rval);\n",
        "1355         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
        "1356         return NULL;\n",
        "1357     }\n",
        "1358 \n",
        "1359     return (PyObject*)rval;\n",
        "1360 }\n",
        "1361 \n",
        "1362 static PyMethodDef CudaNdarray_methods[] =\n",
        "1363 {\n",
        "1364     {\"__array__\",\n",
        "1365         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
        "1366         \"Copy from the device to a numpy ndarray\"},\n",
        "1367     {\"__copy__\",\n",
        "1368         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
        "1369         \"Create a shallow copy of this object. used by module copy\"},\n",
        "1370     {\"__deepcopy__\",\n",
        "1371         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
        "1372         \"Create a copy of this object\"},\n",
        "1373     {\"zeros\",\n",
        "1374         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
        "1375         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
        "1376     {\"copy\",\n",
        "1377         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
        "1378         \"Create a copy of this object\"},\n",
        "1379     {\"is_c_contiguous\",\n",
        "1380         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
        "1381         \"Return True is the object is c contiguous. False otherwise.\"},\n",
        "1382     {\"reduce_sum\",\n",
        "1383         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
        "1384         \"Reduce over the given dimensions by summation\"},\n",
        "1385     {\"exp\",\n",
        "1386         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
        "1387         \"Return the exponential of all elements\"},\n",
        "1388     {\"reshape\",\n",
        "1389         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
        "1390         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
        "1391             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
        "1392     {\"view\",\n",
        "1393         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
        "1394         \"Return an alias of this ndarray\"},\n",
        "1395     {\"_set_stride\",\n",
        "1396         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
        "1397         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
        "1398     {\"take\",\n",
        "1399         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
        "1400         \"Equivalent of numpy.take\"},\n",
        "1401     {\"_set_shape_i\",\n",
        "1402         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
        "1403         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
        "1404     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
        "1405 };\n",
        "1406 \n",
        "1407 \n",
        "1408 ////////////////////\n",
        "1409 // Number protocol\n",
        "1410 ////////////////////\n",
        "1411 \n",
        "1412 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
        "1413     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "1414     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
        "1415 \n",
        "1416     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
        "1417         dest[i] = a[i] + b[i];\n",
        "1418     }\n",
        "1419 }\n",
        "1420 \n",
        "1421 // Will be called by __add__ in Python\n",
        "1422 static PyObject *\n",
        "1423 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
        "1424 {\n",
        "1425     if (! CudaNdarray_Check(py_self)) {\n",
        "1426         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
        "1427         return NULL;\n",
        "1428     }\n",
        "1429     if (! CudaNdarray_Check(py_other)) {\n",
        "1430         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
        "1431         return NULL;\n",
        "1432     }\n",
        "1433     CudaNdarray * self = (CudaNdarray *)py_self;\n",
        "1434     CudaNdarray * other = (CudaNdarray *)py_other;\n",
        "1435     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
        "1436         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
        "1437         return NULL;\n",
        "1438     }\n",
        "1439 \n",
        "1440     //standard elemwise size checks\n",
        "1441     if (self->nd != other->nd)\n",
        "1442     {\n",
        "1443         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
        "1444         return NULL;\n",
        "1445     }\n",
        "1446     //standard elemwise dim checks\n",
        "1447     unsigned int size = 1;\n",
        "1448     for (int i = 0; i< self->nd; ++i)\n",
        "1449     {\n",
        "1450         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
        "1451         {\n",
        "1452             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
        "1453             return NULL;\n",
        "1454         }\n",
        "1455         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
        "1456     }\n",
        "1457     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
        "1458     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
        "1459     {\n",
        "1460         Py_XDECREF(rval);\n",
        "1461         return NULL;\n",
        "1462     }\n",
        "1463 \n",
        "1464     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
        "1465       return (PyObject *) rval;\n",
        "1466     }\n",
        "1467 \n",
        "1468     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
        "1469     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
        "1470     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
        "1471             self->devdata, other->devdata, rval->devdata, size);\n",
        "1472     CNDA_THREAD_SYNC;\n",
        "1473     cudaError_t err = cudaGetLastError();\n",
        "1474     if( cudaSuccess != err)\n",
        "1475     {\n",
        "1476         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
        "1477         Py_DECREF(rval);\n",
        "1478         return NULL;\n",
        "1479     }\n",
        "1480     return (PyObject *) rval;\n",
        "1481 }\n",
        "1482 \n",
        "1483 template <int operator_num>\n",
        "1484 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
        "1485         float* a, const int sA0, const int sA1, const int sA2,\n",
        "1486         const float* b, const int sB0, const int sB1, const int sB2){\n",
        "1487     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
        "1488         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
        "1489             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
        "1490                 switch (operator_num)\n",
        "1491                 {\n",
        "1492                   case IADD:\n",
        "1493                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
        "1494                     break;\n",
        "1495                   case IDIV:\n",
        "1496                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
        "1497                     break;\n",
        "1498                   case CPY:\n",
        "1499                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
        "1500                     break;\n",
        "1501                 }\n",
        "1502             }\n",
        "1503         }\n",
        "1504     }\n",
        "1505 }\n",
        "1506 \n",
        "1507 template <int operator_num>\n",
        "1508 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
        "1509                          float* a, const int sA0, const int sA1,\n",
        "1510                          const int sA2, const int sA3,\n",
        "1511                          const float* b, const int sB0, const int sB1,\n",
        "1512                          const int sB2, const int sB3){\n",
        "1513     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
        "1514         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
        "1515             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
        "1516                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
        "1517                     switch (operator_num) {\n",
        "1518                         case IADD:\n",
        "1519                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
        "1520                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
        "1521                             break;\n",
        "1522                         case IDIV:\n",
        "1523                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
        "1524                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
        "1525                             break;\n",
        "1526                         case CPY:\n",
        "1527                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
        "1528                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
        "1529                             break;\n",
        "1530                     }\n",
        "1531                 }\n",
        "1532             }\n",
        "1533         }\n",
        "1534     }\n",
        "1535 }\n",
        "1536 \n",
        "1537 template <int operator_num>\n",
        "1538 __global__ void k_ielem_6(const int d0, const int d1,\n",
        "1539                           const int d2, const int d3,\n",
        "1540                           const int d4, const int d5,\n",
        "1541                           float* a, const int sA0, const int sA1,\n",
        "1542                           const int sA2, const int sA3,\n",
        "1543                           const int sA4, const int sA5,\n",
        "1544                           const float* b, const int sB0, const int sB1,\n",
        "1545                           const int sB2, const int sB3,\n",
        "1546                           const int sB4, const int sB5\n",
        "1547                           ){\n",
        "1548     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
        "1549         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
        "1550             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
        "1551                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
        "1552                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
        "1553                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
        "1554                             switch (operator_num) {\n",
        "1555                             case IADD:\n",
        "1556                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
        "1557                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
        "1558                                 break;\n",
        "1559                             case IDIV:\n",
        "1560                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
        "1561                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
        "1562                                 break;\n",
        "1563                             case CPY:\n",
        "1564                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
        "1565                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
        "1566                                 break;\n",
        "1567                             }\n",
        "1568                         }\n",
        "1569                     }\n",
        "1570                 }\n",
        "1571             }\n",
        "1572         }\n",
        "1573     }\n",
        "1574 }\n",
        "1575 \n",
        "1576 /*\n",
        "1577 CudaNdarray_inplace_elemwise\n",
        "1578 Compute elemwise, working inplace on A.\n",
        "1579 Currently implemented A / B, A + B and A = B\n",
        "1580 (the last is not tested and not used!)\n",
        "1581 \n",
        "1582 py_self - the CudaNdarray that we'll modify (A)\n",
        "1583 py_other - the other argument (B)\n",
        "1584 fct_nb - which operation to perform (operator_t)\n",
        "1585 \n",
        "1586 Returns 0 on success.\n",
        "1587 Returns -1 on failure, and sets Python exception.\n",
        "1588 \n",
        "1589 */\n",
        "1590 int\n",
        "1591 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
        "1592 {\n",
        "1593     int verbose = 0;\n",
        "1594     void (*k3)(const int, const int, const int,\n",
        "1595                     float*, const int, const int, const int,\n",
        "1596                     const float*, const int, const int, const int);\n",
        "1597     void (*k4)(const int, const int, const int, const int,\n",
        "1598                     float*, const int, const int,\n",
        "1599                     const int, const int,\n",
        "1600                     const float*, const int, const int,\n",
        "1601                     const int, const int);\n",
        "1602     void (*k6)(const int, const int,\n",
        "1603                const int, const int,\n",
        "1604                const int, const int,\n",
        "1605                float*, const int, const int,\n",
        "1606                const int, const int,\n",
        "1607                const int, const int,\n",
        "1608                const float*, const int, const int,\n",
        "1609                const int, const int,\n",
        "1610                const int, const int);\n",
        "1611     switch (fct_nb)\n",
        "1612     {\n",
        "1613         case IADD:\n",
        "1614             k3 = k_ielem_3<IADD>;\n",
        "1615             k4 = k_ielem_4<IADD>;\n",
        "1616             k6 = k_ielem_6<IADD>;\n",
        "1617             break;\n",
        "1618         case IDIV:\n",
        "1619             k3 = k_ielem_3<IDIV>;\n",
        "1620             k4 = k_ielem_4<IDIV>;\n",
        "1621             k6 = k_ielem_6<IDIV>;\n",
        "1622             break;\n",
        "1623         case CPY:\n",
        "1624             k3 = k_ielem_3<CPY>;\n",
        "1625             k4 = k_ielem_4<CPY>;\n",
        "1626             k6 = k_ielem_6<CPY>;\n",
        "1627             break;\n",
        "1628         default:\n",
        "1629             assert (0);\n",
        "1630             PyErr_Format(\n",
        "1631                 PyExc_TypeError,\n",
        "1632                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
        "1633                 (int)fct_nb);\n",
        "1634             return -1;\n",
        "1635     }\n",
        "1636     if (!CudaNdarray_Check(py_self)) {\n",
        "1637         PyErr_SetString(\n",
        "1638             PyExc_TypeError,\n",
        "1639             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
        "1640         return -1;\n",
        "1641     }\n",
        "1642     CudaNdarray * new_other = NULL;\n",
        "1643     if (!CudaNdarray_Check(py_other)) {\n",
        "1644         new_other = (CudaNdarray*) CudaNdarray_New();\n",
        "1645         if(!new_other)\n",
        "1646         {\n",
        "1647             return -1;\n",
        "1648         }\n",
        "1649         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
        "1650         {\n",
        "1651             Py_XDECREF(new_other);\n",
        "1652             return -1;\n",
        "1653         }\n",
        "1654         py_other = (PyObject *) new_other;\n",
        "1655     }\n",
        "1656 \n",
        "1657     CudaNdarray * self = (CudaNdarray *)py_self;\n",
        "1658     CudaNdarray * other = (CudaNdarray *)py_other;\n",
        "1659 \n",
        "1660     if (verbose)\n",
        "1661     {\n",
        "1662         fprintf(stderr,\n",
        "1663             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
        "1664             self->nd, other->nd);\n",
        "1665     }\n",
        "1666 \n",
        "1667     //standard elemwise nb dim checks\n",
        "1668     if (self->nd < other->nd)\n",
        "1669     {\n",
        "1670         PyErr_Format(\n",
        "1671             PyExc_TypeError,\n",
        "1672             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
        "1673             \" same number of dimensions then the source. Got %d and %d.\",\n",
        "1674             self->nd, other->nd);\n",
        "1675         Py_XDECREF(new_other);\n",
        "1676         return -1;\n",
        "1677     }\n",
        "1678 \n",
        "1679     //broadcast to the same number of dimensions.\n",
        "1680     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
        "1681     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
        "1682     int added_dims = self->nd - other->nd;\n",
        "1683     // Add the added broadcasted dimensions\n",
        "1684     for (int i = 0; i< added_dims; ++i)\n",
        "1685     {\n",
        "1686         other_dims[i] = 1;\n",
        "1687         other_strides[i] = 0;\n",
        "1688     }\n",
        "1689     // Copy the existing dimensions\n",
        "1690     for (int i = 0; i< other->nd; ++i)\n",
        "1691     {\n",
        "1692         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
        "1693         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
        "1694     }\n",
        "1695 \n",
        "1696     //standard elemwise dim checks\n",
        "1697     unsigned int size = 1;\n",
        "1698     for (int i = 0; i< self->nd; ++i)\n",
        "1699     {\n",
        "1700         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
        "1701             && (other_dims[i] != 1))\n",
        "1702         {\n",
        "1703             PyErr_SetString(\n",
        "1704                 PyExc_ValueError,\n",
        "1705                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
        "1706             Py_XDECREF(new_other);\n",
        "1707             return -1;\n",
        "1708         }\n",
        "1709         // if we're broadcasting other, then make sure it has stride 0\n",
        "1710         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
        "1711             || (other_strides[i] == 0));\n",
        "1712         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
        "1713     }\n",
        "1714 \n",
        "1715     if (size==0)\n",
        "1716     {\n",
        "1717         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
        "1718         if (!(other_size == 0 || other_size == 1))\n",
        "1719         {\n",
        "1720             PyErr_SetString(\n",
        "1721                 PyExc_ValueError,\n",
        "1722                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
        "1723                 \" un-initialized array when the new value have more than\"\n",
        "1724                 \" 0 or 1 broadcastable dimensions\");\n",
        "1725             Py_XDECREF(new_other);\n",
        "1726             return 0;\n",
        "1727         }\n",
        "1728         Py_XDECREF(new_other);\n",
        "1729         return 0;\n",
        "1730     }\n",
        "1731 \n",
        "1732     switch(self->nd)\n",
        "1733     {\n",
        "1734         case 0:\n",
        "1735             {\n",
        "1736                 dim3 n_blocks(1, 1, 1);\n",
        "1737                 dim3 n_threads(1);\n",
        "1738                 k3<<<n_blocks, n_threads>>>(\n",
        "1739                         1, //d0\n",
        "1740                         1, //d1\n",
        "1741                         1, //d2\n",
        "1742                         CudaNdarray_DEV_DATA(self),\n",
        "1743                         1, //strides\n",
        "1744                         1,\n",
        "1745                         1,\n",
        "1746                         CudaNdarray_DEV_DATA(other),\n",
        "1747                         1, //strides\n",
        "1748                         1,\n",
        "1749                         1);\n",
        "1750                 CNDA_THREAD_SYNC;\n",
        "1751                 cudaError_t err = cudaGetLastError();\n",
        "1752                 if (cudaSuccess != err)\n",
        "1753                 {\n",
        "1754                     PyErr_Format(\n",
        "1755                         PyExc_RuntimeError,\n",
        "1756                         \"Cuda error: %s: %s.\\n\",\n",
        "1757                         \"k3\",\n",
        "1758                         cudaGetErrorString(err));\n",
        "1759                     Py_XDECREF(new_other);\n",
        "1760                     return -1;\n",
        "1761                 }\n",
        "1762             }\n",
        "1763             break;\n",
        "1764         case 1:\n",
        "1765             {\n",
        "1766                 dim3 n_blocks(1, 1, 1);\n",
        "1767                 dim3 n_threads(\n",
        "1768                         std::min(\n",
        "1769                             CudaNdarray_HOST_DIMS(self)[0],\n",
        "1770                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
        "1771                 k3<<<n_blocks, n_threads>>>(\n",
        "1772                         1, //dimensions\n",
        "1773                         1,\n",
        "1774                         CudaNdarray_HOST_DIMS(self)[0],\n",
        "1775                         CudaNdarray_DEV_DATA(self),\n",
        "1776                         1, //strides\n",
        "1777                         1,\n",
        "1778                         CudaNdarray_HOST_STRIDES(self)[0],\n",
        "1779                         CudaNdarray_DEV_DATA(other),\n",
        "1780                         1, //strides\n",
        "1781                         1,\n",
        "1782                         other_strides[0]);\n",
        "1783                 CNDA_THREAD_SYNC;\n",
        "1784                 cudaError_t err = cudaGetLastError();\n",
        "1785                 if (cudaSuccess != err)\n",
        "1786                 {\n",
        "1787                     PyErr_Format(\n",
        "1788                         PyExc_RuntimeError,\n",
        "1789                         \"Cuda error: %s: %s.\\n\",\n",
        "1790                         \"k3\",\n",
        "1791                         cudaGetErrorString(err));\n",
        "1792                     Py_XDECREF(new_other);\n",
        "1793                     return -1;\n",
        "1794                 }\n",
        "1795             }\n",
        "1796             break;\n",
        "1797         case 2:\n",
        "1798             {\n",
        "1799                 //TODO:  if both self and other are f-contiguous\n",
        "1800                 //       Then flip the block and thread dimensions\n",
        "1801                 //       to make contiguous reads & writes\n",
        "1802                 dim3 n_blocks(1,\n",
        "1803                         std::min(\n",
        "1804                             CudaNdarray_HOST_DIMS(self)[0],\n",
        "1805                             NUM_VECTOR_OP_BLOCKS));\n",
        "1806                 dim3 n_threads(\n",
        "1807                         std::min(\n",
        "1808                             CudaNdarray_HOST_DIMS(self)[1],\n",
        "1809                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
        "1810                 k3<<<n_blocks, n_threads>>>(1,\n",
        "1811                         CudaNdarray_HOST_DIMS(self)[0],\n",
        "1812                         CudaNdarray_HOST_DIMS(self)[1],\n",
        "1813                         CudaNdarray_DEV_DATA(self),\n",
        "1814                         1,\n",
        "1815                         CudaNdarray_HOST_STRIDES(self)[0],\n",
        "1816                         CudaNdarray_HOST_STRIDES(self)[1],\n",
        "1817                         CudaNdarray_DEV_DATA(other),\n",
        "1818                         1,\n",
        "1819                         other_strides[0],\n",
        "1820                         other_strides[1]);\n",
        "1821                 CNDA_THREAD_SYNC;\n",
        "1822                 cudaError_t err = cudaGetLastError();\n",
        "1823                 if (cudaSuccess != err)\n",
        "1824                 {\n",
        "1825                     PyErr_Format(\n",
        "1826                         PyExc_RuntimeError,\n",
        "1827                         \"Cuda error: %s: %s.\\n\",\n",
        "1828                         \"k3\",\n",
        "1829                         cudaGetErrorString(err));\n",
        "1830                     Py_XDECREF(new_other);\n",
        "1831                     return -1;\n",
        "1832                 }\n",
        "1833             }\n",
        "1834             break;\n",
        "1835         case 3:\n",
        "1836             {\n",
        "1837                 //TODO:  Dimshuffle so that at least one of the arrays\n",
        "1838                 //       has a contiguous dimension on the thread idx.\n",
        "1839                 dim3 n_blocks(\n",
        "1840                         std::min(\n",
        "1841                             CudaNdarray_HOST_DIMS(self)[0],\n",
        "1842                             NUM_VECTOR_OP_BLOCKS),\n",
        "1843                         CudaNdarray_HOST_DIMS(self)[1]);\n",
        "1844                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
        "1845                     n_blocks.y /= 2;\n",
        "1846                 dim3 n_threads(\n",
        "1847                         std::min(\n",
        "1848                             CudaNdarray_HOST_DIMS(self)[2],\n",
        "1849                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
        "1850                 k3<<<n_blocks, n_threads>>>(\n",
        "1851                         CudaNdarray_HOST_DIMS(self)[0],\n",
        "1852                         CudaNdarray_HOST_DIMS(self)[1],\n",
        "1853                         CudaNdarray_HOST_DIMS(self)[2],\n",
        "1854                         CudaNdarray_DEV_DATA(self),\n",
        "1855                         CudaNdarray_HOST_STRIDES(self)[0],\n",
        "1856                         CudaNdarray_HOST_STRIDES(self)[1],\n",
        "1857                         CudaNdarray_HOST_STRIDES(self)[2],\n",
        "1858                         CudaNdarray_DEV_DATA(other),\n",
        "1859                         other_strides[0],\n",
        "1860                         other_strides[1],\n",
        "1861                         other_strides[2]);\n",
        "1862                 CNDA_THREAD_SYNC;\n",
        "1863                 cudaError_t err = cudaGetLastError();\n",
        "1864                 if (cudaSuccess != err)\n",
        "1865                 {\n",
        "1866                     PyErr_Format(\n",
        "1867                         PyExc_RuntimeError,\n",
        "1868                         \"Cuda error: %s: %s.\\n\",\n",
        "1869                         \"k3\",\n",
        "1870                         cudaGetErrorString(err));\n",
        "1871                     Py_XDECREF(new_other);\n",
        "1872                     return -1;\n",
        "1873                 }\n",
        "1874             }\n",
        "1875             break;\n",
        "1876         case 4:\n",
        "1877             {\n",
        "1878                 dim3 n_blocks(\n",
        "1879                         std::min(\n",
        "1880                             CudaNdarray_HOST_DIMS(self)[0],\n",
        "1881                             NUM_VECTOR_OP_BLOCKS),\n",
        "1882                         CudaNdarray_HOST_DIMS(self)[1]\n",
        "1883                         );\n",
        "1884                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
        "1885                     n_blocks.y /= 2;\n",
        "1886                 dim3 n_threads(\n",
        "1887                         std::min(\n",
        "1888                             CudaNdarray_HOST_DIMS(self)[2],\n",
        "1889                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
        "1890                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
        "1891                             );\n",
        "1892                 k4<<<n_blocks, n_threads>>>(\n",
        "1893                         CudaNdarray_HOST_DIMS(self)[0],\n",
        "1894                         CudaNdarray_HOST_DIMS(self)[1],\n",
        "1895                         CudaNdarray_HOST_DIMS(self)[2],\n",
        "1896                         CudaNdarray_HOST_DIMS(self)[3],\n",
        "1897                         CudaNdarray_DEV_DATA(self),\n",
        "1898                         CudaNdarray_HOST_STRIDES(self)[0],\n",
        "1899                         CudaNdarray_HOST_STRIDES(self)[1],\n",
        "1900                         CudaNdarray_HOST_STRIDES(self)[2],\n",
        "1901                         CudaNdarray_HOST_STRIDES(self)[3],\n",
        "1902                         CudaNdarray_DEV_DATA(other),\n",
        "1903                         other_strides[0],\n",
        "1904                         other_strides[1],\n",
        "1905                         other_strides[2],\n",
        "1906                         other_strides[3]);\n",
        "1907                 CNDA_THREAD_SYNC;\n",
        "1908                 cudaError_t err = cudaGetLastError();\n",
        "1909                 if (cudaSuccess != err)\n",
        "1910                 {\n",
        "1911                     PyErr_Format(\n",
        "1912                         PyExc_RuntimeError,\n",
        "1913                         \"Cuda error: %s: %s.\\n\",\n",
        "1914                         \"k4\",\n",
        "1915                         cudaGetErrorString(err));\n",
        "1916                     Py_XDECREF(new_other);\n",
        "1917                     return -1;\n",
        "1918                 }\n",
        "1919             }\n",
        "1920             break;\n",
        "1921         case 5:\n",
        "1922             {\n",
        "1923                 dim3 n_blocks(\n",
        "1924                         std::min(\n",
        "1925                             CudaNdarray_HOST_DIMS(self)[1],\n",
        "1926                             NUM_VECTOR_OP_BLOCKS),\n",
        "1927                         CudaNdarray_HOST_DIMS(self)[2]);\n",
        "1928                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
        "1929                     n_blocks.y /= 2;\n",
        "1930                 dim3 n_threads(\n",
        "1931                         std::min(\n",
        "1932                             CudaNdarray_HOST_DIMS(self)[3],\n",
        "1933                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
        "1934                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
        "1935                     );\n",
        "1936                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
        "1937                 {\n",
        "1938                      k4<<<n_blocks, n_threads>>>(\n",
        "1939                             CudaNdarray_HOST_DIMS(self)[1],\n",
        "1940                             CudaNdarray_HOST_DIMS(self)[2],\n",
        "1941                             CudaNdarray_HOST_DIMS(self)[3],\n",
        "1942                             CudaNdarray_HOST_DIMS(self)[4],\n",
        "1943                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
        "1944                             CudaNdarray_HOST_STRIDES(self)[1],\n",
        "1945                             CudaNdarray_HOST_STRIDES(self)[2],\n",
        "1946                             CudaNdarray_HOST_STRIDES(self)[3],\n",
        "1947                             CudaNdarray_HOST_STRIDES(self)[4],\n",
        "1948                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
        "1949                             other_strides[1],\n",
        "1950                             other_strides[2],\n",
        "1951                             other_strides[3],\n",
        "1952                             other_strides[4]);\n",
        "1953                     CNDA_THREAD_SYNC;\n",
        "1954                     cudaError_t err = cudaGetLastError();\n",
        "1955                     if( cudaSuccess != err)\n",
        "1956                     {\n",
        "1957                         PyErr_Format(\n",
        "1958                             PyExc_RuntimeError,\n",
        "1959                             \"Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
        "1960                             \"k5 with loop over k4\",\n",
        "1961                             cudaGetErrorString(err),\n",
        "1962                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
        "1963                         Py_XDECREF(new_other);\n",
        "1964                         return -1;\n",
        "1965                     }\n",
        "1966                 }\n",
        "1967             }\n",
        "1968             break;\n",
        "1969         case 6:\n",
        "1970             {\n",
        "1971                 dim3 n_blocks(\n",
        "1972                         std::min(\n",
        "1973                             CudaNdarray_HOST_DIMS(self)[0],\n",
        "1974                             NUM_VECTOR_OP_BLOCKS),\n",
        "1975                         CudaNdarray_HOST_DIMS(self)[1],\n",
        "1976                         CudaNdarray_HOST_DIMS(self)[2]\n",
        "1977                         );\n",
        "1978                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
        "1979                     n_blocks.y /= 2;\n",
        "1980                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
        "1981                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
        "1982                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
        "1983                 //    n_blocks.z /= 2;\n",
        "1984                 n_blocks.z = 1;\n",
        "1985                 dim3 n_threads(\n",
        "1986                         std::min(\n",
        "1987                             CudaNdarray_HOST_DIMS(self)[3],\n",
        "1988                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
        "1989                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
        "1990                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
        "1991                             );\n",
        "1992                 k6<<<n_blocks, n_threads>>>(\n",
        "1993                         CudaNdarray_HOST_DIMS(self)[0],\n",
        "1994                         CudaNdarray_HOST_DIMS(self)[1],\n",
        "1995                         CudaNdarray_HOST_DIMS(self)[2],\n",
        "1996                         CudaNdarray_HOST_DIMS(self)[3],\n",
        "1997                         CudaNdarray_HOST_DIMS(self)[4],\n",
        "1998                         CudaNdarray_HOST_DIMS(self)[5],\n",
        "1999                         CudaNdarray_DEV_DATA(self),\n",
        "2000                         CudaNdarray_HOST_STRIDES(self)[0],\n",
        "2001                         CudaNdarray_HOST_STRIDES(self)[1],\n",
        "2002                         CudaNdarray_HOST_STRIDES(self)[2],\n",
        "2003                         CudaNdarray_HOST_STRIDES(self)[3],\n",
        "2004                         CudaNdarray_HOST_STRIDES(self)[4],\n",
        "2005                         CudaNdarray_HOST_STRIDES(self)[5],\n",
        "2006                         CudaNdarray_DEV_DATA(other),\n",
        "2007                         other_strides[0],\n",
        "2008                         other_strides[1],\n",
        "2009                         other_strides[2],\n",
        "2010                         other_strides[3],\n",
        "2011                         other_strides[4],\n",
        "2012                         other_strides[5]);\n",
        "2013                 CNDA_THREAD_SYNC;\n",
        "2014                 cudaError_t err = cudaGetLastError();\n",
        "2015                 if (cudaSuccess != err)\n",
        "2016                 {\n",
        "2017                     PyErr_Format(\n",
        "2018                         PyExc_RuntimeError,\n",
        "2019                         \"Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
        "2020                         \"k6\",\n",
        "2021                         cudaGetErrorString(err),\n",
        "2022                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
        "2023                         (long) n_threads.x);\n",
        "2024                     Py_XDECREF(new_other);\n",
        "2025                     return -1;\n",
        "2026                 }\n",
        "2027             }\n",
        "2028             break;\n",
        "2029         default:\n",
        "2030         {\n",
        "2031             PyErr_Format(\n",
        "2032                 PyExc_NotImplementedError,\n",
        "2033                 \"inplace_elemwise w nd=%i\\n\",\n",
        "2034                 self->nd);\n",
        "2035             Py_XDECREF(new_other);\n",
        "2036             return -1;\n",
        "2037         }\n",
        "2038     }\n",
        "2039     if (verbose)\n",
        "2040         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
        "2041     Py_XDECREF(new_other);\n",
        "2042     return 0;\n",
        "2043 }\n",
        "2044 \n",
        "2045 /*\n",
        "2046  * We need this inplace Add to support IncSubTensor\n",
        "2047  * It returns py_self on success with an additional reference. Else NULL.\n",
        "2048  */\n",
        "2049 // Will be called by __iadd__ in Python\n",
        "2050 PyObject *\n",
        "2051 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
        "2052 {\n",
        "2053     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
        "2054     {\n",
        "2055         return NULL;\n",
        "2056     }\n",
        "2057     Py_INCREF(py_self);\n",
        "2058     return py_self;\n",
        "2059 }\n",
        "2060 \n",
        "2061 /*\n",
        "2062  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
        "2063  * It returns py_self on success with an additional reference. Else NULL.\n",
        "2064  */\n",
        "2065 // Will be called by __idiv__ in Python\n",
        "2066 static PyObject *\n",
        "2067 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
        "2068 {\n",
        "2069     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
        "2070     {\n",
        "2071         return NULL;\n",
        "2072     }\n",
        "2073     Py_INCREF(py_self);\n",
        "2074     return py_self;\n",
        "2075 }\n",
        "2076 \n",
        "2077 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
        "2078 #if PY_MAJOR_VERSION == 3\n",
        "2079 static PyNumberMethods CudaNdarrayNumberMethods =\n",
        "2080 {\n",
        "2081     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
        "2082     0,  //binaryfunc nb_subtract;\n",
        "2083     0,  //binaryfunc nb_multiply;\n",
        "2084     0,  //binaryfunc nb_remainder;\n",
        "2085     0,  //binaryfunc nb_divmod;\n",
        "2086     0,  //ternaryfunc nb_power;\n",
        "2087     0,  //unaryfunc nb_negative;\n",
        "2088     0,  //unaryfunc nb_positive;\n",
        "2089     0,  //unaryfunc nb_absolute;\n",
        "2090     0,  //inquiry nb_bool;\n",
        "2091     0,  //unaryfunc nb_invert;\n",
        "2092     0,  //binaryfunc nb_lshift;\n",
        "2093     0,  //binaryfunc nb_rshift;\n",
        "2094     0,  //binaryfunc nb_and;\n",
        "2095     0,  //binaryfunc nb_xor;\n",
        "2096     0,  //binaryfunc nb_or;\n",
        "2097     0,  //unaryfunc nb_int;\n",
        "2098     0,  //void *nb_reserved;\n",
        "2099     0,  //unaryfunc nb_float;\n",
        "2100 \n",
        "2101     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
        "2102     0,  //binaryfunc nb_inplace_subtract;\n",
        "2103     0,  //binaryfunc nb_inplace_multiply;\n",
        "2104     0,  //binaryfunc nb_inplace_remainder;\n",
        "2105     0,  //ternaryfunc nb_inplace_power;\n",
        "2106     0,  //binaryfunc nb_inplace_lshift;\n",
        "2107     0,  //binaryfunc nb_inplace_rshift;\n",
        "2108     0,  //binaryfunc nb_inplace_and;\n",
        "2109     0,  //binaryfunc nb_inplace_xor;\n",
        "2110     0,  //binaryfunc nb_inplace_or;\n",
        "2111 \n",
        "2112     0,  //binaryfunc nb_floor_divide;\n",
        "2113     0,  //binaryfunc nb_true_divide;\n",
        "2114     0,  //binaryfunc nb_inplace_floor_divide;\n",
        "2115     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
        "2116 \n",
        "2117     0,  //unaryfunc nb_index\n",
        "2118 };\n",
        "2119 #else\n",
        "2120 static PyNumberMethods CudaNdarrayNumberMethods =\n",
        "2121 {\n",
        "2122     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
        "2123     0,  //binaryfunc nb_subtract;      __sub__\n",
        "2124     0,  //binaryfunc nb_multiply;      __mul__\n",
        "2125     0,  //binaryfunc nb_divide;        __div__\n",
        "2126     0,  //binaryfunc nb_remainder;     __mod__\n",
        "2127     0,  //binaryfunc nb_divmod;        __divmod__\n",
        "2128     0,  //ternaryfunc nb_power;        __pow__\n",
        "2129     0,  //unaryfunc nb_negative;       __neg__\n",
        "2130     0,  //unaryfunc nb_positive;       __pos__\n",
        "2131     0,  //unaryfunc nb_absolute;       __abs__\n",
        "2132     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
        "2133     0,  //unaryfunc nb_invert;         __invert__\n",
        "2134     0,  //binaryfunc nb_lshift;        __lshift__\n",
        "2135     0,  //binaryfunc nb_rshift;        __rshift__\n",
        "2136     0,  //binaryfunc nb_and;           __and__\n",
        "2137     0,  //binaryfunc nb_xor;           __xor__\n",
        "2138     0,  //binaryfunc nb_or;            __or__\n",
        "2139     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
        "2140     0,  //unaryfunc nb_int;            __int__\n",
        "2141     0,  //unaryfunc nb_long;           __long__\n",
        "2142     0,  //unaryfunc nb_float;          __float__\n",
        "2143     0,  //unaryfunc nb_oct;            __oct__\n",
        "2144     0,  //unaryfunc nb_hex;            __hex__\n",
        "2145 \n",
        "2146     /* Added in release 2.0 */\n",
        "2147     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
        "2148     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
        "2149     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
        "2150     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
        "2151     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
        "2152     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
        "2153     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
        "2154     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
        "2155     0,  //binaryfunc nb_inplace_and;           __iand__\n",
        "2156     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
        "2157     0,  //binaryfunc nb_inplace_or;            __ior__\n",
        "2158 \n",
        "2159     /* Added in release 2.2 */\n",
        "2160     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
        "2161     0,  //binaryfunc nb_true_divide;           __truediv__\n",
        "2162     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
        "2163     0,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
        "2164 \n",
        "2165 #if PY_MINOR_VERSION > 4\n",
        "2166     /* Added in release 2.5 */\n",
        "2167     0  //unaryfunc nb_index;  __index__\n",
        "2168 #endif\n",
        "2169 };\n",
        "2170 #endif\n",
        "2171 \n",
        "2172 \n",
        "2173 /////////////////////\n",
        "2174 // Mapping protocol\n",
        "2175 /////////////////////\n",
        "2176 \n",
        "2177 // Will by called by __len__ in Python\n",
        "2178 static Py_ssize_t\n",
        "2179 CudaNdarray_len(PyObject * py_self)\n",
        "2180 {\n",
        "2181     CudaNdarray * self = (CudaNdarray*) py_self;\n",
        "2182     if (self->nd <= 0)\n",
        "2183     {\n",
        "2184         return (Py_ssize_t) 0;\n",
        "2185     }\n",
        "2186     else\n",
        "2187     {\n",
        "2188         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
        "2189     }\n",
        "2190 }\n",
        "2191 \n",
        "2192 // Will by called by __getitem__ in Python\n",
        "2193 PyObject *\n",
        "2194 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
        "2195 {\n",
        "2196     int verbose = 0;\n",
        "2197     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
        "2198     CudaNdarray * self = (CudaNdarray*) py_self;\n",
        "2199     PyObject * py_rval = NULL;\n",
        "2200     CudaNdarray * rval = NULL;\n",
        "2201     PyObject * intobj = NULL;\n",
        "2202 \n",
        "2203     //PyObject_Print(key, stderr, 0);\n",
        "2204 \n",
        "2205     if (key == Py_Ellipsis)\n",
        "2206     {\n",
        "2207         Py_INCREF(py_self);\n",
        "2208         return py_self;\n",
        "2209     }\n",
        "2210     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
        "2211     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
        "2212     {\n",
        "2213         int d_idx = PyInt_AsLong(intobj);\n",
        "2214         Py_DECREF(intobj); intobj=NULL;\n",
        "2215         //int d_idx = PyInt_AsLong(key);\n",
        "2216         if (self->nd == 0)\n",
        "2217         {\n",
        "2218             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
        "2219             return NULL;\n",
        "2220         }\n",
        "2221         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
        "2222         int offset = 0;\n",
        "2223 \n",
        "2224         if ((d_idx >= 0) && (d_idx < d_dim))\n",
        "2225         {\n",
        "2226             //normal indexing\n",
        "2227             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
        "2228         }\n",
        "2229         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
        "2230         {\n",
        "2231             //end-based indexing\n",
        "2232             // d_idx is negative\n",
        "2233             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
        "2234         }\n",
        "2235         else\n",
        "2236         {\n",
        "2237             PyErr_SetString(PyExc_IndexError, \"index out of bounds\");\n",
        "2238             return NULL;\n",
        "2239         }\n",
        "2240 \n",
        "2241         //allocate our subtensor view\n",
        "2242         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
        "2243         rval = (CudaNdarray*) py_rval;\n",
        "2244         if (!rval) return NULL;\n",
        "2245         assert (0 == rval->data_allocated);\n",
        "2246 \n",
        "2247         //initialize the view's data pointer to our own.\n",
        "2248         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
        "2249         {\n",
        "2250             Py_DECREF(rval);\n",
        "2251             return NULL;\n",
        "2252         }\n",
        "2253         for (int d = 1; d < self->nd; ++d)\n",
        "2254         {\n",
        "2255             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
        "2256             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
        "2257         }\n",
        "2258     }\n",
        "2259     else\n",
        "2260     {\n",
        "2261         PyErr_Clear();\n",
        "2262     }\n",
        "2263     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
        "2264     {\n",
        "2265         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
        "2266         if (self->nd == 0)\n",
        "2267         {\n",
        "2268             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
        "2269             return NULL;\n",
        "2270         }\n",
        "2271 \n",
        "2272         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
        "2273         Py_ssize_t start, stop, step, slen;\n",
        "2274         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
        "2275         {\n",
        "2276             if (verbose)\n",
        "2277                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
        "2278             return NULL;\n",
        "2279         }\n",
        "2280         if (verbose)\n",
        "2281         {\n",
        "2282             std::cerr << \"start \" << start << \"\\n\";\n",
        "2283             std::cerr << \"stop \" << stop << \"\\n\";\n",
        "2284             std::cerr << \"step \" << step << \"\\n\";\n",
        "2285             std::cerr << \"slen \" << slen << \"\\n\";\n",
        "2286         }\n",
        "2287 \n",
        "2288         //allocate our subtensor view\n",
        "2289         py_rval = CudaNdarray_new_nd(self->nd);\n",
        "2290         rval = (CudaNdarray*) py_rval;\n",
        "2291         if (!rval) return NULL;\n",
        "2292         assert (0 == rval->data_allocated);\n",
        "2293 \n",
        "2294 \n",
        "2295         //initialize the view's data pointer to our own.\n",
        "2296         if (CudaNdarray_set_device_data(rval,\n",
        "2297                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
        "2298                     self))\n",
        "2299         {\n",
        "2300             Py_DECREF(rval);\n",
        "2301             return NULL;\n",
        "2302         }\n",
        "2303         //initialize dimension 0 of rval\n",
        "2304         CudaNdarray_set_stride(rval, 0,\n",
        "2305                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
        "2306         CudaNdarray_set_dim(rval, 0, slen);\n",
        "2307         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
        "2308         // initialize dimensions > 0 of rval\n",
        "2309         for (int d = 1; d < self->nd; ++d)\n",
        "2310         {\n",
        "2311             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
        "2312             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
        "2313         }\n",
        "2314     }\n",
        "2315     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
        "2316     {\n",
        "2317         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
        "2318         //elements of the tuple can be either integers or slices\n",
        "2319         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
        "2320 \n",
        "2321         if (PyTuple_Size(key) > self->nd)\n",
        "2322         {\n",
        "2323             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
        "2324             return NULL;\n",
        "2325         }\n",
        "2326 \n",
        "2327         //calculate the number of dimensions in the return value\n",
        "2328         int rval_nd = self->nd;\n",
        "2329         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
        "2330         {\n",
        "2331             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
        "2332             //So we use PyArray_IsAnyScalar that should covert everything.\n",
        "2333             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
        "2334         }\n",
        "2335 \n",
        "2336         //allocate our subtensor view\n",
        "2337         py_rval = CudaNdarray_new_nd(rval_nd);\n",
        "2338         rval = (CudaNdarray*) py_rval;\n",
        "2339         if (!rval) return NULL;\n",
        "2340         assert (0 == rval->data_allocated);\n",
        "2341 \n",
        "2342         //initialize the view's data pointer to our own.\n",
        "2343         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
        "2344         {\n",
        "2345             Py_DECREF(rval);\n",
        "2346             return NULL;\n",
        "2347         }\n",
        "2348 \n",
        "2349         // rval_d will refer to the current dimension in the rval.\n",
        "2350         // It will not be incremented for integer keys, but will be incremented for slice\n",
        "2351         // keys\n",
        "2352         int rval_d = 0;\n",
        "2353 \n",
        "2354         for (int d = 0; d < self->nd; ++d)\n",
        "2355         {\n",
        "2356             // keys can be shorter than self->nd.\n",
        "2357             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
        "2358             if (d >=PyTuple_Size(key))\n",
        "2359             {\n",
        "2360                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
        "2361                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
        "2362                 ++rval_d;\n",
        "2363             }\n",
        "2364             else\n",
        "2365             {\n",
        "2366                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
        "2367 \n",
        "2368                 if (PySlice_Check(key_d))\n",
        "2369                 {\n",
        "2370                     Py_ssize_t start, stop, step, slen;\n",
        "2371                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
        "2372                     {\n",
        "2373                         Py_DECREF(rval);\n",
        "2374                         return NULL;\n",
        "2375                     }\n",
        "2376                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
        "2377                     CudaNdarray_set_stride(rval, rval_d,\n",
        "2378                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
        "2379                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
        "2380                     if (0)\n",
        "2381                     {\n",
        "2382                         std::cerr << \"start \" << start << \"\\n\";\n",
        "2383                         std::cerr << \"stop \" << stop << \"\\n\";\n",
        "2384                         std::cerr << \"step \" << step << \"\\n\";\n",
        "2385                         std::cerr << \"slen \" << slen << \"\\n\";\n",
        "2386                     }\n",
        "2387                     ++rval_d;\n",
        "2388                 }\n",
        "2389                 else if ((intobj=PyNumber_Int(key_d)))\n",
        "2390                 {\n",
        "2391                     assert(PyArray_IsAnyScalar(key_d));\n",
        "2392                     int d_idx = PyInt_AsLong(intobj);\n",
        "2393                     Py_DECREF(intobj);\n",
        "2394                     intobj = NULL;\n",
        "2395                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
        "2396 \n",
        "2397                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
        "2398                     {\n",
        "2399                         //normal indexing\n",
        "2400                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
        "2401                     }\n",
        "2402                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
        "2403                     {\n",
        "2404                         //end-based indexing\n",
        "2405                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
        "2406                     }\n",
        "2407                     else\n",
        "2408                     {\n",
        "2409                         PyErr_SetString(PyExc_IndexError, \"index out of bounds\");\n",
        "2410                         Py_DECREF(rval);\n",
        "2411                         return NULL;\n",
        "2412                     }\n",
        "2413                 }\n",
        "2414                 else\n",
        "2415                 {\n",
        "2416                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
        "2417                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
        "2418                     Py_DECREF(rval);\n",
        "2419                     return NULL;\n",
        "2420                 }\n",
        "2421             }\n",
        "2422         }\n",
        "2423     }\n",
        "2424     if (py_rval)\n",
        "2425     {\n",
        "2426         if (verbose) fprint_CudaNdarray(stderr, self);\n",
        "2427         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
        "2428     }\n",
        "2429     else\n",
        "2430     {\n",
        "2431         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
        "2432         return NULL;\n",
        "2433     }\n",
        "2434     return py_rval;\n",
        "2435 }\n",
        "2436 \n",
        "2437 // Will by called by __setitem__ in Python\n",
        "2438 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
        "2439 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
        "2440 // Can only be assigned from a CudaNdarray on the right side\n",
        "2441 // Or a ndarray\n",
        "2442 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
        "2443 static int\n",
        "2444 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
        "2445 {\n",
        "2446     int verbose = 0;\n",
        "2447     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
        "2448     // We try to copy directly into this CudaNdarray from the ndarray\n",
        "2449     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
        "2450     CudaNdarray* new_value = NULL;\n",
        "2451 \n",
        "2452     if(!rval){\n",
        "2453         // CudaNdarray_Subscript failed and set the error msg.\n",
        "2454         Py_XDECREF(rval);\n",
        "2455         return -1;\n",
        "2456     }\n",
        "2457 \n",
        "2458     if(rval != (CudaNdarray*)o &&\n",
        "2459                 (rval->data_allocated ||\n",
        "2460                  // The new array should have a base\n",
        "2461                  !(((CudaNdarray*)rval)->base) ||\n",
        "2462                  // If the original array has no base, the base of the new\n",
        "2463                  // array should be the original one\n",
        "2464                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
        "2465                  // Else, the two arrays should have the same base\n",
        "2466                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
        "2467     {\n",
        "2468         // This case shouldn't happen, based on what I see in Subscript\n",
        "2469         // but just in case it happens sometime in the future\n",
        "2470 \n",
        "2471         PyErr_Format(PyExc_RuntimeError,\n",
        "2472                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
        "2473                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
        "2474                      \" o.base=%p o=%p\",\n",
        "2475                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
        "2476         Py_DECREF(rval);\n",
        "2477         return -1;\n",
        "2478     }\n",
        "2479 \n",
        "2480     PyObject * intobj = NULL;\n",
        "2481     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
        "2482         if (verbose)\n",
        "2483             fprintf(stderr,\n",
        "2484                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
        "2485                     \" value is a ndarray\\n\");\n",
        "2486         new_value = (CudaNdarray*) CudaNdarray_New();\n",
        "2487         if(!new_value)\n",
        "2488         {\n",
        "2489             return -1;\n",
        "2490         }\n",
        "2491         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
        "2492         {\n",
        "2493             Py_XDECREF(new_value);\n",
        "2494             Py_XDECREF(rval);\n",
        "2495             return -1;\n",
        "2496         }\n",
        "2497         value = (PyObject *) new_value;\n",
        "2498     }\n",
        "2499     else if ((intobj=PyNumber_Int(value)))\n",
        "2500     {\n",
        "2501         if (verbose)\n",
        "2502             fprintf(stderr,\n",
        "2503                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
        "2504         if(! CudaNdarray_is_c_contiguous(rval)){\n",
        "2505             PyErr_SetString(PyExc_NotImplementedError,\n",
        "2506                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
        "2507                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
        "2508             Py_XDECREF(rval);\n",
        "2509             return -1;\n",
        "2510         }\n",
        "2511 \n",
        "2512         long val = PyInt_AsLong(intobj);\n",
        "2513         Py_DECREF(intobj); intobj=NULL;\n",
        "2514         if (val == 0)\n",
        "2515         {\n",
        "2516             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
        "2517                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
        "2518             Py_XDECREF(rval);\n",
        "2519             if (err)\n",
        "2520             {\n",
        "2521                 // Clear the error flag, cudaMemset doesn't do it.\n",
        "2522                 // Currently this returns the same thing as err, but if in future\n",
        "2523                 // it returns something else I still don't see why we should ignore\n",
        "2524                 // it.  All we want to do here is reset the flag.\n",
        "2525                 cudaGetLastError();\n",
        "2526                 PyErr_SetString(PyExc_RuntimeError,\n",
        "2527                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
        "2528                 return -1;\n",
        "2529             }\n",
        "2530             return 0;\n",
        "2531         } else {\n",
        "2532             Py_XDECREF(rval);\n",
        "2533             PyErr_SetString(PyExc_NotImplementedError,\n",
        "2534                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
        "2535                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
        "2536                 return -1;\n",
        "2537         }\n",
        "2538     }\n",
        "2539 \n",
        "2540     PyErr_Clear(); // clear PyNumber_Int error.\n",
        "2541 \n",
        "2542     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
        "2543     {\n",
        "2544         PyErr_SetString(PyExc_TypeError,\n",
        "2545           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
        "2546           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
        "2547         Py_XDECREF(new_value);\n",
        "2548         return -1;\n",
        "2549     }\n",
        "2550 \n",
        "2551     if (verbose)\n",
        "2552         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
        "2553 \n",
        "2554     if (cnda_copy_structure_to_device(rval))\n",
        "2555     {\n",
        "2556         PyErr_SetString(PyExc_RuntimeError,\n",
        "2557                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
        "2558         Py_DECREF(rval);\n",
        "2559         Py_XDECREF(new_value);\n",
        "2560 \n",
        "2561         if (verbose)\n",
        "2562             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
        "2563         return -1;\n",
        "2564     }\n",
        "2565 \n",
        "2566     PyObject *baseSavedForComparison = rval->base;\n",
        "2567 \n",
        "2568     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
        "2569     {\n",
        "2570         Py_DECREF((PyObject*)rval);\n",
        "2571         Py_XDECREF(new_value);\n",
        "2572 \n",
        "2573         if (verbose)\n",
        "2574             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
        "2575         return -1;\n",
        "2576     }\n",
        "2577 \n",
        "2578     assert (rval->base == baseSavedForComparison);\n",
        "2579     assert (rval->dev_structure_fresh);\n",
        "2580 \n",
        "2581     // Clean up locally-created references\n",
        "2582     Py_DECREF(rval);\n",
        "2583     Py_XDECREF(new_value);\n",
        "2584 \n",
        "2585     return 0;\n",
        "2586 }\n",
        "2587 \n",
        "2588 \n",
        "2589 PyMappingMethods CudaNdarrayMappingMethods = {\n",
        "2590     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
        "2591     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
        "2592     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
        "2593 };\n",
        "2594 \n",
        "2595 ////////////////////\n",
        "2596 //\n",
        "2597 ////////////////////\n",
        "2598 \n",
        "2599 static PyObject *\n",
        "2600 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
        "2601 {\n",
        "2602     if (self->nd < 0)\n",
        "2603     {\n",
        "2604         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
        "2605         return NULL;\n",
        "2606     }\n",
        "2607     PyObject * rval = PyTuple_New(self->nd);\n",
        "2608     for (int i = 0; i < self->nd; ++i)\n",
        "2609     {\n",
        "2610         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
        "2611         {\n",
        "2612             Py_XDECREF(rval);\n",
        "2613             return NULL;\n",
        "2614         }\n",
        "2615 \n",
        "2616     }\n",
        "2617     return rval;\n",
        "2618 }\n",
        "2619 \n",
        "2620 static int\n",
        "2621 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
        "2622 {\n",
        "2623     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
        "2624     return -1;\n",
        "2625 }\n",
        "2626 \n",
        "2627 static PyObject *\n",
        "2628 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
        "2629 {\n",
        "2630     if (self->nd < 0)\n",
        "2631     {\n",
        "2632         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
        "2633         return NULL;\n",
        "2634     }\n",
        "2635     PyObject * rval = PyTuple_New(self->nd);"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "2636     for (int i = 0; i < self->nd; ++i)\n",
        "2637     {\n",
        "2638         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
        "2639         {\n",
        "2640             Py_XDECREF(rval);\n",
        "2641             return NULL;\n",
        "2642         }\n",
        "2643 \n",
        "2644     }\n",
        "2645     return rval;\n",
        "2646 }\n",
        "2647 \n",
        "2648 static int\n",
        "2649 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
        "2650 {\n",
        "2651     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
        "2652     if (PyTuple_Check(value)){\n",
        "2653         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
        "2654             PyErr_SetString(PyExc_ValueError,\n",
        "2655                             \"The new strides tuple must have the same length\"\n",
        "2656                             \" as the number of dimensions\");\n",
        "2657             return -1;\n",
        "2658         }\n",
        "2659     }else if (PyList_Check(value)){\n",
        "2660         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
        "2661             PyErr_SetString(PyExc_ValueError,\n",
        "2662                             \"The new strides list must have the same length\"\n",
        "2663                             \" as the number of dimensions\");\n",
        "2664             return -1;\n",
        "2665         }\n",
        "2666     }else{\n",
        "2667         PyErr_SetString(PyExc_ValueError,\n",
        "2668                         \"The new strides need to be encoded in a tuple or list\");\n",
        "2669         return -1;\n",
        "2670     }\n",
        "2671     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
        "2672     if (PyTuple_Check(value)){\n",
        "2673         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
        "2674             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
        "2675             //newstrides_bytes[i] = newstrides[i] * 4;\n",
        "2676         }\n",
        "2677     }else if (PyList_Check(value)){\n",
        "2678         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
        "2679             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
        "2680             //newstrides_bytes[i] = newstrides[i] * 4;\n",
        "2681         }\n",
        "2682     }\n",
        "2683     /*\n",
        "2684     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
        "2685     // to do it.\n",
        "2686     npy_intp dims[PyTuple_Size(value)];\n",
        "2687     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
        "2688         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
        "2689     }\n",
        "2690     if (!PyArray_CheckStrides(4,\n",
        "2691                               CudaNdarray_NDIM(self),\n",
        "2692                               0, 0,\n",
        "2693                               dims,\n",
        "2694                               newstrides_bytes)){\n",
        "2695         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
        "2696         return -1;\n",
        "2697         }\n",
        "2698     */\n",
        "2699     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
        "2700         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
        "2701     }\n",
        "2702     return 0;\n",
        "2703 }\n",
        "2704 \n",
        "2705 static PyObject *\n",
        "2706 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
        "2707 {\n",
        "2708     float * p =  CudaNdarray_DEV_DATA(self);\n",
        "2709     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
        "2710     return PyInt_FromLong((long int) CudaNdarray_DEV_DATA(self));\n",
        "2711 }\n",
        "2712 \n",
        "2713 static int\n",
        "2714 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
        "2715 {\n",
        "2716     long int newdevdata = PyInt_AsLong(value);\n",
        "2717     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
        "2718     if (PyErr_Occurred())\n",
        "2719     {\n",
        "2720         return -1;\n",
        "2721     }\n",
        "2722     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
        "2723 }\n",
        "2724 \n",
        "2725 static PyObject *\n",
        "2726 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
        "2727 {\n",
        "2728     return PyString_FromString(\"float32\");\n",
        "2729 }\n",
        "2730 \n",
        "2731 static PyObject *\n",
        "2732 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
        "2733 {\n",
        "2734     return PyInt_FromLong(self->nd);\n",
        "2735 }\n",
        "2736 \n",
        "2737 static PyObject *\n",
        "2738 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
        "2739 {\n",
        "2740     PyObject * base = self->base;\n",
        "2741     if (!base)\n",
        "2742     {\n",
        "2743         // We cannot return a NULL pointer, use None instead\n",
        "2744         base = Py_None;\n",
        "2745     }\n",
        "2746     Py_INCREF(base);\n",
        "2747     return base;\n",
        "2748 }\n",
        "2749 \n",
        "2750 void put_in_dict(PyObject * dict, const char * key, int val)\n",
        "2751 {\n",
        "2752   PyObject * k = PyString_FromString(key);\n",
        "2753   PyObject * v = PyInt_FromLong(val);\n",
        "2754   PyDict_SetItem(dict, k, v);\n",
        "2755   Py_DECREF(k);\n",
        "2756   Py_DECREF(v);\n",
        "2757 }\n",
        "2758 \n",
        "2759 PyObject *\n",
        "2760 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
        "2761 {\n",
        "2762   int dev_id = -1;\n",
        "2763   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
        "2764     return NULL;\n",
        "2765   cudaDeviceProp deviceProp;\n",
        "2766   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
        "2767 \n",
        "2768   PyObject * dict = PyDict_New();\n",
        "2769   PyObject * str= PyString_FromString(\"name\");\n",
        "2770   PyObject * i = PyString_FromString(deviceProp.name);\n",
        "2771   PyDict_SetItem(dict, str, i);\n",
        "2772   Py_DECREF(str);\n",
        "2773   Py_DECREF(i);\n",
        "2774 \n",
        "2775   put_in_dict(dict, \"major\", deviceProp.major);\n",
        "2776   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
        "2777 #if CUDART_VERSION >= 2020\n",
        "2778   int driverVersion = 0, runtimeVersion = 0;\n",
        "2779   cudaDriverGetVersion(&driverVersion);\n",
        "2780   cudaRuntimeGetVersion(&runtimeVersion);\n",
        "2781   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
        "2782   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
        "2783 #endif\n",
        "2784 #if CUDART_VERSION >= 2000\n",
        "2785 \n",
        "2786   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
        "2787   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
        "2788   int sm_cores = -1;\n",
        "2789   if(deviceProp.major==1)\n",
        "2790     sm_cores = 32;\n",
        "2791   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
        "2792     sm_cores = 32;\n",
        "2793   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
        "2794     sm_cores = 48;\n",
        "2795   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
        "2796 #endif\n",
        "2797   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
        "2798   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
        "2799   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
        "2800   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
        "2801   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
        "2802   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
        "2803   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
        "2804   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
        "2805   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
        "2806   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
        "2807   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
        "2808   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
        "2809   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
        "2810   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
        "2811 #if CUDART_VERSION >= 2000\n",
        "2812   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
        "2813 #endif\n",
        "2814 #if CUDART_VERSION >= 2020\n",
        "2815   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
        "2816   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
        "2817   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
        "2818   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
        "2819   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
        "2820 #endif\n",
        "2821 #if CUDART_VERSION >= 3000\n",
        "2822   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
        "2823 #endif\n",
        "2824 #if CUDART_VERSION >= 3010\n",
        "2825   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
        "2826 #endif\n",
        "2827 #if CUDART_VERSION >= 3020\n",
        "2828   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
        "2829 #endif\n",
        "2830 \n",
        "2831   return dict;\n",
        "2832 }\n",
        "2833 \n",
        "2834 /*\n",
        "2835  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
        "2836  */\n",
        "2837 PyObject *\n",
        "2838 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
        "2839 {\n",
        "2840     size_t free = 0, total = 0;\n",
        "2841     if(g_gpu_context_active == 0){\n",
        "2842         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
        "2843         return NULL;\n",
        "2844     }\n",
        "2845 \n",
        "2846     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
        "2847     if (err != cudaSuccess){\n",
        "2848         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
        "2849         // Currently this returns the same thing as err, but if in future\n",
        "2850         // it returns something else I still don't see why we should ignore\n",
        "2851         // it.  All we want to do here is reset the flag.\n",
        "2852         cudaGetLastError();\n",
        "2853         PyErr_Format(PyExc_RuntimeError,\n",
        "2854                      \"Error while getting memory info about the gpu: %s\",\n",
        "2855                      cudaGetErrorString(err));\n",
        "2856         return NULL;\n",
        "2857     }\n",
        "2858     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));\n",
        "2859 }\n",
        "2860 \n",
        "2861 /*\n",
        "2862  * Synchronize with all the gpu device stream.\n",
        "2863  */\n",
        "2864 PyObject *\n",
        "2865 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
        "2866 {\n",
        "2867     CNDA_BEGIN_ALLOW_THREADS\n",
        "2868     cudaThreadSynchronize();\n",
        "2869     CNDA_END_ALLOW_THREADS\n",
        "2870     Py_INCREF(Py_None);\n",
        "2871     return Py_None;\n",
        "2872 }\n",
        "2873 \n",
        "2874 /*\n",
        "2875  * Exist and return true if we link with cublas v2.\n",
        "2876  */\n",
        "2877 PyObject *\n",
        "2878 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
        "2879 {\n",
        "2880     Py_INCREF(Py_True);\n",
        "2881     return Py_True;\n",
        "2882 }\n",
        "2883 \n",
        "2884 #if COMPUTE_GPU_MEM_USED\n",
        "2885 /*\n",
        "2886  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
        "2887  */\n",
        "2888 PyObject *\n",
        "2889 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
        "2890 {\n",
        "2891     PyObject* a = PyLong_FromLong(_allocated_size);\n",
        "2892     PyObject* b = PyLong_FromLong(_max_allocated_size);\n",
        "2893 \n",
        "2894     PyObject* tuple = PyTuple_New(2);\n",
        "2895     PyTuple_SetItem(tuple, 0, a);\n",
        "2896     PyTuple_SetItem(tuple, 1, b);\n",
        "2897     return tuple;\n",
        "2898 }\n",
        "2899 #endif\n",
        "2900 \n",
        "2901 static PyGetSetDef CudaNdarray_getset[] = {\n",
        "2902     {\"shape\",\n",
        "2903         (getter)CudaNdarray_get_shape,\n",
        "2904         (setter)CudaNdarray_set_shape,\n",
        "2905         \"shape of this ndarray (tuple)\",\n",
        "2906         NULL},\n",
        "2907     {\"_strides\",\n",
        "2908         (getter)CudaNdarray_get_strides,\n",
        "2909         (setter)CudaNdarray_set_strides,\n",
        "2910         \"data pointer strides (in elements)\",\n",
        "2911         NULL},\n",
        "2912     {\"strides\",\n",
        "2913         (getter)CudaNdarray_get_strides,\n",
        "2914         (setter)CudaNdarray_set_strides,\n",
        "2915         \"data pointer strides (in elements)\",\n",
        "2916         NULL},\n",
        "2917     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
        "2918     {\"gpudata\",\n",
        "2919         (getter)CudaNdarray_get_dev_data,\n",
        "2920         NULL,\n",
        "2921         \"device data pointer\",\n",
        "2922         NULL},\n",
        "2923     {\"_dev_data\",\n",
        "2924         (getter)CudaNdarray_get_dev_data,\n",
        "2925         (setter)CudaNdarray_set_dev_data,\n",
        "2926         \"device data pointer\",\n",
        "2927         NULL},\n",
        "2928     {\"dtype\",\n",
        "2929         (getter)CudaNdarray_get_dtype,\n",
        "2930         NULL,\n",
        "2931         \"The dtype of the element. Now always float32\",\n",
        "2932         NULL},\n",
        "2933     {\"size\",\n",
        "2934         (getter)CudaNdarray_SIZE_Object,\n",
        "2935         NULL,\n",
        "2936         \"The number of elements in this object.\",\n",
        "2937         NULL},\n",
        "2938     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
        "2939     {\"mem_size\",\n",
        "2940         (getter)CudaNdarray_SIZE_Object,\n",
        "2941         NULL,\n",
        "2942         \"The number of elements in this object.\",\n",
        "2943         NULL},\n",
        "2944     {\"ndim\",\n",
        "2945         (getter)CudaNdarray_get_ndim,\n",
        "2946         NULL,\n",
        "2947         \"The number of dimensions in this object.\",\n",
        "2948         NULL},\n",
        "2949     {\"base\",\n",
        "2950         (getter)CudaNdarray_get_base,\n",
        "2951         NULL,\n",
        "2952         \"If this ndarray is a view, base is the original ndarray.\",\n",
        "2953         NULL},\n",
        "2954 \n",
        "2955     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
        "2956 };\n",
        "2957 \n",
        "2958 static PyTypeObject CudaNdarrayType =\n",
        "2959 {\n",
        "2960 #if PY_MAJOR_VERSION >= 3\n",
        "2961     PyVarObject_HEAD_INIT(NULL, 0)\n",
        "2962 #else\n",
        "2963     PyObject_HEAD_INIT(NULL)\n",
        "2964     0,                         /*ob_size*/\n",
        "2965 #endif\n",
        "2966     \"CudaNdarray\",             /*tp_name*/\n",
        "2967     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
        "2968     0,                         /*tp_itemsize*/\n",
        "2969     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
        "2970     0,                         /*tp_print*/\n",
        "2971     0,                         /*tp_getattr*/\n",
        "2972     0,                         /*tp_setattr*/\n",
        "2973     0,                         /*tp_compare*/\n",
        "2974     0,                         /*tp_repr*/\n",
        "2975     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
        "2976     0,                         /*tp_as_sequence*/\n",
        "2977     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
        "2978     0,                         /*tp_hash */\n",
        "2979     0,                         /*tp_call*/\n",
        "2980     0,                         /*tp_str*/\n",
        "2981     0,                         /*tp_getattro*/\n",
        "2982     0,                         /*tp_setattro*/\n",
        "2983     0,                         /*tp_as_buffer*/\n",
        "2984 #if PY_MAJOR_VERSION >= 3\n",
        "2985     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
        "2986     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
        "2987 #else\n",
        "2988     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
        "2989 #endif\n",
        "2990     \"CudaNdarray objects\",     /* tp_doc */\n",
        "2991     0,                         /* tp_traverse */\n",
        "2992     0,                         /* tp_clear */\n",
        "2993     0,                         /* tp_richcompare */\n",
        "2994     0,                         /* tp_weaklistoffset */\n",
        "2995     0,                         /* tp_iter */\n",
        "2996     0,                         /* tp_iternext */\n",
        "2997     CudaNdarray_methods,       /* tp_methods */\n",
        "2998     CudaNdarray_members,       /* tp_members */\n",
        "2999     CudaNdarray_getset,        /* tp_getset */\n",
        "3000     0,                         /* tp_base */\n",
        "3001     0,                         /* tp_dict */\n",
        "3002     0,                         /* tp_descr_get */\n",
        "3003     0,                         /* tp_descr_set */\n",
        "3004     0,                         /* tp_dictoffset */\n",
        "3005     (initproc)CudaNdarray_init,/* tp_init */\n",
        "3006     0,                         /* tp_alloc */\n",
        "3007     CudaNdarray_new,           /* tp_new */\n",
        "3008 };\n",
        "3009 \n",
        "3010 static __global__ void get_gpu_ptr_size(int* dst)\n",
        "3011 {\n",
        "3012     dst[0] = sizeof(float*);\n",
        "3013     dst[1] = sizeof(int);\n",
        "3014 }\n",
        "3015 \n",
        "3016 PyObject *\n",
        "3017 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
        "3018 {\n",
        "3019     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
        "3020     if(gpu_data == NULL){\n",
        "3021         return PyErr_Format(PyExc_MemoryError,\n",
        "3022                             \"CudaNdarray_ptr_int_size: Can't allocate memory on the gpu.\");\n",
        "3023     }\n",
        "3024     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
        "3025     if (cudaSuccess != cudaGetLastError()){\n",
        "3026 \n",
        "3027         device_free(gpu_data);\n",
        "3028         return PyErr_Format(PyExc_RuntimeError,\n",
        "3029                             \"CudaNdarray_ptr_int_size: error when calling the gpu code.\");\n",
        "3030     }\n",
        "3031 \n",
        "3032     // Transfer the result to cpu\n",
        "3033     int gpu_sizes[] = {-1,-1};\n",
        "3034     cublasStatus_t err;\n",
        "3035     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
        "3036     device_free(gpu_data);\n",
        "3037 \n",
        "3038     if (CUBLAS_STATUS_SUCCESS != err){\n",
        "3039         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
        "3040         return NULL;\n",
        "3041     }\n",
        "3042     return Py_BuildValue(\"iiii\", gpu_sizes[0], sizeof(float*), sizeof(int), gpu_sizes[1]);\n",
        "3043 }\n",
        "3044 \n",
        "3045 static int cublas_init();\n",
        "3046 static void cublas_shutdown();\n",
        "3047 // Initialize the gpu.\n",
        "3048 // Takes one optional parameter, the device number.\n",
        "3049 // If provided, it sets that device to be the active device.\n",
        "3050 // If not provided (usually just to test whether the gpu is available at all),\n",
        "3051 // it does not set an active device.\n",
        "3052 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
        "3053 PyObject *\n",
        "3054 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
        "3055 {\n",
        "3056     int card_nb = 0;\n",
        "3057     int card_number_provided = 1;\n",
        "3058 \n",
        "3059     PyArg_ParseTuple(args, \"|i\", &card_nb); // if we're given something wildly invalid, this will throw a TypeError\n",
        "3060 \n",
        "3061     if(PyTuple_Size(args) == 0) {\n",
        "3062         card_number_provided = 0;\n",
        "3063         card_nb = 0;\n",
        "3064     }\n",
        "3065 \n",
        "3066     int deviceCount;\n",
        "3067     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
        "3068     if(cudaSuccess != err) {\n",
        "3069         return PyErr_Format(PyExc_EnvironmentError,\n",
        "3070                             \"Unable to get the number of gpus available: %s\",\n",
        "3071                             cudaGetErrorString(cudaGetLastError()));\n",
        "3072     }\n",
        "3073 \n",
        "3074     // as soon as the first successful call to a cuda* function is made, a\n",
        "3075     // gpu context has been created\n",
        "3076     g_gpu_context_active = 1;\n",
        "3077 \n",
        "3078     if(deviceCount <= 0) {\n",
        "3079         return PyErr_Format(PyExc_EnvironmentError,\n",
        "3080                             \"Can't use the GPU, no devices support CUDA\");\n",
        "3081     }\n",
        "3082     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
        "3083         return PyErr_Format(PyExc_ValueError,\n",
        "3084                             \"Bad device number %d. Only %d devices available.\",\n",
        "3085                             card_nb,\n",
        "3086                             deviceCount);\n",
        "3087     }\n",
        "3088 \n",
        "3089     cudaDeviceProp deviceProp;\n",
        "3090     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
        "3091     if(cudaSuccess != err) {\n",
        "3092         return PyErr_Format(PyExc_EnvironmentError,\n",
        "3093                             \"Unable to get properties of gpu %i: %s\",\n",
        "3094                             card_nb,\n",
        "3095                             cudaGetErrorString(cudaGetLastError()));\n",
        "3096     }\n",
        "3097 \n",
        "3098     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
        "3099         return PyErr_Format(PyExc_EnvironmentError,\n",
        "3100                             \"There is no device that supports CUDA\");\n",
        "3101     }\n",
        "3102 \n",
        "3103     if(card_number_provided) {\n",
        "3104         err = cudaSetDevice(card_nb);\n",
        "3105         if(cudaSuccess != err) {\n",
        "3106             return PyErr_Format(PyExc_EnvironmentError,\n",
        "3107                                 \"Unable to set device %i: %s\",\n",
        "3108                                 card_nb,\n",
        "3109                                 cudaGetErrorString(cudaGetLastError()));\n",
        "3110         }\n",
        "3111         if (cublas_init() == -1)\n",
        "3112             return NULL;\n",
        "3113     }\n",
        "3114 \n",
        "3115     Py_INCREF(Py_None);\n",
        "3116     return Py_None;\n",
        "3117 }\n",
        "3118 \n",
        "3119 PyObject *\n",
        "3120 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
        "3121     // NB: No cuda error checking here; keeps things simple, and it's not\n",
        "3122     // really necessary.\n",
        "3123     int currentDevice;\n",
        "3124     cudaGetDevice(&currentDevice);\n",
        "3125     return PyInt_FromLong(currentDevice);\n",
        "3126 }\n",
        "3127 \n",
        "3128 PyObject *\n",
        "3129 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
        "3130     // NB: No cuda error checking here; keeps things simple, and it's not\n",
        "3131     // really necessary.\n",
        "3132     int currentDevice;\n",
        "3133     cudaGetDevice(&currentDevice);\n",
        "3134 \n",
        "3135     cudaDeviceProp deviceProp;\n",
        "3136     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
        "3137     return PyString_FromString(deviceProp.name);\n",
        "3138 }\n",
        "3139 \n",
        "3140 PyObject *\n",
        "3141 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
        "3142     // Don't handle errors here\n",
        "3143     cublas_shutdown();\n",
        "3144     cudaThreadExit();\n",
        "3145     g_gpu_context_active = 0; // context has now been closed down\n",
        "3146     Py_INCREF(Py_None);\n",
        "3147     return Py_None;\n",
        "3148 }\n",
        "3149 \n",
        "3150 /*\n",
        "3151  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
        "3152  */\n",
        "3153 PyObject *\n",
        "3154 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
        "3155 {\n",
        "3156     int verbose = 0;\n",
        "3157     PyObject *gpu_ptr = NULL;\n",
        "3158     PyObject *shapes = NULL;\n",
        "3159     PyObject *strides = NULL;\n",
        "3160     PyObject *base = NULL;\n",
        "3161     PyObject *rval = NULL;\n",
        "3162 \n",
        "3163     //args should consist of 3 python objects\n",
        "3164     //The first is the gpu ptr\n",
        "3165     //The second if the shape\n",
        "3166     //The third if the strides\n",
        "3167     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
        "3168         return NULL;\n",
        "3169 \n",
        "3170     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
        "3171     if (!PyLong_Check(gpu_ptr))\n",
        "3172     {\n",
        "3173         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
        "3174         return NULL;\n",
        "3175     }\n",
        "3176 \n",
        "3177     Py_ssize_t nd =  PyObject_Length(shapes);\n",
        "3178     if (nd < 0)\n",
        "3179     {\n",
        "3180         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
        "3181         return NULL;\n",
        "3182     }\n",
        "3183     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
        "3184     if (nd_stride < 0)\n",
        "3185     {\n",
        "3186         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
        "3187         return NULL;\n",
        "3188     }\n",
        "3189 \n",
        "3190     if (nd != nd_stride)\n",
        "3191     {\n",
        "3192         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
        "3193         return NULL;\n",
        "3194     }\n",
        "3195 \n",
        "3196     rval = CudaNdarray_New();\n",
        "3197 \n",
        "3198     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
        "3199     {\n",
        "3200         //CudaNdarray_set_nd set the error msg\n",
        "3201         return NULL;\n",
        "3202     }\n",
        "3203     // set gpu pointeur\n",
        "3204     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
        "3205     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
        "3206     {\n",
        "3207         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
        "3208         return NULL;\n",
        "3209 \n",
        "3210     }\n",
        "3211 \n",
        "3212     // Set dims and strides\n",
        "3213     for (int i = nd-1; i >= 0; --i)\n",
        "3214     {\n",
        "3215         PyObject * idx = PyLong_FromLong(i);\n",
        "3216         if (idx == NULL)\n",
        "3217         {\n",
        "3218             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
        "3219             return NULL;\n",
        "3220         }\n",
        "3221         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
        "3222         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
        "3223         if (!PyInt_Check(dim_))\n",
        "3224         {\n",
        "3225             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
        "3226             return NULL;\n",
        "3227         }\n",
        "3228         if (!PyInt_Check(strd_))\n",
        "3229         {\n",
        "3230             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
        "3231             return NULL;\n",
        "3232         }\n",
        "3233         int dim = PyInt_AsLong(dim_);\n",
        "3234         int strd = PyInt_AsLong(strd_);\n",
        "3235         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
        "3236         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
        "3237         Py_DECREF(idx);\n",
        "3238         Py_DECREF(dim_);\n",
        "3239         Py_DECREF(strd_);\n",
        "3240     }\n",
        "3241     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
        "3242     return rval;\n",
        "3243 }\n",
        "3244 \n",
        "3245 PyObject *\n",
        "3246 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
        "3247 {\n",
        "3248     PyObject *l=NULL;\n",
        "3249     PyObject *r=NULL;\n",
        "3250     PyObject * rval = NULL;\n",
        "3251 \n",
        "3252     //args should consist of two python objects (\"OO\")\n",
        "3253     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
        "3254         return NULL;\n",
        "3255 \n",
        "3256     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
        "3257     {\n",
        "3258         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n",
        "3259         goto CudaNdarray_dot_fail;\n",
        "3260     }\n",
        "3261     if (((CudaNdarray*)l)->nd != 2)\n",
        "3262     {\n",
        "3263         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
        "3264         goto CudaNdarray_dot_fail;\n",
        "3265     }\n",
        "3266     if (((CudaNdarray*)r)->nd != 2)\n",
        "3267     {\n",
        "3268         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
        "3269         goto CudaNdarray_dot_fail;\n",
        "3270     }\n",
        "3271     rval = CudaNdarray_New();\n",
        "3272     if (!rval)\n",
        "3273     {\n",
        "3274         goto CudaNdarray_dot_fail;\n",
        "3275     }\n",
        "3276     int dims[2];\n",
        "3277     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
        "3278     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
        "3279     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
        "3280     {\n",
        "3281         goto CudaNdarray_dot_fail;\n",
        "3282     }\n",
        "3283     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
        "3284     {\n",
        "3285         goto CudaNdarray_dot_fail;\n",
        "3286     }\n",
        "3287 \n",
        "3288     return rval;\n",
        "3289 \n",
        "3290     CudaNdarray_dot_fail:\n",
        "3291     Py_XDECREF(rval);\n",
        "3292     return NULL;\n",
        "3293 }\n",
        "3294 \n",
        "3295 static PyObject *\n",
        "3296 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
        "3297 {\n",
        "3298     /*\n",
        "3299      * TODO: DOC what this function should do in the various cases of\n",
        "3300      * What is 'strict' supposed to mean in the context of this function?\n",
        "3301      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
        "3302      *\n",
        "3303      */\n",
        "3304     PyObject *py_data=NULL;\n",
        "3305     PyArrayObject * data = NULL;\n",
        "3306     int strict = 0;\n",
        "3307     PyObject * broadcastable=NULL;\n",
        "3308     PyObject * storage=NULL;\n",
        "3309     CudaNdarray * rval=NULL;\n",
        "3310 \n",
        "3311     //Python object references which are provided to the caller are borrowed references\n",
        "3312     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
        "3313 \n",
        "3314     if (!PyTuple_Check(broadcastable)){\n",
        "3315         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
        "3316         return NULL;\n",
        "3317     }\n",
        "3318     Py_INCREF(py_data);\n",
        "3319     Py_INCREF(broadcastable);\n",
        "3320 \n",
        "3321     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
        "3322 \n",
        "3323     if (strict || CudaNdarray_Check(py_data))\n",
        "3324     {\n",
        "3325         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
        "3326         if (!CudaNdarray_Check(py_data))\n",
        "3327         {\n",
        "3328             Py_DECREF(py_data);\n",
        "3329             Py_DECREF(broadcastable);\n",
        "3330             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
        "3331             return NULL;\n",
        "3332         }\n",
        "3333         if (cnda->nd != PyTuple_Size(broadcastable))\n",
        "3334         {\n",
        "3335             Py_DECREF(py_data);\n",
        "3336             Py_DECREF(broadcastable);\n",
        "3337             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
        "3338             return NULL;\n",
        "3339         }\n",
        "3340         for (int i = 0; i < cnda->nd; ++i)\n",
        "3341         {\n",
        "3342             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
        "3343             {\n",
        "3344                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
        "3345                 Py_DECREF(py_data);\n",
        "3346                 Py_DECREF(broadcastable);\n",
        "3347                 return NULL;\n",
        "3348             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
        "3349                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
        "3350                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
        "3351                 Py_DECREF(py_data);\n",
        "3352                 Py_DECREF(broadcastable);\n",
        "3353                 return NULL;\n",
        "3354             }\n",
        "3355         }\n",
        "3356         Py_DECREF(broadcastable);\n",
        "3357         return py_data;\n",
        "3358     }\n",
        "3359     else\n",
        "3360     {\n",
        "3361         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
        "3362         if (!data)\n",
        "3363         {\n",
        "3364             //err message already defined\n",
        "3365             Py_DECREF(py_data);\n",
        "3366             Py_DECREF(broadcastable);\n",
        "3367             return NULL;\n",
        "3368         }\n",
        "3369         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
        "3370         {\n",
        "3371             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
        "3372             {\n",
        "3373                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
        "3374                 Py_DECREF(data);\n",
        "3375                 Py_DECREF(py_data);\n",
        "3376                 Py_DECREF(broadcastable);\n",
        "3377                 return NULL;\n",
        "3378             }\n",
        "3379         }\n",
        "3380         if (storage && CudaNdarray_Check(storage))\n",
        "3381         {\n",
        "3382             rval = (CudaNdarray*) storage;\n",
        "3383             Py_INCREF(rval);\n",
        "3384         }\n",
        "3385         else\n",
        "3386         {\n",
        "3387             rval = (CudaNdarray*) CudaNdarray_New();\n",
        "3388         }\n",
        "3389         if (rval)\n",
        "3390         {\n",
        "3391             if (CudaNdarray_CopyFromArray(rval, data))\n",
        "3392             {\n",
        "3393                 Py_DECREF(rval);\n",
        "3394                 rval = NULL;\n",
        "3395             }\n",
        "3396         }\n",
        "3397         Py_DECREF(data);\n",
        "3398         Py_DECREF(py_data);\n",
        "3399         Py_DECREF(broadcastable);\n",
        "3400         return (PyObject*)rval;\n",
        "3401     }\n",
        "3402 }\n",
        "3403 \n",
        "3404 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
        "3405 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
        "3406 static PyMethodDef module_methods[] = {\n",
        "3407     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
        "3408     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
        "3409     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
        "3410     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
        "3411     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
        "3412     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
        "3413     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
        "3414     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
        "3415 #if COMPUTE_GPU_MEM_USED\n",
        "3416     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
        "3417 #endif\n",
        "3418     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
        "3419     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
        "3420     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
        "3421     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
        "3422     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
        "3423     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
        "3424      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
        "3425     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
        "3426 };\n",
        "3427 \n",
        "3428 #ifndef PyMODINIT_FUNC  /* declarations for DLL import/export */\n",
        "3429 #define PyMODINIT_FUNC void\n",
        "3430 #endif\n",
        "3431 \n",
        "3432 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
        "3433 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
        "3434 \n",
        "3435 #if PY_MAJOR_VERSION == 3\n",
        "3436 static struct PyModuleDef cuda_ndarray_moduledef =\n",
        "3437 {\n",
        "3438     PyModuleDef_HEAD_INIT,\n",
        "3439     CNDA_MOD_NAME,\n",
        "3440     CNDA_DOCSTRING,\n",
        "3441     -1,     /* size of per-interpreter state of the module,\n",
        "3442                or -1 if the module keeps state in global variables. */\n",
        "3443     module_methods\n",
        "3444 };\n",
        "3445 \n",
        "3446 PyMODINIT_FUNC\n",
        "3447 PyInit_cuda_ndarray(void)\n",
        "3448 #else\n",
        "3449 PyMODINIT_FUNC\n",
        "3450 initcuda_ndarray(void)\n",
        "3451 #endif\n",
        "3452 {\n",
        "3453     import_array();\n",
        "3454 \n",
        "3455     PyObject* m;\n",
        "3456 \n",
        "3457     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
        "3458 #if PY_MAJOR_VERSION == 3\n",
        "3459         return NULL;\n",
        "3460 #else\n",
        "3461         return;\n",
        "3462 #endif\n",
        "3463     }\n",
        "3464 \n",
        "3465 #if PY_MAJOR_VERSION == 3\n",
        "3466     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
        "3467 #else\n",
        "3468     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
        "3469 #endif\n",
        "3470 \n",
        "3471     if (m == NULL) {\n",
        "3472 #if PY_MAJOR_VERSION == 3\n",
        "3473         return NULL;\n",
        "3474 #else\n",
        "3475         return;\n",
        "3476 #endif\n",
        "3477     }\n",
        "3478 \n",
        "3479     Py_INCREF(&CudaNdarrayType);\n",
        "3480     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
        "3481 #if COMPUTE_GPU_MEM_USED\n",
        "3482     for(int i=0;i<TABLE_SIZE;i++){\n",
        "3483         _alloc_size_table[i].ptr=NULL;\n",
        "3484         _alloc_size_table[i].size=0;\n",
        "3485     }\n",
        "3486 #endif\n",
        "3487     //    cublasInit();\n",
        "3488     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
        "3489     //{\n",
        "3490         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
        "3491     //}\n",
        "3492     if (0) //TODO: is this necessary?\n",
        "3493     {\n",
        "3494         int deviceId = 0; // TODO: what number goes here?\n",
        "3495         cudaSetDevice(deviceId);\n",
        "3496         cudaError_t err = cudaGetLastError();\n",
        "3497         if( cudaSuccess != err)\n",
        "3498         {\n",
        "3499             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
        "3500         }\n",
        "3501     }\n",
        "3502 \n",
        "3503 #if PY_MAJOR_VERSION == 3\n",
        "3504     return m;\n",
        "3505 #endif\n",
        "3506 }\n",
        "3507 \n",
        "3508 \n",
        "3509 //////////////////////////////////////\n",
        "3510 //\n",
        "3511 // C API FOR CudaNdarray\n",
        "3512 //\n",
        "3513 //////////////////////////////////////\n",
        "3514 \n",
        "3515 int\n",
        "3516 CudaNdarray_Check(const PyObject * ob)\n",
        "3517 {\n",
        "3518     //TODO: doesn't work with inheritance\n",
        "3519     return CudaNdarray_CheckExact(ob);\n",
        "3520 }\n",
        "3521 int\n",
        "3522 CudaNdarray_CheckExact(const PyObject * ob)\n",
        "3523 {\n",
        "3524     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
        "3525 }\n",
        "3526 \n",
        "3527 PyObject *\n",
        "3528 CudaNdarray_New(int nd)\n",
        "3529 {\n",
        "3530     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
        "3531     if (self == NULL)\n",
        "3532     {\n",
        "3533         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
        "3534         return NULL;\n",
        "3535     }\n",
        "3536     CudaNdarray_null_init(self);\n",
        "3537 \n",
        "3538     if (nd == 0)\n",
        "3539     {\n",
        "3540         self->nd = 0;\n",
        "3541     }\n",
        "3542     else if (nd > 0)\n",
        "3543     {\n",
        "3544         if (CudaNdarray_set_nd(self, nd))\n",
        "3545         {\n",
        "3546             Py_DECREF(self);\n",
        "3547             return NULL;\n",
        "3548         }\n",
        "3549     }\n",
        "3550     ++_outstanding_mallocs[1];\n",
        "3551     return (PyObject *)self;\n",
        "3552 }\n",
        "3553 \n",
        "3554 \n",
        "3555 \n",
        "3556 //////////////////////////////\n",
        "3557 //\n",
        "3558 // Published helper functions\n",
        "3559 //\n",
        "3560 //////////////////////////////\n",
        "3561 \n",
        "3562 static int\n",
        "3563 cublas_init()\n",
        "3564 {\n",
        "3565     cublasStatus_t err;\n",
        "3566     err = cublasCreate(&handle);\n",
        "3567     if (CUBLAS_STATUS_SUCCESS != err)\n",
        "3568     {\n",
        "3569         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
        "3570             PyErr_SetString(PyExc_RuntimeError,\n",
        "3571                             \"cublasCreate() returned this error \"\n",
        "3572                             \"'the CUDA Runtime initialization failed'\");\n",
        "3573         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
        "3574             PyErr_SetString(PyExc_RuntimeError,\n",
        "3575                             \"cublasCreate() returned this error \"\n",
        "3576                             \"'the resources could not be allocated'\");\n",
        "3577         else\n",
        "3578             PyErr_SetString(PyExc_RuntimeError,\n",
        "3579                             \"unknow error during returned by cublasCreate()\");\n",
        "3580         return -1;\n",
        "3581     }\n",
        "3582     // Set the default stream as the one to execute on (default)\n",
        "3583     cublasSetStream(handle, NULL);\n",
        "3584     // Pointer to scalars are on the host (also default)\n",
        "3585     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
        "3586 #if CUDA_VERSION >= 5000\n",
        "3587     // atomics can be used in kernels to speed up operations (not default)\n",
        "3588     // This may lead to a slight variance from run to run in some operations\n",
        "3589     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
        "3590 #endif\n",
        "3591     return 0;\n",
        "3592 }\n",
        "3593 \n",
        "3594 static void\n",
        "3595 cublas_shutdown()\n",
        "3596 {\n",
        "3597     if (handle != NULL)\n",
        "3598         cublasDestroy(handle);\n",
        "3599     // No point in handling any errors here\n",
        "3600     handle = NULL;\n",
        "3601 }\n",
        "3602 \n",
        "3603 int\n",
        "3604 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
        "3605 {\n",
        "3606     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
        "3607                                            PyArray_DIMS(obj));\n",
        "3608     if (err) {\n",
        "3609         return err;\n",
        "3610     }\n",
        "3611 \n",
        "3612     int typenum = PyArray_TYPE(obj);\n",
        "3613     if (typenum != REAL_TYPENUM)\n",
        "3614     {\n",
        "3615         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
        "3616         return -1;\n",
        "3617     }\n",
        "3618     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
        "3619     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
        "3620         (PyObject*)obj, typenum, self->nd, self->nd);\n",
        "3621     if (!py_src) {\n",
        "3622         return -1;\n",
        "3623     }\n",
        "3624     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
        "3625     void *py_src_data = PyArray_DATA(py_src);\n",
        "3626     cublasStatus_t cerr;\n",
        "3627     CNDA_BEGIN_ALLOW_THREADS\n",
        "3628     cerr = cublasSetVector(py_src_size,\n",
        "3629                            sizeof(real),\n",
        "3630                            py_src_data, 1,\n",
        "3631                            self->devdata, 1);\n",
        "3632     //CNDA_THREAD_SYNC;  // unneeded because cublasSetVector is blocking anyway\n",
        "3633     CNDA_END_ALLOW_THREADS\n",
        "3634     if (CUBLAS_STATUS_SUCCESS != cerr)\n",
        "3635     {\n",
        "3636         PyErr_SetString(PyExc_RuntimeError, \"error copying data to device memory\");\n",
        "3637         Py_DECREF(py_src);\n",
        "3638         return -1;\n",
        "3639     }\n",
        "3640     Py_DECREF(py_src);\n",
        "3641     return 0;\n",
        "3642 }\n",
        "3643 \n",
        "3644 PyObject *\n",
        "3645 CudaNdarray_new_nd(int nd)\n",
        "3646 {\n",
        "3647     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
        "3648     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
        "3649     {\n",
        "3650         Py_XDECREF(rval);\n",
        "3651         rval = NULL;\n",
        "3652     }\n",
        "3653     return (PyObject *) rval;\n",
        "3654 }\n",
        "3655 \n",
        "3656 \n",
        "3657 /**\n",
        "3658  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
        "3659  */\n",
        "3660 \n",
        "3661 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
        "3662 {\n",
        "3663     if (self->data_allocated)\n",
        "3664     {\n",
        "3665         assert(self->devdata);\n",
        "3666         if (device_free(self->devdata))\n",
        "3667         {\n",
        "3668             self->devdata = NULL;\n",
        "3669             self->data_allocated = 0;\n",
        "3670             return -1;\n",
        "3671         }\n",
        "3672     }\n",
        "3673     // Get the original base object (base.base.base...)\n",
        "3674     PyObject * orig_base = base;\n",
        "3675     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
        "3676     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
        "3677     {\n",
        "3678         // base_base is itself a view\n",
        "3679         orig_base = ((CudaNdarray*) orig_base)->base;\n",
        "3680     }\n",
        "3681     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
        "3682     if (self->base != orig_base)\n",
        "3683     {\n",
        "3684         Py_XDECREF(self->base);\n",
        "3685         self->base = orig_base;\n",
        "3686         Py_XINCREF(self->base);\n",
        "3687     }\n",
        "3688     self->data_allocated = 0;\n",
        "3689     self->devdata = data;\n",
        "3690     return 0;\n",
        "3691 }\n",
        "3692 \n",
        "3693 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
        "3694 {\n",
        "3695     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
        "3696     {\n",
        "3697         y[i*sy] = x[i*sx];\n",
        "3698     }\n",
        "3699 }\n",
        "3700 \n",
        "3701 // N1 through N4 are the size of y\n",
        "3702 static __global__ void k_copy_4d(const int N1,\n",
        "3703         const int N2, const int N3, const int N4,\n",
        "3704         const float * x, const int sx1, const int sx2, const int sx3,\n",
        "3705         const int sx4,  float * y, const int sy1, const int sy2,\n",
        "3706         const int sy3, const int sy4)\n",
        "3707 {\n",
        "3708     // These must be made int instead of unsigned int due to a bug in nvcc\n",
        "3709     int bx = blockIdx.x;\n",
        "3710     int by = blockIdx.y;\n",
        "3711 \n",
        "3712     for (int i = bx; i < N1; i += gridDim.x)\n",
        "3713     {\n",
        "3714         for (int j = by; j < N2; j += gridDim.y)\n",
        "3715         {\n",
        "3716             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
        "3717             {\n",
        "3718                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
        "3719                 {\n",
        "3720                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
        "3721                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
        "3722                 }\n",
        "3723             }\n",
        "3724         }\n",
        "3725     }\n",
        "3726 }\n",
        "3727 \n",
        "3728 //copy from other into self\n",
        "3729 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
        "3730                                     const CudaNdarray * other,\n",
        "3731                                     bool unbroadcast)\n",
        "3732 {\n",
        "3733     int verbose = 0;\n",
        "3734     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
        "3735 \n",
        "3736     //standard elemwise size checks\n",
        "3737     if (self->nd == -1)\n",
        "3738     {\n",
        "3739         PyErr_SetString(PyExc_TypeError,\n",
        "3740                         \"can't copy into un-initialized CudaNdarray\");\n",
        "3741         return -1;\n",
        "3742     }\n",
        "3743     CudaNdarray * new_other = NULL;\n",
        "3744 \n",
        "3745     if (self->nd < other->nd)\n",
        "3746     {\n",
        "3747         PyErr_Format(PyExc_NotImplementedError,\n",
        "3748             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
        "3749             \"destination needs to be >= the number of dimensions of the \"\n",
        "3750             \"source. Got %d and %d.\", self->nd, other->nd);\n",
        "3751         return -1;\n",
        "3752     }\n",
        "3753     else if (self->nd != other->nd)\n",
        "3754     {\n",
        "3755         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
        "3756         int added_dims = self->nd - other->nd;\n",
        "3757         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
        "3758         for(int i = 0; i < added_dims; i++)\n",
        "3759             pattern[i] = -1;\n",
        "3760         for(int i = 0; i < other->nd; i++)\n",
        "3761             pattern[i + added_dims] = i;\n",
        "3762         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
        "3763         other = new_other;\n",
        "3764     }\n",
        "3765     assert(self->nd == other->nd);\n",
        "3766     //standard elemwise dim checks (also compute total size)\n",
        "3767     unsigned int size = 1;\n",
        "3768     unsigned int size_source = 1;\n",
        "3769     for (int i = 0; i< self->nd; ++i)\n",
        "3770     {\n",
        "3771         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
        "3772             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
        "3773         {\n",
        "3774           PyErr_Format(PyExc_ValueError,\n",
        "3775                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
        "3776                        \" need same dimensions for dim %d,\"\n",
        "3777                        \" destination=%d, source=%d\",\n",
        "3778                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
        "3779                        CudaNdarray_HOST_DIMS(other)[i]);\n",
        "3780           Py_XDECREF(new_other);\n",
        "3781           return -1;\n",
        "3782         }\n",
        "3783         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
        "3784         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
        "3785     }\n",
        "3786     if (0 == size)\n",
        "3787     {\n",
        "3788         Py_XDECREF(new_other);\n",
        "3789         return 0; //nothing to copy, we're done.\n",
        "3790     }\n",
        "3791     if (CudaNdarray_is_c_contiguous(self) &&\n",
        "3792         CudaNdarray_is_c_contiguous(other) &&\n",
        "3793         size == size_source)\n",
        "3794     {\n",
        "3795         if (verbose)\n",
        "3796             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
        "3797 \n",
        "3798         cublasStatus_t err;\n",
        "3799         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
        "3800                           CudaNdarray_DEV_DATA(self), 1);\n",
        "3801         CNDA_THREAD_SYNC;\n",
        "3802         Py_XDECREF(new_other);\n",
        "3803         if (CUBLAS_STATUS_SUCCESS != err)\n",
        "3804         {\n",
        "3805             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
        "3806             return -1;\n",
        "3807         }\n",
        "3808         return 0;\n",
        "3809     }\n",
        "3810     //TODO: rewrite these copy operations to be more efficient\n",
        "3811     //      See, for example the transpose example in the cuda_sdk.\n",
        "3812     switch (self->nd)\n",
        "3813     {\n",
        "3814         case 0: // scalar\n",
        "3815             {\n",
        "3816                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
        "3817                 assert(0);\n",
        "3818             }; break;\n",
        "3819         case 1: // vector\n",
        "3820             {\n",
        "3821                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
        "3822                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
        "3823                 unsigned int n_blocks = std::min(size,\n",
        "3824                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
        "3825                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
        "3826                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
        "3827                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
        "3828                                             CudaNdarray_DEV_DATA(other),\n",
        "3829                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
        "3830                                             CudaNdarray_DEV_DATA(self),\n",
        "3831                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
        "3832                 CNDA_THREAD_SYNC;\n",
        "3833                 cudaError_t err = cudaGetLastError();\n",
        "3834                 if( cudaSuccess != err)\n",
        "3835                 {\n",
        "3836                     PyErr_Format(PyExc_RuntimeError,\n",
        "3837                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
        "3838                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
        "3839                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
        "3840                     Py_XDECREF(new_other);\n",
        "3841                     return -1;\n",
        "3842                 }\n",
        "3843             }; break;\n",
        "3844         case 4: // 4-tensor\n",
        "3845             {\n",
        "3846                 if (verbose)\n",
        "3847                 {\n",
        "3848                     if (0 != fprint_CudaNdarray(stderr, other))\n",
        "3849                     {\n",
        "3850                         Py_XDECREF(new_other);\n",
        "3851                         return -1;\n",
        "3852                     }\n",
        "3853                 }\n",
        "3854 \n",
        "3855                 // The blocks implement the looping over the first two axes so\n",
        "3856                 // this needs to be (N1, N2)\n",
        "3857                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
        "3858                                         NUM_VECTOR_OP_BLOCKS),\n",
        "3859                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
        "3860                                         NUM_VECTOR_OP_BLOCKS));\n",
        "3861                 // For the threads, just make as many as possible\n",
        "3862                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
        "3863                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
        "3864                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
        "3865                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
        "3866 \n",
        "3867                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
        "3868                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
        "3869 \n",
        "3870                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
        "3871                                             // size of y\n",
        "3872                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
        "3873                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
        "3874                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
        "3875                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
        "3876                                             CudaNdarray_DEV_DATA(other), // x\n",
        "3877                                             // x strides\n",
        "3878                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
        "3879                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
        "3880                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
        "3881                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
        "3882                                             CudaNdarray_DEV_DATA(self), // y\n",
        "3883                                             // y strides\n",
        "3884                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
        "3885                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
        "3886                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
        "3887                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
        "3888                                             );\n",
        "3889                 CNDA_THREAD_SYNC;\n",
        "3890                 cudaError_t err = cudaGetLastError();\n",
        "3891                 if( cudaSuccess != err)\n",
        "3892                 {\n",
        "3893                     PyErr_Format(PyExc_RuntimeError,\n",
        "3894                                  \"Cuda error: %s: %s.\",\n",
        "3895                                  \"k_copy_4d\",\n",
        "3896                                  cudaGetErrorString(err));\n",
        "3897                     Py_XDECREF(new_other);\n",
        "3898                     return -1;\n",
        "3899                 }\n",
        "3900             }; break;\n",
        "3901         default:\n",
        "3902             {\n",
        "3903                 assert (cudaSuccess == cudaGetLastError());\n",
        "3904                 if (verbose)\n",
        "3905                     fprintf(stderr,\n",
        "3906                             \"Copying with default version unbroadcast=%d\\n\",\n",
        "3907                             unbroadcast);\n",
        "3908                 // call worker routine\n",
        "3909                 unsigned int threads_per_block = std::min(size,\n",
        "3910                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
        "3911                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n",
        "3912                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
        "3913                 const CudaNdarray * cuda_dims = other;\n",
        "3914                 if(unbroadcast)\n",
        "3915                     cuda_dims = self;\n",
        "3916                 //copy from other into self\n",
        "3917                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
        "3918                         size,\n",
        "3919                         (unsigned int)other->nd,\n",
        "3920                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
        "3921                         (const float*)CudaNdarray_DEV_DATA(other),\n",
        "3922                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
        "3923                         CudaNdarray_DEV_DATA(self),\n",
        "3924                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
        "3925                 CNDA_THREAD_SYNC;\n",
        "3926                 cudaError_t err = cudaGetLastError();\n",
        "3927                 if(verbose>1)\n",
        "3928                     fprintf(stderr,\n",
        "3929                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
        "3930                             \" n_threads_per_block=%i)\\n\",\n",
        "3931                             n_blocks, threads_per_block);\n",
        "3932                 if( cudaSuccess != err)\n",
        "3933                 {\n",
        "3934                     //fprint_CudaNdarray(stderr, self);\n",
        "3935                     //fprint_CudaNdarray(stderr, other);\n",
        "3936                     PyErr_Format(PyExc_RuntimeError,\n",
        "3937                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
        "3938                                  \" n_threads_per_block=%i)\\n\",\n",
        "3939                                  \"k_elemwise_unary_rowmajor_copy\",\n",
        "3940                                  cudaGetErrorString(err), n_blocks,\n",
        "3941                                  threads_per_block);\n",
        "3942                     Py_XDECREF(new_other);\n",
        "3943                     return -1;\n",
        "3944                 }\n",
        "3945             }\n",
        "3946     };\n",
        "3947     Py_XDECREF(new_other);\n",
        "3948     return 0;\n",
        "3949 }\n",
        "3950 \n",
        "3951 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
        "3952 {\n",
        "3953     if (A->nd != 2)\n",
        "3954     {\n",
        "3955         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
        "3956         return -1;\n",
        "3957     }\n",
        "3958     if (B->nd != 2)\n",
        "3959     {\n",
        "3960         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
        "3961         return -1;\n",
        "3962     }\n",
        "3963     if (C->nd != 2)\n",
        "3964     {\n",
        "3965         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
        "3966         return -1;\n",
        "3967     }\n",
        "3968 \n",
        "3969     // We must allow dimensions to be zeros.\n",
        "3970     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
        "3971             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
        "3972             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
        "3973     {\n",
        "3974         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
        "3975                 CudaNdarray_HOST_DIMS(A)[0],\n",
        "3976                 CudaNdarray_HOST_DIMS(A)[1],\n",
        "3977                 CudaNdarray_HOST_DIMS(B)[0],\n",
        "3978                 CudaNdarray_HOST_DIMS(B)[1],\n",
        "3979                 CudaNdarray_HOST_DIMS(C)[0],\n",
        "3980                 CudaNdarray_HOST_DIMS(C)[1]);\n",
        "3981         return -1;\n",
        "3982     }\n",
        "3983 \n",
        "3984     // If matrix A or B has non-unit size and non-unit stride in both\n",
        "3985     // dimensions, we can make a copy.\n",
        "3986     CudaNdarray * A_new = NULL;\n",
        "3987     CudaNdarray * B_new = NULL;\n",
        "3988     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
        "3989          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
        "3990          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
        "3991          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
        "3992         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
        "3993         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
        "3994     {\n",
        "3995         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
        "3996         if (!A_new)\n",
        "3997             return -1;\n",
        "3998         A = A_new;\n",
        "3999     }\n",
        "4000 \n",
        "4001     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
        "4002          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
        "4003          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
        "4004          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
        "4005         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
        "4006         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
        "4007     {\n",
        "4008         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
        "4009         if (!B_new)\n",
        "4010         {\n",
        "4011             // If A_new is NULL, meaning A was not copied nothing happens\n",
        "4012             Py_XDECREF(A_new);\n",
        "4013             return -1;\n",
        "4014         }\n",
        "4015         B = B_new;\n",
        "4016     }\n",
        "4017 \n",
        "4018     // If matrix C has non-unit size and non-unit stride in both\n",
        "4019     // dimensions, or negative strides, we can't operate. We cannot copy\n",
        "4020     // C either, because the calling code will expect the result to be\n",
        "4021     // in the original C container.\n",
        "4022     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
        "4023          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
        "4024          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
        "4025          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
        "4026         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
        "4027         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
        "4028     {\n",
        "4029         PyErr_Format(PyExc_AssertionError,\n",
        "4030                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
        "4031                      CudaNdarray_HOST_STRIDES(C)[0],\n",
        "4032                      CudaNdarray_HOST_STRIDES(C)[1],\n",
        "4033                      CudaNdarray_HOST_DIMS(C)[0],\n",
        "4034                      CudaNdarray_HOST_DIMS(C)[1]);\n",
        "4035         Py_XDECREF(A_new);\n",
        "4036         Py_XDECREF(B_new);\n",
        "4037         return -1;\n",
        "4038     }\n",
        "4039 \n",
        "4040     // the unit integer is divided logically into three fields of 4 bits\n",
        "4041     // the lowermost 4 bits encode the stride pattern of the output\n",
        "4042     // the next higher 4 bits encode the B variable (or y)\n",
        "4043     // the next higher 4 bits encode the C variable (or x)\n",
        "4044     //\n",
        "4045     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
        "4046     //                                                 1 for unit stride from row to row (Col major)\n",
        "4047 \n",
        "4048     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
        "4049     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
        "4050     // If a dimension is 0, its stride will not be used either, so we can\n",
        "4051     // consider it a 'unit' stride too.\n",
        "4052     int unit = 0;\n",
        "4053     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
        "4054         unit |= (0x0 << 8);\n",
        "4055     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
        "4056         unit |= (0x1 << 8);\n",
        "4057     } else {\n",
        "4058         unit |= (0x2 << 8);\n",
        "4059     }\n",
        "4060     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
        "4061         unit |= (0x0 << 4);\n",
        "4062     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
        "4063         unit |= (0x1 << 4);\n",
        "4064     } else {\n",
        "4065         unit |= (0x2 << 4);\n",
        "4066     }\n",
        "4067     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
        "4068         unit |= (0x0 << 0);\n",
        "4069     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
        "4070         unit |= (0x1 << 0);\n",
        "4071     } else {\n",
        "4072         unit |= (0x2 << 0);\n",
        "4073     }\n",
        "4074 \n",
        "4075     /* create appropriate strides for malformed matrices that are row or column\n",
        "4076      * vectors\n",
        "4077      */\n",
        "4078     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
        "4079     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
        "4080     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
        "4081     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
        "4082     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
        "4083     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
        "4084 \n",
        "4085     float* a = CudaNdarray_DEV_DATA(A);\n",
        "4086     float* b = CudaNdarray_DEV_DATA(B);\n",
        "4087     float* c = CudaNdarray_DEV_DATA(C);\n",
        "4088     cublasOperation_t N = CUBLAS_OP_N;\n",
        "4089     cublasOperation_t T = CUBLAS_OP_T;\n",
        "4090     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
        "4091     // There should be no negative stride at that point\n",
        "4092 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
        "4093     if (sx == 0){sx = 1;}\\\n",
        "4094     if (sy == 0){sy = 1;}\\\n",
        "4095     if (sz == 0){sz = 1;}\\\n",
        "4096     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
        "4097         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
        "4098     } else { \\\n",
        "4099         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
        "4100         Py_XDECREF(A_new);\\\n",
        "4101         Py_XDECREF(B_new);\\\n",
        "4102         return -1; \\\n",
        "4103     }\n",
        "4104 \n",
        "4105     cublasStatus_t err;\n",
        "4106     switch(unit)\n",
        "4107     {\n",
        "4108         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
        "4109         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
        "4110         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
        "4111         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
        "4112         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
        "4113         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
        "4114         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
        "4115         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
        "4116         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
        "4117                  return -1;\n",
        "4118     };\n",
        "4119     CNDA_THREAD_SYNC;\n",
        "4120     Py_XDECREF(A_new);\n",
        "4121     Py_XDECREF(B_new);\n",
        "4122 \n",
        "4123     if (CUBLAS_STATUS_SUCCESS != err)\n",
        "4124     {\n",
        "4125         PyErr_Format(PyExc_RuntimeError,\n",
        "4126                      \"cublasSgemm failed (%i) %s\\n\"\n",
        "4127                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
        "4128                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
        "4129                      err,  cublasGetErrorString(err),\n",
        "4130                      unit, N,\n",
        "4131                      CudaNdarray_HOST_DIMS(C)[0],\n",
        "4132                      CudaNdarray_HOST_DIMS(C)[1],\n",
        "4133                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
        "4134                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
        "4135 \n",
        "4136         return -1;\n",
        "4137     }\n",
        "4138     return 0;\n",
        "4139 }\n",
        "4140 \n",
        "4141 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
        "4142 {\n",
        "4143     /**\n",
        "4144     * C <- alpha A B + beta C\n",
        "4145     *    A : matrix\n",
        "4146     *    B, C: vector\n",
        "4147     *    alpha, beta: scalars\n",
        "4148     */\n",
        "4149     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
        "4150     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
        "4151     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
        "4152 \n",
        "4153     // We must allow dimensions to be zeros.\n",
        "4154     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
        "4155             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
        "4156     {\n",
        "4157         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
        "4158                 CudaNdarray_HOST_DIMS(A)[0],\n",
        "4159                 CudaNdarray_HOST_DIMS(A)[1],\n",
        "4160                 CudaNdarray_HOST_DIMS(B)[0],\n",
        "4161                 CudaNdarray_HOST_DIMS(C)[0]);\n",
        "4162         return -1;\n",
        "4163     }\n",
        "4164 \n",
        "4165     // If matrix A has non-unit size and non-unit stride in both\n",
        "4166     // dimensions, or negative strides, we cannot operate, but we can\n",
        "4167     // make a copy.\n",
        "4168     CudaNdarray * A_new = NULL;\n",
        "4169     CudaNdarray * B_new = NULL;\n",
        "4170     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
        "4171          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
        "4172          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
        "4173          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
        "4174         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
        "4175         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
        "4176     {\n",
        "4177         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
        "4178         if (!A_new)\n",
        "4179             return -1;\n",
        "4180         A = A_new;\n",
        "4181     }\n",
        "4182 \n",
        "4183     // If vector B as a negative stride, we also have to make a copy.\n",
        "4184     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
        "4185     {\n",
        "4186         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
        "4187         if (!B_new)\n",
        "4188         {\n",
        "4189             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
        "4190             // do anything\n",
        "4191             Py_XDECREF(A_new);\n",
        "4192             return -1;\n",
        "4193         }\n",
        "4194         B = B_new;\n",
        "4195     }\n",
        "4196 \n",
        "4197     // cudablas does not handle negative strides as expected\n",
        "4198     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
        "4199         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
        "4200     {\n",
        "4201         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
        "4202                 CudaNdarray_HOST_STRIDES(A)[0],\n",
        "4203                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
        "4204         Py_XDECREF(A_new);\n",
        "4205         Py_XDECREF(B_new);\n",
        "4206         return -1;\n",
        "4207     }\n",
        "4208 \n",
        "4209     /* create appropriate strides for malformed matrices that are row or column\n",
        "4210      * vectors\n",
        "4211      */\n",
        "4212     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
        "4213     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
        "4214     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
        "4215     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
        "4216 \n",
        "4217     if (sa_0 == 0)\n",
        "4218         sa_0 = 1;\n",
        "4219     if (sa_1 == 0)\n",
        "4220         sa_1 = 1;\n",
        "4221 \n",
        "4222     // This is important because we can end up not calling Sgemv at all\n",
        "4223     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
        "4224     if (CudaNdarray_SIZE(C)) {\n",
        "4225         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
        "4226             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
        "4227                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
        "4228         {\n",
        "4229             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
        "4230                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
        "4231                     &alpha,\n",
        "4232                     CudaNdarray_DEV_DATA(A), sa_1,\n",
        "4233                     CudaNdarray_DEV_DATA(B), sb_0,\n",
        "4234                     &beta,\n",
        "4235                     CudaNdarray_DEV_DATA(C), sc_0);\n",
        "4236         }\n",
        "4237         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
        "4238                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
        "4239                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
        "4240         {\n",
        "4241             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
        "4242                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
        "4243                     &alpha,\n",
        "4244                     CudaNdarray_DEV_DATA(A), sa_0,\n",
        "4245                     CudaNdarray_DEV_DATA(B), sb_0,\n",
        "4246                     &beta,\n",
        "4247                     CudaNdarray_DEV_DATA(C), sc_0);\n",
        "4248         }\n",
        "4249         else\n",
        "4250         {\n",
        "4251             PyErr_Format(PyExc_AssertionError,\n",
        "4252                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
        "4253                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
        "4254                          CudaNdarray_HOST_STRIDES(A)[0],\n",
        "4255                          CudaNdarray_HOST_STRIDES(A)[1],\n",
        "4256                          CudaNdarray_HOST_STRIDES(B)[0],\n",
        "4257                          CudaNdarray_HOST_STRIDES(C)[0],\n",
        "4258                          CudaNdarray_HOST_DIMS(A)[0],\n",
        "4259                          CudaNdarray_HOST_DIMS(A)[1],\n",
        "4260                          CudaNdarray_HOST_DIMS(B)[0],\n",
        "4261                          CudaNdarray_HOST_DIMS(C)[0]);\n",
        "4262             Py_XDECREF(A_new);\n",
        "4263             Py_XDECREF(B_new);\n",
        "4264             return -1;\n",
        "4265         }\n",
        "4266     }\n",
        "4267 \n",
        "4268     CNDA_THREAD_SYNC;\n",
        "4269     Py_XDECREF(A_new);\n",
        "4270     Py_XDECREF(B_new);\n",
        "4271 \n",
        "4272     if (CUBLAS_STATUS_SUCCESS != err)\n",
        "4273     {\n",
        "4274         PyErr_Format(PyExc_RuntimeError,\n",
        "4275                      \"cublasSgemv failed (%i)\",\n",
        "4276                      err);\n",
        "4277         return -1;\n",
        "4278     }\n",
        "4279     return 0;\n",
        "4280 }\n",
        "4281 \n",
        "4282 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
        "4283     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
        "4284     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
        "4285     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
        "4286 \n",
        "4287     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
        "4288         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
        "4289         PyErr_Format(PyExc_ValueError,\n",
        "4290                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
        "4291                      CudaNdarray_HOST_DIMS(x)[0],\n",
        "4292                      CudaNdarray_HOST_DIMS(y)[0],\n",
        "4293                      CudaNdarray_HOST_DIMS(A)[0],\n",
        "4294                      CudaNdarray_HOST_DIMS(A)[1]);\n",
        "4295         return -1;\n",
        "4296     }\n",
        "4297 \n",
        "4298     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
        "4299     CudaNdarray * x_new = NULL;\n",
        "4300     if(x_strides == 0){\n",
        "4301         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
        "4302             PyErr_Format(PyExc_RuntimeError,\n",
        "4303                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
        "4304                          \" We received a CudaNdarray vector with a stride of 0\"\n",
        "4305                          \" that has more than 1 element!\");\n",
        "4306             return -1;\n",
        "4307         }\n",
        "4308         x_strides = 1;\n",
        "4309     } else if(x_strides < 0){\n",
        "4310         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
        "4311         x = x_new;\n",
        "4312         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
        "4313     }\n",
        "4314 \n",
        "4315     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
        "4316     CudaNdarray * y_new = NULL;\n",
        "4317     if(y_strides == 0){\n",
        "4318         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
        "4319             PyErr_Format(PyExc_RuntimeError,\n",
        "4320                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
        "4321                          \" We received a CudaNdarray vector with a stride of 0\"\n",
        "4322                          \" that has more than 1 elements!\");\n",
        "4323             Py_XDECREF(x_new);\n",
        "4324             return -1;\n",
        "4325         }\n",
        "4326         y_strides = 1;\n",
        "4327     } else if(y_strides < 0){\n",
        "4328         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
        "4329         y = y_new;\n",
        "4330         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
        "4331     }\n",
        "4332 \n",
        "4333     // Create appropriate strides if A is a row or column vector\n",
        "4334     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
        "4335                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
        "4336     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
        "4337                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
        "4338 \n",
        "4339     // This is important because we can end up not calling Sger at all\n",
        "4340     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
        "4341     if(CudaNdarray_SIZE(A)){\n",
        "4342         // If A is in col-major\n",
        "4343         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
        "4344             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
        "4345                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
        "4346         {\n",
        "4347             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
        "4348                        CudaNdarray_DEV_DATA(x), x_strides,\n",
        "4349                        CudaNdarray_DEV_DATA(y), y_strides,\n",
        "4350                        CudaNdarray_DEV_DATA(A), sa_1);\n",
        "4351         }\n",
        "4352         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
        "4353         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
        "4354                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
        "4355                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
        "4356         {\n",
        "4357             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
        "4358                        CudaNdarray_DEV_DATA(y), y_strides,\n",
        "4359                        CudaNdarray_DEV_DATA(x), x_strides,\n",
        "4360                        CudaNdarray_DEV_DATA(A), sa_0);\n",
        "4361         }\n",
        "4362         // A has to be either c- or f-contiguous, with no negative strides\n",
        "4363         else\n",
        "4364         {\n",
        "4365             PyErr_SetString(PyExc_NotImplementedError,\n",
        "4366                             \"non-contiguous A, or negative strides, in sger\");\n",
        "4367             Py_XDECREF(x_new);\n",
        "4368             Py_XDECREF(y_new);\n",
        "4369             return -1;\n",
        "4370         }\n",
        "4371     }\n",
        "4372     CNDA_THREAD_SYNC;\n",
        "4373     Py_XDECREF(x_new);\n",
        "4374     Py_XDECREF(y_new);\n",
        "4375 \n",
        "4376     if (CUBLAS_STATUS_SUCCESS != err)\n",
        "4377     {\n",
        "4378         PyErr_Format(PyExc_RuntimeError,\n",
        "4379                      \"cublasSger failed (%i)\",\n",
        "4380                      err);\n",
        "4381         return -1;\n",
        "4382     }\n",
        "4383 \n",
        "4384     return 0;\n",
        "4385 }\n",
        "4386 \n",
        "4387 /**\n",
        "4388  *\n",
        "4389  * Precondition:\n",
        "4390  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
        "4391  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
        "4392  *\n",
        "4393  *  TODO: templatize this function to support other reductions.\n",
        "4394  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
        "4395  */\n",
        "4396 \n",
        "4397 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
        "4398         const unsigned int nd,\n",
        "4399         const int * dims_a,\n",
        "4400         const int * log2_dims_a,\n",
        "4401         const int * a_str,\n",
        "4402         const float * a_data,\n",
        "4403         const int * z_str,\n",
        "4404         float * z_data)\n",
        "4405 {\n",
        "4406     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "4407     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
        "4408 \n",
        "4409     //structure data contains the strides and dimensions of both a and z\n",
        "4410     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
        "4411     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
        "4412     // a_str[0], ... a_str[nd-1],\n",
        "4413     // z_str[0], ... z_str[nd-1]\n",
        "4414     extern __shared__ int structure_data[];\n",
        "4415     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
        "4416     {\n",
        "4417         structure_data[i+0*nd] = dims_a[i];\n",
        "4418         structure_data[i+1*nd] = log2_dims_a[i];\n",
        "4419         structure_data[i+2*nd] = a_str[i];\n",
        "4420         structure_data[i+3*nd] = z_str[i];\n",
        "4421     }\n",
        "4422     dims_a = structure_data;\n",
        "4423     log2_dims_a = structure_data + nd;\n",
        "4424     a_str = structure_data + 2*nd;\n",
        "4425     z_str = structure_data + 3*nd;\n",
        "4426 \n",
        "4427     __syncthreads(); //wait for all the shared structure to be loaded\n",
        "4428 \n",
        "4429     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
        "4430     {\n",
        "4431         unsigned int ii = i;\n",
        "4432         const float * a_data_i = a_data;\n",
        "4433         float * z_data_i = z_data;\n",
        "4434         unsigned int n_reduce_elements = 1;\n",
        "4435         unsigned int n_reduce_dims = 0;\n",
        "4436         unsigned int reduce_dim0 = nd-1;\n",
        "4437 \n",
        "4438 \n",
        "4439         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
        "4440         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
        "4441         for (unsigned int d = 0; d < nd; ++d)\n",
        "4442         {\n",
        "4443             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
        "4444             {\n",
        "4445                 n_reduce_elements *= dims_a[d];\n",
        "4446                 n_reduce_dims += 1;\n",
        "4447                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
        "4448             }\n",
        "4449             else //'d' is not a dimension that we are reducing over\n",
        "4450             {\n",
        "4451                 unsigned int pos_d;\n",
        "4452                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
        "4453                 {\n",
        "4454                     // this branch is not preferred,\n",
        "4455                     // because the manual said that integer mod and div operations are slow on gpu\n",
        "4456                     pos_d = (ii % dims_a[d]);\n",
        "4457                     ii = (ii / dims_a[d]);\n",
        "4458                 }\n",
        "4459                 else\n",
        "4460                 {\n",
        "4461                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
        "4462                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
        "4463                 }\n",
        "4464                 a_data_i += pos_d * a_str[d];\n",
        "4465                 z_data_i += pos_d * z_str[d];\n",
        "4466             }\n",
        "4467         }\n",
        "4468         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
        "4469         // do a similar loop\n",
        "4470 \n",
        "4471         float sum = 0.0f;\n",
        "4472         switch(n_reduce_dims)\n",
        "4473         {\n",
        "4474             case 0:\n",
        "4475                 {\n",
        "4476                     sum = a_data_i[0];\n",
        "4477                 }\n",
        "4478                 break;\n",
        "4479             case 1:\n",
        "4480                 {\n",
        "4481                     const int stride = a_str[reduce_dim0];\n",
        "4482                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
        "4483                     while (a_data_i != a_data_i_max)\n",
        "4484                     {\n",
        "4485                         sum += a_data_i[0];\n",
        "4486                         a_data_i += stride;\n",
        "4487                     }\n",
        "4488                 }\n",
        "4489                 break;\n",
        "4490             case 2:\n",
        "4491                 {\n",
        "4492                     int rd = reduce_dim0+1;\n",
        "4493                     for (; rd < nd; ++rd)\n",
        "4494                     {\n",
        "4495                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
        "4496                             break;\n",
        "4497                     }\n",
        "4498                     const int stride0 = a_str[reduce_dim0];\n",
        "4499                     const int stride1 = a_str[rd];\n",
        "4500                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
        "4501                     {\n",
        "4502                         const float * a_data_ri = a_data_i + ii * stride1;\n",
        "4503                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
        "4504                         while (a_data_ri != a_data_ri_max)\n",
        "4505                         {\n",
        "4506                             sum += a_data_ri[0];\n",
        "4507                             a_data_ri += stride0;\n",
        "4508                         }\n",
        "4509                     }\n",
        "4510                 };\n",
        "4511                 break;\n",
        "4512             default:\n",
        "4513                 {\n",
        "4514                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
        "4515                     {\n",
        "4516                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
        "4517                         unsigned int reduce_ii = reduce_i;\n",
        "4518                         const float * a_data_ri = a_data_i;\n",
        "4519 \n",
        "4520                         //This loop finds the element in the a slice to add.\n",
        "4521                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
        "4522                         {\n",
        "4523                             unsigned int pos_d;\n",
        "4524                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
        "4525                             {\n",
        "4526                                 if (log2_dims_a[rd]==-1)\n",
        "4527                                 {\n",
        "4528                                     // this branch is not preferred,\n",
        "4529                                     // because the manual said that integer mod and div operations are slow on gpu\n",
        "4530                                     pos_d = (reduce_ii % dims_a[rd]);\n",
        "4531                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
        "4532                                 }\n",
        "4533                                 else\n",
        "4534                                 {\n",
        "4535                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
        "4536                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
        "4537                                 }\n",
        "4538                                 a_data_ri += pos_d * a_str[rd];\n",
        "4539                             }\n",
        "4540                         }\n",
        "4541                         sum += a_data_ri[0];\n",
        "4542                     }\n",
        "4543                 }\n",
        "4544         }\n",
        "4545         z_data_i[0] = sum;\n",
        "4546     }\n",
        "4547 }\n",
        "4548 \n",
        "4549 static __global__ void kernel_reduce_sum_1011(\n",
        "4550         const unsigned int d0,\n",
        "4551         const unsigned int d1,\n",
        "4552         const unsigned int d2,\n",
        "4553         const unsigned int d3,\n",
        "4554         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
        "4555         float * Z, const int sZ0)\n",
        "4556 {\n",
        "4557     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
        "4558     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
        "4559     extern __shared__ float buf[];\n",
        "4560     float mysum = 0.0f;\n",
        "4561 \n",
        "4562     if (warpSize != 32)\n",
        "4563     {\n",
        "4564         return;  //TODO: set error code\n",
        "4565     }\n",
        "4566 \n",
        "4567     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
        "4568     {\n",
        "4569         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
        "4570         mysum += Ai;\n",
        "4571     }\n",
        "4572     buf[threadNum] = mysum;\n",
        "4573     __syncthreads();\n",
        "4574 \n",
        "4575     // rest of function is handled by one warp\n",
        "4576     if (threadNum < warpSize)\n",
        "4577     {\n",
        "4578         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
        "4579         {\n",
        "4580             mysum += buf[i];\n",
        "4581         }\n",
        "4582         buf[threadNum] = mysum;\n",
        "4583         if (threadNum < 16)\n",
        "4584         {\n",
        "4585             //reduce so that threadNum 0 has the sum of everything\n",
        "4586             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
        "4587             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
        "4588             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
        "4589             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
        "4590             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
        "4591             if (threadNum == 0)\n",
        "4592             {\n",
        "4593                 Z[blockIdx.x*sZ0] = buf[0];\n",
        "4594             }\n",
        "4595         }\n",
        "4596     }\n",
        "4597 }\n",
        "4598 /**\n",
        "4599  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
        "4600  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
        "4601  */\n",
        "4602 int\n",
        "4603 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
        "4604 {\n",
        "4605     int verbose = 0;\n",
        "4606     //check input rank\n",
        "4607     if (self->nd != A->nd)\n",
        "4608     {\n",
        "4609         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
        "4610         return -1;\n",
        "4611     }\n",
        "4612     for (int i = 0; i < self->nd; ++i)\n",
        "4613     {\n",
        "4614         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
        "4615         {\n",
        "4616             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
        "4617                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
        "4618             return -1;\n",
        "4619         }\n",
        "4620     }\n",
        "4621 \n",
        "4622     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
        "4623     if (verbose)\n",
        "4624     {\n",
        "4625         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
        "4626         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
        "4627         fprint_CudaNdarray(stderr, A);\n",
        "4628         fprint_CudaNdarray(stderr, self);\n",
        "4629     }\n",
        "4630     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
        "4631             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
        "4632             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
        "4633             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
        "4634        )\n",
        "4635     {\n",
        "4636         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
        "4637         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
        "4638         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
        "4639         n_threads.z -= 1;\n",
        "4640         if (n_threads.z > 64) n_threads.z = 64;\n",
        "4641         if (n_threads.z)\n",
        "4642         {\n",
        "4643             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
        "4644             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
        "4645             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
        "4646                     CudaNdarray_HOST_DIMS(A)[0],\n",
        "4647                     CudaNdarray_HOST_DIMS(A)[1],\n",
        "4648                     CudaNdarray_HOST_DIMS(A)[2],\n",
        "4649                     CudaNdarray_HOST_DIMS(A)[3],\n",
        "4650                     CudaNdarray_DEV_DATA(A),\n",
        "4651                     CudaNdarray_HOST_STRIDES(A)[0],\n",
        "4652                     CudaNdarray_HOST_STRIDES(A)[1],\n",
        "4653                     CudaNdarray_HOST_STRIDES(A)[2],\n",
        "4654                     CudaNdarray_HOST_STRIDES(A)[3],\n",
        "4655                     CudaNdarray_DEV_DATA(self),\n",
        "4656                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
        "4657             CNDA_THREAD_SYNC;\n",
        "4658             if (cudaSuccess == cudaGetLastError()) return 0;\n",
        "4659             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
        "4660         }\n",
        "4661     }\n",
        "4662 \n",
        "4663     int n_threads_per_block = std::min(n_summations,\n",
        "4664             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
        "4665     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
        "4666             NUM_VECTOR_OP_BLOCKS);\n",
        "4667     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
        "4668 \n",
        "4669     if (verbose)\n",
        "4670     {\n",
        "4671         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
        "4672     }\n",
        "4673     assert (self->nd > 0);\n",
        "4674     assert (self->nd == A->nd);\n",
        "4675     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
        "4676             n_summations,\n",
        "4677             self->nd,\n",
        "4678             CudaNdarray_DEV_DIMS(A),\n",
        "4679             CudaNdarray_DEV_LOG2DIMS(A),\n",
        "4680             CudaNdarray_DEV_STRIDES(A),\n",
        "4681             CudaNdarray_DEV_DATA(A),\n",
        "4682             CudaNdarray_DEV_STRIDES(self),\n",
        "4683             CudaNdarray_DEV_DATA(self));\n",
        "4684     CNDA_THREAD_SYNC;\n",
        "4685     cudaError_t err = cudaGetLastError();\n",
        "4686     if (cudaSuccess != err)\n",
        "4687     {\n",
        "4688         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
        "4689         return -1;\n",
        "4690     }\n",
        "4691     return 0;\n",
        "4692 }\n",
        "4693 int\n",
        "4694 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
        "4695 {\n",
        "4696     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
        "4697     return -1;\n",
        "4698 }\n",
        "4699 int\n",
        "4700 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
        "4701 {\n",
        "4702     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
        "4703     return -1;\n",
        "4704 }\n",
        "4705 int\n",
        "4706 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
        "4707 {\n",
        "4708     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
        "4709     return -1;\n",
        "4710 }\n",
        "4711 \n",
        "4712 \n",
        "4713 /**\n",
        "4714  *\n",
        "4715  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
        "4716  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
        "4717  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
        "4718  *\n",
        "4719  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
        "4720  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
        "4721  */\n",
        "4722 int\n",
        "4723 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
        "4724 {\n",
        "4725     //TODO: pass a workspace pointer to avoid the internal malloc\n",
        "4726     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
        "4727     int * newstrides = newdims + len;\n",
        "4728     int * dims_taken = newstrides + len;\n",
        "4729     if (!newdims)\n",
        "4730     {\n",
        "4731         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
        "4732         return -1;\n",
        "4733     }\n",
        "4734     for (int i = 0; i < self->nd; ++i)\n",
        "4735     {\n",
        "4736         dims_taken[i] = 0;\n",
        "4737     }\n",
        "4738     for (int i = 0; i < len; ++i)\n",
        "4739     {\n",
        "4740         if (pattern[i] < 0)\n",
        "4741         {\n",
        "4742             newdims[i] = 1;\n",
        "4743             newstrides[i] = 0;\n",
        "4744         }\n",
        "4745         else if(dims_taken[pattern[i]])\n",
        "4746         {\n",
        "4747             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
        "4748                          pattern[i]);\n",
        "4749             free(newdims);\n",
        "4750             return -1;\n",
        "4751         }\n",
        "4752         else if (pattern[i]>= self->nd)\n",
        "4753         {\n",
        "4754             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
        "4755                          pattern[i], self->nd);\n",
        "4756             free(newdims);\n",
        "4757             return -1;\n",
        "4758         }\n",
        "4759         else\n",
        "4760         {\n",
        "4761             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
        "4762             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
        "4763             dims_taken[pattern[i]] = 1;\n",
        "4764         }\n",
        "4765     }\n",
        "4766     //Check if we dropped not broadcastable dims\n",
        "4767     for (int i = 0; i < self->nd; ++i)\n",
        "4768     {\n",
        "4769         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
        "4770         {\n",
        "4771             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
        "4772             free(newdims);\n",
        "4773             return -1;\n",
        "4774         }\n",
        "4775     }\n",
        "4776     //swap this structure in for the one in self, and sync to the card\n",
        "4777     if (CudaNdarray_set_nd(self, len))\n",
        "4778     {\n",
        "4779         free(newdims);\n",
        "4780         return -1;\n",
        "4781     }\n",
        "4782     for (int i = 0; i < len; ++i)\n",
        "4783     {\n",
        "4784         CudaNdarray_set_dim(self, i, newdims[i]);\n",
        "4785         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
        "4786     }\n",
        "4787     if (cnda_copy_structure_to_device(self))\n",
        "4788     {\n",
        "4789         free(newdims);\n",
        "4790         return -1;\n",
        "4791     }\n",
        "4792     free(newdims);\n",
        "4793     return 0;\n",
        "4794 }\n",
        "4795 \n",
        "4796 \n",
        "4797 \n",
        "4798 /**\n",
        "4799  *\n",
        "4800  *  This is the function that bind to python.\n",
        "4801  *  See CudaNdarray_dimshuffle to call from C.\n",
        "4802  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
        "4803  */\n",
        "4804 PyObject *\n",
        "4805 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
        "4806 {\n",
        "4807     PyObject * self = NULL;\n",
        "4808     PyObject * pattern_object = NULL;\n",
        "4809     int * pattern = NULL;\n",
        "4810     PyObject * rval = NULL;\n",
        "4811     int success = -1;\n",
        "4812     //const int * dims = NULL;\n",
        "4813 \n",
        "4814     //args should consist of two python objects (\"OO\")\n",
        "4815     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
        "4816         return NULL;\n",
        "4817 \n",
        "4818     if (!CudaNdarray_Check(self) )\n",
        "4819     {\n",
        "4820         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
        "4821         return NULL;\n",
        "4822     }\n",
        "4823 \n",
        "4824     //parse pattern_object into int * pattern\n",
        "4825 \n",
        "4826     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
        "4827 \n",
        "4828     if (pattern_dim < 0)\n",
        "4829     {\n",
        "4830         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
        "4831         return NULL;\n",
        "4832     }\n",
        "4833 \n",
        "4834     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
        "4835 \n",
        "4836     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
        "4837     {\n",
        "4838         PyObject * idx = PyLong_FromLong(i);\n",
        "4839 \n",
        "4840         if (idx == NULL)\n",
        "4841         {\n",
        "4842             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
        "4843             goto CudaNdarray_dimshuffle_fail;\n",
        "4844         }\n",
        "4845 \n",
        "4846         long elem_value = 0;\n",
        "4847 \n",
        "4848         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
        "4849 \n",
        "4850         if (elem == NULL)\n",
        "4851         {\n",
        "4852             Py_XDECREF( elem);\n",
        "4853             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
        "4854             goto CudaNdarray_dimshuffle_fail;\n",
        "4855         }\n",
        "4856 \n",
        "4857         elem_value = PyInt_AsLong(elem);\n",
        "4858 \n",
        "4859         if (elem_value == -1 && PyErr_Occurred() )\n",
        "4860         {\n",
        "4861             Py_XDECREF(elem);\n",
        "4862             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
        "4863             goto CudaNdarray_dimshuffle_fail;\n",
        "4864         }\n",
        "4865 \n",
        "4866         pattern[i] = elem_value;\n",
        "4867 \n",
        "4868         Py_XDECREF( elem );\n",
        "4869         Py_XDECREF( idx );\n",
        "4870     }\n",
        "4871 \n",
        "4872     //allocate rval\n",
        "4873     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
        "4874 \n",
        "4875     if (rval == NULL)\n",
        "4876     {\n",
        "4877         //CudaNdarray_New should have set the exception string\n",
        "4878         goto CudaNdarray_dimshuffle_fail;\n",
        "4879     }\n",
        "4880 \n",
        "4881 \n",
        "4882     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
        "4883     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
        "4884     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
        "4885     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
        "4886 \n",
        "4887     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
        "4888 \n",
        "4889     if (success != 0)\n",
        "4890     {\n",
        "4891         //Exception string should already be set by CudaNdarray_dimshuffle\n",
        "4892         goto CudaNdarray_dimshuffle_fail;\n",
        "4893     }\n",
        "4894 \n",
        "4895     free(pattern);\n",
        "4896 \n",
        "4897     return rval;\n",
        "4898 \n",
        "4899     CudaNdarray_dimshuffle_fail:\n",
        "4900 \n",
        "4901     if (pattern != NULL)\n",
        "4902         free(pattern);\n",
        "4903 \n",
        "4904     Py_XDECREF(rval);\n",
        "4905     return NULL;\n",
        "4906 }\n",
        "4907 \n",
        "4908 \n",
        "4909 int\n",
        "4910 cnda_structure_size(int nd)\n",
        "4911 {\n",
        "4912     // dim0, dim1, ...\n",
        "4913     // str0, str1, ...\n",
        "4914     // log2(dim0), log2(dim1), ...\n",
        "4915     return nd + nd + nd;\n",
        "4916 }\n",
        "4917 \n",
        "4918 const int *\n",
        "4919 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
        "4920 {\n",
        "4921     return self->host_structure;\n",
        "4922 }\n",
        "4923 \n",
        "4924 const int *\n",
        "4925 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
        "4926 {\n",
        "4927     return self->host_structure + self->nd;\n",
        "4928 }\n",
        "4929 const int *\n",
        "4930 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
        "4931 {\n",
        "4932     return self->host_structure + 2*self->nd;\n",
        "4933 }\n",
        "4934 \n",
        "4935 int\n",
        "4936 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
        "4937 {\n",
        "4938     int verbose = 0;\n",
        "4939 \n",
        "4940     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
        "4941     {\n",
        "4942         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
        "4943         return 0;\n",
        "4944     }\n",
        "4945 \n",
        "4946     if (cnda1->nd != cnda2->nd)\n",
        "4947     {\n",
        "4948         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
        "4949         return 0;\n",
        "4950     }\n",
        "4951 \n",
        "4952     for (int i=0; i < 2*cnda1->nd; i++)\n",
        "4953     {\n",
        "4954         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
        "4955         {\n",
        "4956             if(verbose)\n",
        "4957                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
        "4958             return 0;\n",
        "4959         }\n",
        "4960     }\n",
        "4961 \n",
        "4962     if (!ignoreBase && cnda1->base != cnda2->base)\n",
        "4963     {\n",
        "4964         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
        "4965         return 0;\n",
        "4966     }\n",
        "4967     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
        "4968     {\n",
        "4969         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
        "4970         return 0;\n",
        "4971     }\n",
        "4972     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
        "4973     {\n",
        "4974         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
        "4975         // no need to check devdata if data is not allocated\n",
        "4976         return 0;\n",
        "4977     }\n",
        "4978 \n",
        "4979     return 1;\n",
        "4980 }\n",
        "4981 \n",
        "4982 \n",
        "4983 int\n",
        "4984 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
        "4985 {\n",
        "4986     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
        "4987 }\n",
        "4988 \n",
        "4989 int\n",
        "4990 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
        "4991 {\n",
        "4992     //If the device structure do not exists, create it.\n",
        "4993     //We allocate it here as we do not need it often.\n",
        "4994     //In fact, we need it so infrequently that we expect\n",
        "4995     //that most object won't need it. Not allocating it\n",
        "4996     //save a significant when creating object.\n",
        "4997     //This speed up a benchmark by 8% with the gc.\n",
        "4998     if (!self->dev_structure)\n",
        "4999     {\n",
        "5000         int struct_size = cnda_structure_size(self->nd);\n",
        "5001         if (struct_size)\n",
        "5002         {\n",
        "5003             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
        "5004             if (NULL == self->dev_structure)\n",
        "5005             {\n",
        "5006                 return -1;\n",
        "5007             }\n",
        "5008         }\n",
        "5009     }\n",
        "5010     if (cublasSetVector(cnda_structure_size(self->nd),\n",
        "5011                         sizeof(int),\n",
        "5012                         self->host_structure,\n",
        "5013                         1,\n",
        "5014                         self->dev_structure,\n",
        "5015                         1) != CUBLAS_STATUS_SUCCESS)\n",
        "5016     {\n",
        "5017         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
        "5018         return -1;\n",
        "5019     }\n",
        "5020     self->dev_structure_fresh = 1;\n",
        "5021     return 0;\n",
        "5022 }\n",
        "5023 \n",
        "5024 const int *\n",
        "5025 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
        "5026 {\n",
        "5027     if (!self->dev_structure_fresh)\n",
        "5028     {\n",
        "5029         if (cnda_copy_structure_to_device(self))\n",
        "5030             return NULL;\n",
        "5031     }\n",
        "5032     return self->dev_structure;\n",
        "5033 }\n",
        "5034 const int *\n",
        "5035 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
        "5036 {\n",
        "5037     if (!self->dev_structure_fresh)\n",
        "5038     {\n",
        "5039         if (cnda_copy_structure_to_device(self))\n",
        "5040             return NULL;\n",
        "5041     }\n",
        "5042     return self->dev_structure + self->nd;\n",
        "5043 }\n",
        "5044 const int *\n",
        "5045 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
        "5046 {\n",
        "5047     if (!self->dev_structure_fresh)\n",
        "5048     {\n",
        "5049         if (cnda_copy_structure_to_device(self))\n",
        "5050             return NULL;\n",
        "5051     }\n",
        "5052     return self->dev_structure + 2*self->nd;\n",
        "5053 }\n",
        "5054 float *\n",
        "5055 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
        "5056 {\n",
        "5057     return self->devdata;\n",
        "5058 }\n",
        "5059 \n",
        "5060 /**\n",
        "5061  * Return the number of elements in the ndarray (product of the dimensions)\n",
        "5062  */\n",
        "5063 int\n",
        "5064 CudaNdarray_SIZE(const CudaNdarray *self)\n",
        "5065 {\n",
        "5066     if (self->nd == -1) return 0;\n",
        "5067     int size = 1;\n",
        "5068     for (int i = 0; i < self->nd; ++i)\n",
        "5069     {\n",
        "5070         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
        "5071     }\n",
        "5072     return size;\n",
        "5073 }\n",
        "5074 \n",
        "5075 PyObject *\n",
        "5076 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
        "5077 {\n",
        "5078     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
        "5079 }\n",
        "5080 \n",
        "5081 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
        "5082 {\n",
        "5083     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
        "5084 }\n",
        "5085 \n",
        "5086 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
        "5087 {\n",
        "5088     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
        "5089 }\n",
        "5090 \n",
        "5091 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
        "5092 {\n",
        "5093     cudaError_t err = cudaGetLastError();\n",
        "5094     if( cudaSuccess != err)\n",
        "5095     {\n",
        "5096         PyErr_Format(PyExc_RuntimeError,\n",
        "5097                      \"Cuda error: %s: %s.\",\n",
        "5098                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
        "5099                      cudaGetErrorString(err));\n",
        "5100         return -1;\n",
        "5101     }\n",
        "5102     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
        "5103             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
        "5104     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
        "5105     for (int i = 0; i < self->nd; ++i)\n",
        "5106     {\n",
        "5107         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
        "5108     }\n",
        "5109     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
        "5110     for (int i = 0; i < self->nd; ++i)\n",
        "5111     {\n",
        "5112         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
        "5113     }\n",
        "5114 \n",
        "5115     if (self->dev_structure)\n",
        "5116     {\n",
        "5117         int data=0;\n",
        "5118         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
        "5119         for (int i = 0; i < self->nd; ++i)\n",
        "5120         {\n",
        "5121             cublasGetVector(1, sizeof(int),\n",
        "5122                             self->dev_structure+i, 1,\n",
        "5123                             &data, 1);\n",
        "5124             fprintf(fd, \"%i\\t\", data);\n",
        "5125         }\n",
        "5126         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
        "5127         for (int i = 0; i < self->nd; ++i)\n",
        "5128         {\n",
        "5129             cublasGetVector(1, sizeof(int),\n",
        "5130                             self->dev_structure + self->nd+i, 1,\n",
        "5131                             &data, 1);\n",
        "5132             fprintf(fd, \"%i \\t\", data);\n",
        "5133         }\n",
        "5134         fprintf(fd, \"\\n\");\n",
        "5135     }\n",
        "5136     else\n",
        "5137     {\n",
        "5138         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
        "5139     }\n",
        "5140 \n",
        "5141     err = cudaGetLastError();\n",
        "5142     if( cudaSuccess != err)\n",
        "5143     {\n",
        "5144         PyErr_Format(PyExc_RuntimeError,\n",
        "5145                      \"Cuda error: %s: %s.\",\n",
        "5146                      \"fprint_CudaNdarray\",\n",
        "5147                      cudaGetErrorString(err));\n",
        "5148         return -1;\n",
        "5149     }\n",
        "5150     return 0;\n",
        "5151 }\n",
        "5152 \n",
        "5153 \n",
        "5154 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
        "5155                             const int * dims, int fortran)\n",
        "5156 {\n",
        "5157     bool allocated = false;\n",
        "5158     if (*arr == NULL)\n",
        "5159     {\n",
        "5160         // This allocates the metadata but not the data\n",
        "5161         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
        "5162         if (*arr == NULL)\n",
        "5163             return -1;\n",
        "5164         allocated = true;\n",
        "5165     }\n",
        "5166 \n",
        "5167     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
        "5168     {\n",
        "5169         if (allocated)\n",
        "5170         {\n",
        "5171             Py_DECREF(*arr);\n",
        "5172             *arr = NULL;\n",
        "5173         }\n",
        "5174         return -1;\n",
        "5175     }\n",
        "5176     return 0;\n",
        "5177 }\n",
        "5178 \n",
        "5179 \n",
        "5180 /*\n",
        "5181   Local Variables:\n",
        "5182   mode:c++\n",
        "5183   c-basic-offset:4\n",
        "5184 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  c-file-style:\"stroustrup\"\n",
        "5185   indent-tabs-mode:nil\n",
        "5186   fill-column:79\n",
        "5187   End:\n",
        "5188 */\n",
        "5189 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
        "5190 \n",
        "===============================\n",
        "z:\\theano\\theano\\sandbox\\cuda\\cuda_ndarray.cuh(39) : fatal error C1083: Cannot open include file: 'stdint.h': No such file or directory\r\n",
        "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 2, 'for cmd', 'nvcc -shared -O3 -Xlinker /DEBUG -m64 -Xcompiler -DCUDA_NDARRAY_CUH=1169b416617a5217d1fd0f7a952f428b,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD -Iz:\\\\theano\\\\theano\\\\sandbox\\\\cuda -IC:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include -IC:\\\\Anaconda\\\\include -o C:\\\\Users\\\\mfiedler\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.7-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd mod.cu -LC:\\\\Anaconda\\\\libs -LC:\\\\Anaconda -lpython27 -lcublas -lcudart')\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: ('nvcc return status', 2, 'for cmd', 'nvcc -shared -O3 -Xlinker /DEBUG -m64 -Xcompiler -DCUDA_NDARRAY_CUH=1169b416617a5217d1fd0f7a952f428b,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD -Iz:\\\\theano\\\\theano\\\\sandbox\\\\cuda -IC:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include -IC:\\\\Anaconda\\\\include -o C:\\\\Users\\\\mfiedler\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.7-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd mod.cu -LC:\\\\Anaconda\\\\libs -LC:\\\\Anaconda -lpython27 -lcublas -lcudart')\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['nvcc', '-shared', '-O3', '-Xlinker', '/DEBUG', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=1169b416617a5217d1fd0f7a952f428b,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD', '-Iz:\\\\theano\\\\theano\\\\sandbox\\\\cuda', '-IC:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include', '-IC:\\\\Anaconda\\\\include', '-o', 'C:\\\\Users\\\\mfiedler\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.7-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd', 'mod.cu', '-LC:\\\\Anaconda\\\\libs', '-LC:\\\\Anaconda', '-lpython27', '-lcublas', '-lcudart']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prepare Data\n",
      "============\n",
      "\n",
      "We load the MNIST data. It is available at http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz.\n",
      "\n",
      "The inputs all lie between 0 and 1. Each target is a variable where the correct class is marked with a ``1`` and the wrong classes with ``-1``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafile = 'mnist.pkl.gz'\n",
      "# Load data.                                                                                                   \n",
      "\n",
      "with gzip.open(datafile,'rb') as f:                                                                        \n",
      "    train_set, val_set, test_set = cPickle.load(f)                                                       \n",
      "\n",
      "X, Z = train_set                                                                                               \n",
      "VX, VZ = val_set\n",
      "TX, TZ = test_set\n",
      "\n",
      "Z = one_hot(Z, 10)\n",
      "VZ = one_hot(VZ, 10)\n",
      "TZ = one_hot(TZ, 10)\n",
      "\n",
      "image_dims = 28, 28\n",
      "\n",
      "X, Z, VX, VZ, TX, TZ = [breze.learn.base.cast_array_to_local_type(i) for i in (X, Z, VX, VZ, TX, TZ)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Be mindful of the combination of optimizer, loss and output transfer you choose. Make sure to check the documentation, e.g. to see what form of input the chosen loss function expects. \n",
      "'cat_ce', e.g. expects an array in which the rows have the same dimensionality as in the one-hot target matrix and its entries have to be normalized to represent the prediction probabilities that the input belongs to a specific target class; i.e. be normalized to be between 0 and 1 and sum up to 1 row-wise, which means it has to be combined with 'softmax' or 'simple_normalize' as output transfer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_passes = 20\n",
      "batch_size = 500\n",
      "max_iter = max_passes * X.shape[0] / batch_size\n",
      "n_report = X.shape[0] / batch_size\n",
      "\n",
      "stop = climin.stops.AfterNIterations(max_iter)\n",
      "pause = climin.stops.ModuloNIterations(n_report)\n",
      "\n",
      "optimizer = 'adadelta', {'steprate': 0.9, 'momentum': 0.7, 'decay': 0.7}\n",
      "#optimizer = 'gd', {'steprate': 0.1}\n",
      "#optimizer = dropout_optimizer_conf(steprate_0=1, n_repeats=1)\n",
      "m = Cnn(784, [50], [500], 10, ['tanh'], ['tanh'], out_transfer='simple_normalize',\n",
      "        loss='cat_ce', image_height=28, image_width=28, n_image_channel=1, optimizer=optimizer, batch_size=batch_size, max_iter=max_iter, \n",
      "        init_weights_stdev=[0.001, 0.001, 0.001], init_biases_stdev=[0.001, 0.001, 0.001])\n",
      "\n",
      "m.parameters.data[...] = np.random.normal(0, 0.01, m.parameters.data.shape)\n",
      "\n",
      "m.draw_params()\n",
      "    \n",
      "def f_n_wrong(x, z):\n",
      "    y = m.predict(x)\n",
      "    print y[:8] # print first 8 rows of predicition to better understand how algorithm works\n",
      "    print z[:8] # and compare them to the first 8 rows of the target at each iteration\n",
      "    return (y.argmax(axis=1) != z.argmax(axis=1)).sum()\n",
      "        \n",
      "    \n",
      "losses = []\n",
      "print 'max iter', max_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "resulting image size\n",
        "144\n",
        "image shapes\n",
        "(2L, 4L)\n",
        "[(500, 1, 28, 28), (500, 50, 12, 12)]\n",
        "n_inpt(500, 1, 28, 28)\n",
        "reshaped input"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(500, 1, 28, 28)\n",
        "image shapes in simple.py\n",
        "[500   1  28  28]\n",
        "[500  50  12  12]\n",
        "padding 0\n",
        "filtershape: (50, 1, 5, 5)\n",
        "pool shape: [2, 2]\n",
        "pool shift: [[0], [0]]\n",
        "number of inputs to fully connected layer: 7200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "max iter"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 600\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "# Set up a nice printout.\n",
      "keys = '#', 'seconds', 'loss', 'val loss', 'train emp', 'test emp'\n",
      "max_len = max(len(i) for i in keys)\n",
      "header = '\\t'.join(i for i in keys)\n",
      "print header\n",
      "print '-' * len(header)\n",
      "\n",
      "for i, info in enumerate(m.powerfit((X, Z), (VX, VZ), stop, pause)):\n",
      "    if info['n_iter'] % n_report != 0:\n",
      "        continue\n",
      "    passed = time.time() - start\n",
      "    losses.append((info['loss'], info['val_loss']))\n",
      "    \n",
      "    #img = tile_raster_images(fe.parameters['in_to_hidden'].T, image_dims, feature_dims, (1, 1))\n",
      "    #save_and_display(img, 'filters-%i.png' % i)  \n",
      "    info.update({\n",
      "        'time': passed,\n",
      "        'train_emp': f_n_wrong(X, Z),\n",
      "        'test_emp': f_n_wrong(TX, TZ),\n",
      "    })\n",
      "    row = '%(n_iter)i\\t%(time)g\\t%(loss)g\\t%(val_loss)g\\t%(train_emp)g\\t%(test_emp)g' % info\n",
      "    print row"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#\tseconds\tloss\tval loss\ttrain emp\ttest emp\n",
        "------------------------------------------\n",
        "[[ 0.04867788  0.14942027  0.14166437  0.07454935  0.          0.17813832\n",
        "   0.09277107  0.07276681  0.14605425  0.09595766]\n",
        " [ 0.37902627  0.02342004  0.10980849  0.06319074  0.          0.14576961\n",
        "   0.09634795  0.03552078  0.03109361  0.1158225 ]\n",
        " [ 0.13091843  0.06397154  0.11453051  0.08046601  0.1564664   0.08944085\n",
        "   0.02911911  0.16141763  0.          0.17366952]\n",
        " [ 0.03136345  0.29682643  0.13252298  0.0785698   0.08922568  0.15064149\n",
        "   0.          0.02050225  0.07766939  0.12267852]\n",
        " [ 0.01238443  0.17072726  0.08118582  0.          0.23354248  0.07959699\n",
        "   0.02819108  0.08971758  0.00410068  0.30055369]\n",
        " [ 0.0396726   0.15934115  0.26443789  0.          0.10203829  0.07841084\n",
        "   0.0756684   0.05458964  0.0431659   0.18267527]\n",
        " [ 0.05869213  0.25560083  0.09889749  0.08755131  0.04582021  0.13580068\n",
        "   0.          0.04422897  0.17898686  0.09442152]\n",
        " [ 0.04991428  0.07829605  0.13291142  0.19507303  0.00887464  0.14052385\n",
        "   0.          0.06200402  0.17168528  0.16071742]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
        "[[ 0.04160713  0.12930354  0.01655881  0.0494123   0.06821719  0.04304457\n",
        "   0.          0.3975136   0.02882715  0.22551571]\n",
        " [ 0.13129919  0.12315664  0.17550948  0.10113886  0.03487745  0.16536231\n",
        "   0.10677242  0.05733828  0.10454537  0.        ]\n",
        " [ 0.08171065  0.46191406  0.06095229  0.07983521  0.          0.10769451\n",
        "   0.06718204  0.02429703  0.08865961  0.02775459]\n",
        " [ 0.36588185  0.11246192  0.10383395  0.04228394  0.          0.09212937\n",
        "   0.10665431  0.10493587  0.03638106  0.03543774]\n",
        " [ 0.08382896  0.02481647  0.06745753  0.          0.16057855  0.14684265\n",
        "   0.0823828   0.19003673  0.05565328  0.18840302]\n",
        " [ 0.08554245  0.35257288  0.          0.05095896  0.05146079  0.09326724\n",
        "   0.07313144  0.08298469  0.1105648   0.09951674]\n",
        " [ 0.0757692   0.          0.03311616  0.09019925  0.1477843   0.11647046\n",
        "   0.07252774  0.18917293  0.14218665  0.1327733 ]\n",
        " [ 0.00366092  0.1351257   0.04661196  0.0193448   0.19356561  0.12655539\n",
        "   0.08117362  0.          0.06293862  0.33102338]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
        "100\t130.2\t1.57748\t1.54137\t17346\t3310\n",
        "[[ 0.01211915  0.08093152  0.          0.23542151  0.02629834  0.27572917\n",
        "   0.15783393  0.14914764  0.04603627  0.01648248]\n",
        " [ 0.45245135  0.12687296  0.          0.03405079  0.0440709   0.12050634\n",
        "   0.06577896  0.07282935  0.05283124  0.03060811]\n",
        " [ 0.04778969  0.15454957  0.06150931  0.06258958  0.42533298  0.00626727\n",
        "   0.05353388  0.          0.00766711  0.18076063]\n",
        " [ 0.03320234  0.47322185  0.06894914  0.01725769  0.09611192  0.09204849\n",
        "   0.04099707  0.01299637  0.16521514  0.        ]\n",
        " [ 0.01288342  0.07766836  0.09077322  0.06142974  0.09652135  0.02127802\n",
        "   0.          0.09842076  0.20459044  0.33643469]\n",
        " [ 0.06982187  0.          0.19960693  0.08326828  0.12194748  0.05984912\n",
        "   0.11809547  0.09296493  0.13873259  0.11571333]\n",
        " [ 0.          0.32150156  0.04541836  0.14033666  0.07102037  0.06721028\n",
        "   0.07562126  0.05040687  0.1141486   0.11433604]\n",
        " [ 0.09896748  0.04572952  0.1288903   0.30624407  0.00878566  0.11531632\n",
        "   0.          0.06248986  0.15503928  0.07853752]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
        "[[ 0.          0.00078758  0.03308365  0.09145381  0.04559742  0.04735567\n",
        "   0.14895741  0.50836756  0.02991351  0.09448339]\n",
        " [ 0.14083994  0.08338043  0.21056641  0.13449525  0.06890637  0.11386404\n",
        "   0.15153967  0.0461039   0.05030398  0.        ]\n",
        " [ 0.01884329  0.69053055  0.02942709  0.03922109  0.02504759  0.06290182\n",
        "   0.          0.03159343  0.05202252  0.05041261]\n",
        " [ 0.46822727  0.05878238  0.08290232  0.04122976  0.06278429  0.\n",
        "   0.08531671  0.09439854  0.02987205  0.07648668]\n",
        " [ 0.04842454  0.00628643  0.10372892  0.          0.32202411  0.07035093\n",
        "   0.14775264  0.06840468  0.0731254   0.15990236]\n",
        " [ 0.          0.51329285  0.03371681  0.04866302  0.04823716  0.11647968\n",
        "   0.02325882  0.12083705  0.01185111  0.0836635 ]\n",
        " [ 0.04066546  0.          0.01714968  0.09598052  0.14079637  0.08714252\n",
        "   0.02593381  0.34498281  0.07542585  0.17192298]\n",
        " [ 0.          0.10021563  0.08821275  0.06950709  0.24350209  0.06073471\n",
        "   0.14473014  0.02485169  0.05505728  0.21318859]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
        "200\t286.442\t1.33432\t1.30198\t10808\t2065\n",
        "[[ 0.          0.076228    0.09120268  0.19375111  0.07686693  0.17078047\n",
        "   0.10680362  0.05430996  0.11994972  0.11010752]\n",
        " [ 0.46382775  0.04996079  0.          0.16892412  0.04152331  0.05673932\n",
        "   0.03145862  0.08074557  0.01953544  0.08728507]\n",
        " [ 0.03926253  0.11762853  0.08706594  0.14711128  0.27690893  0.04212672\n",
        "   0.04668742  0.07902742  0.          0.16418123]\n",
        " [ 0.00060965  0.42172774  0.06563685  0.17350082  0.07604099  0.\n",
        "   0.05667619  0.07254182  0.06705908  0.06620687]\n",
        " [ 0.          0.06761827  0.01441019  0.18407155  0.24978165  0.00573663\n",
        "   0.02790676  0.08396311  0.05394152  0.31257032]\n",
        " [ 0.08742124  0.          0.25880148  0.16215519  0.06737869  0.00964668\n",
        "   0.05459019  0.12230852  0.11668813  0.12100989]\n",
        " [ 0.0228534   0.31224697  0.05330868  0.21624335  0.160606    0.\n",
        "   0.04241519  0.08053708  0.0453034   0.06648593]\n",
        " [ 0.0784831   0.05269015  0.1081613   0.2879504   0.08010404  0.05707714\n",
        "   0.          0.08901755  0.13157696  0.11493936]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
        "[[  3.14992200e-02   5.67187825e-02   9.29752065e-02   1.51409232e-01\n",
        "    7.51471323e-02   2.79861649e-02   0.00000000e+00   3.94803957e-01\n",
        "    8.00311181e-02   8.94291870e-02]\n",
        " [  1.49085779e-01   1.21340915e-01   1.57622446e-01   1.73738149e-01\n",
        "    4.86723369e-02   1.08362538e-01   7.39081218e-02   7.82959009e-02\n",
        "    8.89738121e-02   0.00000000e+00]\n",
        " [  2.86545595e-02   4.27978467e-01   3.54898216e-02   2.00085954e-01\n",
        "    6.83267838e-02   1.19374779e-04   0.00000000e+00   1.03801703e-01\n",
        "    8.19076627e-03   1.27352570e-01]\n",
        " [  3.13164395e-01   9.62159469e-02   8.91339722e-02   1.24503574e-01\n",
        "    5.20393282e-02   0.00000000e+00   1.15696864e-01   1.01000045e-01\n",
        "    3.80548290e-02   7.01910449e-02]\n",
        " [  0.00000000e+00   2.42376674e-02   6.94440524e-02   1.08221501e-01\n",
        "    2.95348900e-01   1.00332294e-02   1.78062856e-01   7.86475288e-02\n",
        "    5.30274069e-02   1.82976858e-01]\n",
        " [  5.16832759e-02   3.45752106e-01   6.02369161e-02   1.58848106e-01\n",
        "    8.60996776e-02   0.00000000e+00   2.88184537e-02   1.01799412e-01\n",
        "    4.32077044e-02   1.23554348e-01]\n",
        " [  1.95447591e-02   0.00000000e+00   1.85632692e-02   1.12368782e-01\n",
        "    2.32090566e-01   9.96064622e-02   8.66716014e-02   1.70742701e-01\n",
        "    9.20314873e-02   1.68380373e-01]\n",
        " [  0.00000000e+00   1.13237015e-01   1.05767021e-01   8.82681526e-02\n",
        "    2.02738017e-01   7.83698897e-02   1.49804980e-01   5.05423254e-02\n",
        "    2.84369053e-02   1.82835695e-01]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
        "300\t441.574\t1.35053\t1.32296\t11491\t2245\n",
        "[[ 0.          0.07455315  0.04713159  0.16605888  0.08683458  0.24059998\n",
        "   0.09055207  0.06523131  0.12196512  0.10707332]\n",
        " [ 0.67044809  0.04967142  0.03419349  0.01606859  0.00368508  0.07414235\n",
        "   0.09903244  0.04505425  0.          0.00770429]\n",
        " [ 0.06646559  0.13352931  0.07374431  0.04360733  0.36815449  0.\n",
        "   0.09296805  0.01332052  0.02709905  0.18111135]\n",
        " [ 0.05212529  0.46786199  0.          0.05311001  0.08151027  0.07011435\n",
        "   0.05686457  0.07038584  0.10261315  0.04541454]\n",
        " [ 0.00347898  0.07489009  0.05090404  0.          0.23985492  0.02661659\n",
        "   0.06256614  0.11975707  0.06244409  0.35948807]\n",
        " [ 0.04583376  0.0090684   0.456765    0.08748637  0.07007765  0.09786714\n",
        "   0.08194078  0.14309251  0.00786841  0.        ]\n",
        " [ 0.07960942  0.50021823  0.05225745  0.08046301  0.10257287  0.\n",
        "   0.04773969  0.04426048  0.06026972  0.03260913]\n",
        " [ 0.06082103  0.03519214  0.10423017  0.26407052  0.04712151  0.1204162\n",
        "   0.          0.03265126  0.2561989   0.07929828]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
        "[[ 0.06378345  0.04490422  0.05894668  0.10266284  0.0514164   0.04390932\n",
        "   0.          0.50447703  0.07197664  0.0579234 ]\n",
        " [ 0.07569808  0.09975607  0.2697629   0.16747458  0.03143456  0.09868793\n",
        "   0.0456102   0.0669842   0.14459147  0.        ]\n",
        " [ 0.03309263  0.75613501  0.03473172  0.          0.05558735  0.07283304\n",
        "   0.02457961  0.00880727  0.01165458  0.00257879]\n",
        " [ 0.55559748  0.05977586  0.05017928  0.03170928  0.          0.00254614\n",
        "   0.14132558  0.0835443   0.02067898  0.05464309]\n",
        " [ 0.05215303  0.01561012  0.07397474  0.          0.37131839  0.06749326\n",
        "   0.07116391  0.04227431  0.06834569  0.23766654]\n",
        " [ 0.02181314  0.68930595  0.01131158  0.03417106  0.06827025  0.06945809\n",
        "   0.01405248  0.          0.04084024  0.0507772 ]\n",
        " [ 0.05065514  0.04013166  0.06429947  0.08812452  0.16756548  0.08268979\n",
        "   0.          0.12908702  0.18533754  0.19210938]\n",
        " [ 0.          0.07287651  0.07939371  0.05744035  0.20236705  0.12085769\n",
        "   0.09888884  0.04745873  0.08097638  0.23974074]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
        "400\t598.088\t1.15458\t1.11653\t8937\t1641\n",
        "[[ 0.00527386  0.01297063  0.1133123   0.16226271  0.          0.29148117\n",
        "   0.09257265  0.099691    0.15947245  0.06296323]\n",
        " [ 0.44491387  0.04468546  0.0716683   0.09631053  0.          0.15226349\n",
        "   0.05376611  0.0714894   0.05300036  0.01190247]\n",
        " [ 0.07444232  0.07308612  0.13816356  0.09211444  0.28274406  0.08562786\n",
        "   0.0444141   0.06304471  0.          0.14636283]\n",
        " [ 0.0478657   0.50337674  0.05007988  0.02703745  0.01685039  0.13735617\n",
        "   0.02305029  0.07198503  0.12239834  0.        ]\n",
        " [ 0.          0.06818105  0.07663255  0.05848842  0.21005188  0.12701956\n",
        "   0.01585871  0.09953497  0.01635473  0.32787812]\n",
        " [ 0.04414095  0.0537855   0.26260892  0.15068711  0.          0.1400364\n",
        "   0.02636061  0.15271688  0.11937719  0.05028643]\n",
        " [ 0.04243186  0.33638174  0.1330079   0.10193898  0.08642571  0.15144151\n",
        "   0.03247676  0.05821873  0.          0.0576768 ]\n",
        " [ 0.02380293  0.          0.15085145  0.23323959  0.06841201  0.08287634\n",
        "   0.02268507  0.11961273  0.20186141  0.09665847]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
        "[[ 0.00744885  0.0105816   0.10696201  0.08546695  0.05731538  0.13452207\n",
        "   0.          0.4917185   0.0567221   0.04926254]\n",
        " [ 0.09850423  0.11201217  0.23992481  0.08559595  0.01809138  0.16367116\n",
        "   0.08237827  0.03769573  0.16212629  0.        ]\n",
        " [ 0.          0.52558699  0.09525052  0.03253741  0.03204753  0.13871428\n",
        "   0.03408365  0.05944365  0.02753965  0.05479632]\n",
        " [ 0.36886214  0.00608236  0.07256318  0.15758553  0.          0.16154655\n",
        "   0.07414716  0.06891149  0.06391438  0.02638722]\n",
        " [ 0.00811061  0.05366507  0.08687197  0.0083862   0.36203866  0.13682476\n",
        "   0.          0.08310965  0.0295311   0.23146199]\n",
        " [ 0.          0.59250943  0.079916    0.01147877  0.02617622  0.10599149\n",
        "   0.02733228  0.09255401  0.01786997  0.04617184]\n",
        " [ 0.          0.02270456  0.11815778  0.13717318  0.12404448  0.22074278\n",
        "   0.08934197  0.1242984   0.05131316  0.11222369]\n",
        " [ 0.01338363  0.07200094  0.14439785  0.03459698  0.15153347  0.20653785\n",
        "   0.00812537  0.17123752  0.          0.19818639]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
        "500\t754.113\t1.25788\t1.23443\t9407\t1724\n",
        "[[ 0.          0.02891975  0.04881382  0.30302224  0.00075248  0.46057521\n",
        "   0.05378389  0.00323883  0.09621244  0.00468135]\n",
        " [ 0.61757863  0.03816305  0.          0.0389604   0.01145292  0.09240406\n",
        "   0.06944841  0.04472913  0.0554948   0.0317686 ]\n",
        " [ 0.00463092  0.10865042  0.13667078  0.05068241  0.41390131  0.01389684\n",
        "   0.0233763   0.06677081  0.          0.18142023]\n",
        " [ 0.01466275  0.63705984  0.04004558  0.04287079  0.05133166  0.07650085\n",
        "   0.04772935  0.          0.07427749  0.0155217 ]\n",
        " [ 0.00457532  0.05977424  0.04877531  0.00745145  0.25891632  0.02581625\n",
        "   0.02546274  0.09916574  0.          0.47006263]\n",
        " [ 0.05062894  0.01291651  0.27756124  0.13568949  0.07745184  0.13371536\n",
        "   0.          0.07166404  0.16931074  0.07106183]\n",
        " [ 0.01995421  0.75138493  0.00946618  0.01591605  0.02769214  0.04591491\n",
        "   0.038952    0.05352488  0.0371947   0.        ]\n",
        " [ 0.04837994  0.00240921  0.2122756   0.36578896  0.          0.04712913\n",
        "   0.00479854  0.0295247   0.2203183   0.06937563]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
        "[[ 0.          0.01985041  0.0675927   0.04642947  0.01842277  0.04145575\n",
        "   0.00170529  0.70557462  0.06061517  0.03835382]\n",
        " [ 0.05369484  0.12611793  0.32184163  0.11015512  0.02767687  0.08041747\n",
        "   0.12338377  0.05745063  0.09926175  0.        ]\n",
        " [ 0.03160222  0.7226151   0.01083586  0.06084669  0.00685638  0.08788819\n",
        "   0.01358421  0.03813298  0.02763837  0.        ]\n",
        " [ 0.4956465   0.03457117  0.04459743  0.09026853  0.05997904  0.\n",
        "   0.11198345  0.04804229  0.06416407  0.05074752]\n",
        " [ 0.00949373  0.01818062  0.07812057  0.03422944  0.47670385  0.03955034\n",
        "   0.          0.03364471  0.00943969  0.30063704]\n",
        " [ 0.02532349  0.62855001  0.04452251  0.04029086  0.04656468  0.\n",
        "   0.03152085  0.08125483  0.04727819  0.05469457]\n",
        " [ 0.02909834  0.          0.05840667  0.11033179  0.32486774  0.01959523\n",
        "   0.13003703  0.13713776  0.13377199  0.05675346]\n",
        " [ 0.          0.03766496  0.14692979  0.03840858  0.2567558   0.10825623\n",
        "   0.05322789  0.0626081   0.04627424  0.24987442]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
        "600\t910.048\t1.03296\t1.01747\t7875\t1393\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\\\\srv-file.brml.tum.de\\nthome\\mfiedler\\AppData\\Roaming\\Python\\Python27\\site-packages\\climin\\util.py:148: UserWarning: Argument named steprate is not expected by <class 'climin.adadelta.Adadelta'>\n",
        "  % (i, klass))\n",
        "\\\\srv-file.brml.tum.de\\nthome\\mfiedler\\AppData\\Roaming\\Python\\Python27\\site-packages\\climin\\util.py:148: UserWarning: Argument named f is not expected by <class 'climin.adadelta.Adadelta'>\n",
        "  % (i, klass))\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parameters that did best on the, uh, testing set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.parameters.data[...] = info['best_pars']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluation\n",
      "=========="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(16, 9))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(losses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[<matplotlib.lines.Line2D at 0x343629e8>,\n",
        " <matplotlib.lines.Line2D at 0x34362c18>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAA58AAAIXCAYAAAAbhep9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1ba295f8>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_predict = m.function(['inpt'], T.argmax(m.mlp.layers[-1].output_in, axis=1))\n",
      "\n",
      "data = minibatches(TX, batch_size, 0)\n",
      "TY = np.concatenate([f_predict(element)\n",
      "                                for element in data], axis=0)\n",
      "# TY = m.predict(TX)\n",
      "\n",
      "print (np.asarray(TY) != np.asarray(TZ.argmax(axis=1)))\n",
      "print 'Number of mistakes on the testing set:', (np.asarray(TY) != np.asarray(TZ.argmax(axis=1))).sum()\n",
      "print TY[:50]\n",
      "print TZ[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[False False False ...,  True  True False]\n",
        "Number of mistakes on the testing set: 1393\n",
        "[7 2 1 0 4 1 4 4 6 7 0 6 9 0 1 5 9 7 6 4 9 6 4 5 4 0 7 4 0 1 3 1 3 6 7 2 7\n",
        " 1 2 1 1 7 4 1 3 5 1 2 4 4]\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
        " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}